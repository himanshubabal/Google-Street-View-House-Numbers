{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import h5py\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf_file = 'datasets/pickles/SVHN_multi.hdf5'\n",
    "\n",
    "hdf = h5py.File(hdf_file,'r')\n",
    "svhn_test_dataset = hdf['test_images'][:]\n",
    "svhn_test_labels = hdf['test_labels'][:]\n",
    "svhn_train_dataset = hdf['train_images'][:]\n",
    "svhn_train_labels = hdf['train_labels'][:]\n",
    "svhn_valid_dataset = hdf['valid_images'][:]\n",
    "svhn_valid_labels = hdf['valid_labels'][:]\n",
    "\n",
    "hdf.close()    \n",
    "\n",
    "print(svhn_train_dataset.shape, svhn_train_labels.shape)\n",
    "print(svhn_test_dataset.shape, svhn_test_labels.shape)\n",
    "print(svhn_valid_dataset.shape, svhn_valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svhn_train_dataset = svhn_train_dataset.astype(np.float32)\n",
    "svhn_test_dataset = svhn_test_dataset.astype(np.float32)\n",
    "svhn_valid_dataset = svhn_valid_dataset.astype(np.float32)\n",
    "\n",
    "svhn_train_labels = svhn_train_labels.astype(np.int32)\n",
    "svhn_test_labels = svhn_test_labels.astype(np.int32)\n",
    "svhn_valid_labels = svhn_valid_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230069, 32, 96, 1) (230069, 6)\n",
      "(13068, 32, 96, 1) (13068, 6)\n",
      "(5684, 32, 96, 1) (5684, 6)\n"
     ]
    }
   ],
   "source": [
    "hdf_file = 'datasets/pickles/box/SVHN_multi_box.hdf5'\n",
    "\n",
    "hdf = h5py.File(hdf_file,'r')\n",
    "svhn_train_box_dataset = hdf['train_images'][:]\n",
    "svhn_train_box_labels = hdf['train_labels'][:]\n",
    "svhn_test_box_dataset = hdf['test_images'][:]\n",
    "svhn_test_box_labels = hdf['test_labels'][:]\n",
    "svhn_valid_box_dataset = hdf['valid_images'][:]\n",
    "svhn_valid_box_labels = hdf['valid_labels'][:]\n",
    "            \n",
    "hdf.close()    \n",
    "\n",
    "print(svhn_train_box_dataset.shape, svhn_train_box_labels.shape)\n",
    "print(svhn_test_box_dataset.shape, svhn_test_box_labels.shape)\n",
    "print(svhn_valid_box_dataset.shape, svhn_valid_box_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svhn_train_box_dataset = svhn_train_box_dataset.astype(np.float32)\n",
    "svhn_test_box_dataset = svhn_test_box_dataset.astype(np.float32)\n",
    "svhn_valid_box_dataset = svhn_valid_box_dataset.astype(np.float32)\n",
    "\n",
    "svhn_train_box_labels = svhn_train_box_labels.astype(np.int32)\n",
    "svhn_test_box_labels = svhn_test_box_labels.astype(np.int32)\n",
    "svhn_valid_box_labels = svhn_valid_box_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(svhn_train_dataset.shape, svhn_train_labels.shape)\n",
    "print(svhn_test_dataset.shape, svhn_test_labels.shape)\n",
    "print(svhn_valid_dataset.shape, svhn_valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_dataset(images, labels):\n",
    "    shuffle = list(zip(images, labels))\n",
    "    np.random.shuffle(shuffle)\n",
    "    i, l = zip(*shuffle)\n",
    "    i, l = np.asarray(i), np.asarray(l)\n",
    "    return i, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svhn_train_dataset, svhn_train_labels = randomize_dataset(svhn_train_dataset, svhn_train_labels)\n",
    "svhn_test_dataset, svhn_test_labels = randomize_dataset(svhn_test_dataset, svhn_test_labels)\n",
    "svhn_valid_dataset, svhn_valid_labels = randomize_dataset(svhn_valid_dataset, svhn_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svhn_train_dataset_1 = svhn_train_dataset[0:125000]\n",
    "svhn_train_labels_1 = svhn_train_labels[0:125000]\n",
    "\n",
    "print(svhn_train_dataset_1.shape, svhn_train_labels_1.shape)\n",
    "print(svhn_train_dataset.shape, svhn_train_labels.shape)\n",
    "print(svhn_test_dataset.shape, svhn_test_labels.shape)\n",
    "print(svhn_valid_dataset.shape, svhn_valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph_svhn = tf.Graph()\n",
    "\n",
    "with graph_svhn.as_default():\n",
    "    HEIGHT = 32\n",
    "    WIDTH = 32*3\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, 1])\n",
    "    Y_ = tf.placeholder(tf.int32, [None, 6])\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Dropout Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # 5 Layers and their no of neurons\n",
    "    # 3 Convolutional Layers and a fully connected layer\n",
    "    K = 6     # First Conv Layer with depth 6\n",
    "    L = 12     # Second Conv Layer with depth 12\n",
    "    M = 24    # Third Conv layer with depth 24\n",
    "    N = 200   # Fourth Fully Connected layer with 200 neurons\n",
    "    # Last one will be softmax layer with 10 output channels\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))    # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n",
    "    \n",
    "    W5_1 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1))\n",
    "    B5_1 = tf.Variable(tf.constant(0.1, tf.float32, [11]))\n",
    "    \n",
    "    W5_2 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1))\n",
    "    B5_2 = tf.Variable(tf.constant(0.1, tf.float32, [11]))\n",
    "    \n",
    "    W5_3 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1))\n",
    "    B5_3 = tf.Variable(tf.constant(0.1, tf.float32, [11]))\n",
    "    \n",
    "    W5_4 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1))\n",
    "    B5_4 = tf.Variable(tf.constant(0.1, tf.float32, [11]))\n",
    "    \n",
    "    W5_5 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1))\n",
    "    B5_5 = tf.Variable(tf.constant(0.1, tf.float32, [11]))\n",
    "    \n",
    "    # Model\n",
    "    stride = 1  # output is 32x96\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    \n",
    "    stride = 2  # output is 16x48\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    \n",
    "    stride = 2  # output is 8x24\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    shape = Y3.get_shape().as_list()\n",
    "    YY = tf.reshape(Y3, shape=[-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([shape[1] * shape[2] * shape[3], N], stddev=0.1))\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n",
    "\n",
    "    Y4 = tf.sigmoid(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    \n",
    "    Ylogits_1 = tf.matmul(YY4, W5_1) + B5_1\n",
    "    Ylogits_2 = tf.matmul(YY4, W5_2) + B5_2\n",
    "    Ylogits_3 = tf.matmul(YY4, W5_3) + B5_3\n",
    "    Ylogits_4 = tf.matmul(YY4, W5_4) + B5_4\n",
    "    Ylogits_5 = tf.matmul(YY4, W5_5) + B5_5   \n",
    "    ## ('Ylogits_1 shape : ', [None, 11])\n",
    "    \n",
    "    Y_1 = tf.nn.softmax(Ylogits_1)\n",
    "    Y_2 = tf.nn.softmax(Ylogits_2)\n",
    "    Y_3 = tf.nn.softmax(Ylogits_3)\n",
    "    Y_4 = tf.nn.softmax(Ylogits_4)\n",
    "    Y_5 = tf.nn.softmax(Ylogits_5)\n",
    "   \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_1, Y_[:,1])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_2, Y_[:,2])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_3, Y_[:,3])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_4, Y_[:,4])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_5, Y_[:,5]))\n",
    "\n",
    "    train_prediction = tf.pack([Y_1, Y_2, Y_3, Y_4, Y_5])\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "    \n",
    "    W_s = tf.pack([tf.reduce_max(tf.abs(W1)),tf.reduce_max(tf.abs(W2)),tf.reduce_max(tf.abs(W3)),tf.reduce_max(tf.abs(W4))])\n",
    "    b_s = tf.pack([tf.reduce_max(tf.abs(B1)),tf.reduce_max(tf.abs(B2)),tf.reduce_max(tf.abs(B3)),tf.reduce_max(tf.abs(B4))])\n",
    "    \n",
    "    \n",
    "    model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = svhn_train_dataset_1\n",
    "label_data = svhn_train_labels_1\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)\n",
    "\n",
    "num_steps_1 = 10001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "#     tf.initialize_all_variables().run() \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps_1):\n",
    "        #  learning rate decay\n",
    "        max_learning_rate = 0.0005\n",
    "        min_learning_rate = 0.0001\n",
    "#         max_learning_rate = 0.000\n",
    "#         min_learning_rate = 0.000\n",
    "        decay_speed = 5000.0\n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "#         print(svhn_train_labels.shape[0])\n",
    "        offset = (step * batch_size) % (label_data.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = label_data[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "#         _, l, train_pred = session.run([train_step, cross_entropy, train_prediction], feed_dict=feed_dict)\n",
    "        t_step, l, train_pred, W, b = session.run([train_step, cross_entropy, train_prediction, W_s, b_s], feed_dict=feed_dict)\n",
    "\n",
    "#         if(step > 1520 and step < 1526) :\n",
    "#             if(step % 1 == 0):\n",
    "#                 print('~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "#                 print('W : ', W)\n",
    "#                 print('b : ', b)\n",
    "#                 print('loss : ', l)\n",
    "#                 print('t_s : ', t_step)\n",
    "#                 print('pred : ', np.argmax(train_pred))\n",
    "#                 print('~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        \n",
    "#         if (step % 100 == 0):\n",
    "#             print('W : ', W)\n",
    "#             print('b : ', b)\n",
    "#             print('loss : ', l)\n",
    "#             print('pred : ', np.argmax(train_pred))\n",
    "#             print('       ')\n",
    "    \n",
    "        if (step % 500 == 0):\n",
    "            print('W : ', W)\n",
    "            print('b : ', b)\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % acc(train_pred, batch_labels[:,1:6]))\n",
    "            print('Learning rate : ', learning_rate)\n",
    "            print('    ')\n",
    "            \n",
    "    print('Training Complete on SVHN Data')\n",
    "    \n",
    "    save_path = model_saver.save(session, \"saved_models/CNN_SVHN_New.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  (230069, 32, 96, 1)   test :  (230069, 6)\n",
      "Initialized\n",
      "W :  [ 0.19074975  0.1999169   0.19990675  0.19999984]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "Loss at step 0: 11.967587\n",
      "Minibatch accuracy: 8.4%\n",
      "Learning rate :  0.0005\n",
      "    \n",
      "W :  [ 0.25407082  0.27844286  0.30393046  0.30161068]\n",
      "b :  [ 0.14115366  0.11585545  0.1124301   0.11016332]\n",
      "Loss at step 500: 4.559722\n",
      "Minibatch accuracy: 71.7%\n",
      "Learning rate :  0.00046193496721438383\n",
      "    \n",
      "W :  [ 0.26474366  0.34329158  0.312334    0.33900106]\n",
      "b :  [ 0.19042931  0.12220962  0.11910044  0.11027805]\n",
      "Loss at step 1000: 3.211305\n",
      "Minibatch accuracy: 82.0%\n",
      "Learning rate :  0.00042749230123119273\n",
      "    \n",
      "W :  [ 0.26771152  0.37546405  0.32671165  0.3927843 ]\n",
      "b :  [ 0.22279507  0.12939571  0.13068403  0.10978457]\n",
      "Loss at step 1500: 2.812945\n",
      "Minibatch accuracy: 83.8%\n",
      "Learning rate :  0.00039632728827268716\n",
      "    \n",
      "W :  [ 0.27037984  0.3821643   0.33027631  0.425749  ]\n",
      "b :  [ 0.25708118  0.13420933  0.14712055  0.11008257]\n",
      "Loss at step 2000: 2.288783\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate :  0.00036812801841425575\n",
      "    \n",
      "W :  [ 0.2743122   0.39418799  0.33320016  0.44738805]\n",
      "b :  [ 0.2753801   0.13853194  0.15599994  0.11238343]\n",
      "Loss at step 2500: 2.106749\n",
      "Minibatch accuracy: 88.0%\n",
      "Learning rate :  0.0003426122638850534\n",
      "    \n",
      "W :  [ 0.27553397  0.4104805   0.34292361  0.46243861]\n",
      "b :  [ 0.29552349  0.144373    0.16473027  0.1144991 ]\n",
      "Loss at step 3000: 1.897456\n",
      "Minibatch accuracy: 88.4%\n",
      "Learning rate :  0.00031952465443761056\n",
      "    \n",
      "W :  [ 0.27495533  0.43406558  0.34019697  0.47105843]\n",
      "b :  [ 0.3064329   0.1476016   0.18373352  0.117007  ]\n",
      "Loss at step 3500: 1.843472\n",
      "Minibatch accuracy: 89.7%\n",
      "Learning rate :  0.00029863412151656383\n",
      "    \n",
      "W :  [ 0.27896392  0.44648933  0.3497701   0.48058128]\n",
      "b :  [ 0.31254449  0.15547998  0.19710851  0.11989002]\n",
      "Loss at step 4000: 1.784093\n",
      "Minibatch accuracy: 88.6%\n",
      "Learning rate :  0.00027973158564688865\n",
      "    \n",
      "W :  [ 0.27996609  0.45597652  0.36580241  0.49007434]\n",
      "b :  [ 0.32591656  0.15838547  0.20784689  0.12180606]\n",
      "Loss at step 4500: 1.280073\n",
      "Minibatch accuracy: 92.8%\n",
      "Learning rate :  0.0002626278638962397\n",
      "    \n",
      "W :  [ 0.28121179  0.46550894  0.37923801  0.49078196]\n",
      "b :  [ 0.3317304   0.16307724  0.21770951  0.12316132]\n",
      "Loss at step 5000: 1.836472\n",
      "Minibatch accuracy: 89.5%\n",
      "Learning rate :  0.00024715177646857697\n",
      "    \n",
      "W :  [ 0.28174514  0.48940936  0.37983337  0.49880192]\n",
      "b :  [ 0.33210313  0.16790722  0.22930102  0.12166246]\n",
      "Loss at step 5500: 1.245696\n",
      "Minibatch accuracy: 92.8%\n",
      "Learning rate :  0.0002331484334792318\n",
      "    \n",
      "W :  [ 0.28435165  0.49278218  0.38983014  0.51491755]\n",
      "b :  [ 0.33685294  0.17220286  0.2359543   0.12389767]\n",
      "Loss at step 6000: 1.279894\n",
      "Minibatch accuracy: 92.8%\n",
      "Learning rate :  0.00022047768476488088\n",
      "    \n",
      "W :  [ 0.28489175  0.49767533  0.39865509  0.5308336 ]\n",
      "b :  [ 0.34596056  0.17454913  0.2454607   0.12318891]\n",
      "Loss at step 6500: 1.292459\n",
      "Minibatch accuracy: 92.7%\n",
      "Learning rate :  0.00020901271721360503\n",
      "    \n",
      "W :  [ 0.28660303  0.51097697  0.40056089  0.54003203]\n",
      "b :  [ 0.3485989   0.1766848   0.25128555  0.1234462 ]\n",
      "Loss at step 7000: 1.542030\n",
      "Minibatch accuracy: 90.0%\n",
      "Learning rate :  0.0001986387855766426\n",
      "    \n",
      "W :  [ 0.2871049   0.52287048  0.40485141  0.54985458]\n",
      "b :  [ 0.34931707  0.18220524  0.25973499  0.12181874]\n",
      "Loss at step 7500: 1.187361\n",
      "Minibatch accuracy: 93.6%\n",
      "Learning rate :  0.00018925206405937195\n",
      "    \n",
      "W :  [ 0.2885049   0.51991343  0.40748879  0.55806863]\n",
      "b :  [ 0.35396677  0.18566017  0.26322681  0.12276098]\n",
      "Loss at step 8000: 1.250049\n",
      "Minibatch accuracy: 93.6%\n",
      "Learning rate :  0.00018075860719786216\n",
      "    \n",
      "W :  [ 0.29025951  0.52052897  0.4112758   0.56223989]\n",
      "b :  [ 0.35903201  0.18889293  0.26983622  0.12368811]\n",
      "Loss at step 8500: 0.927943\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate :  0.00017307340962109387\n",
      "    \n",
      "W :  [ 0.28980246  0.53379899  0.41008529  0.56871641]\n",
      "b :  [ 0.3613427   0.18895695  0.27615955  0.12281077]\n",
      "Loss at step 9000: 1.284112\n",
      "Minibatch accuracy: 94.2%\n",
      "Learning rate :  0.00016611955528863463\n",
      "    \n",
      "W :  [ 0.29155049  0.53463888  0.41096339  0.57894468]\n",
      "b :  [ 0.3615891   0.19418967  0.27959695  0.1228827 ]\n",
      "Loss at step 9500: 0.833695\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate :  0.00015982744768905404\n",
      "    \n",
      "W :  [ 0.29417145  0.53623503  0.41547763  0.58071411]\n",
      "b :  [ 0.36543205  0.19735108  0.28353304  0.1228082 ]\n",
      "Loss at step 10000: 1.211462\n",
      "Minibatch accuracy: 92.8%\n",
      "Learning rate :  0.0001541341132946451\n",
      "    \n",
      "W :  [ 0.2950826   0.53978741  0.41822404  0.5863474 ]\n",
      "b :  [ 0.36908591  0.19977391  0.2854982   0.12137581]\n",
      "Loss at step 10500: 1.194972\n",
      "Minibatch accuracy: 93.0%\n",
      "Learning rate :  0.00014898257130119277\n",
      "    \n",
      "W :  [ 0.29558206  0.54911023  0.4149704   0.59277993]\n",
      "b :  [ 0.36978871  0.20174526  0.29215091  0.12328744]\n",
      "Loss at step 11000: 1.018055\n",
      "Minibatch accuracy: 94.1%\n",
      "Learning rate :  0.00014432126334493355\n",
      "    \n",
      "W :  [ 0.29676139  0.54646671  0.41670534  0.59534526]\n",
      "b :  [ 0.37303826  0.20589893  0.29378578  0.12142335]\n",
      "Loss at step 11500: 0.973085\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.0001401035374891215\n",
      "    \n",
      "W :  [ 0.29735181  0.54774255  0.42021921  0.60028112]\n",
      "b :  [ 0.37718651  0.20665251  0.29724923  0.12194237]\n",
      "Loss at step 12000: 0.872884\n",
      "Minibatch accuracy: 94.5%\n",
      "Learning rate :  0.00013628718131576502\n",
      "    \n",
      "W :  [ 0.29711276  0.55460328  0.42304957  0.60038853]\n",
      "b :  [ 0.37888891  0.20760959  0.30216509  0.12138173]\n",
      "Loss at step 12500: 1.167741\n",
      "Minibatch accuracy: 93.1%\n",
      "Learning rate :  0.00013283399944955952\n",
      "    \n",
      "W :  [ 0.29873344  0.55817556  0.42387283  0.60924602]\n",
      "b :  [ 0.37951958  0.21221951  0.30557516  0.12308294]\n",
      "Loss at step 13000: 0.681008\n",
      "Minibatch accuracy: 95.8%\n",
      "Learning rate :  0.00012970943128573357\n",
      "    \n",
      "W :  [ 0.29951504  0.55773437  0.42396322  0.60901976]\n",
      "b :  [ 0.38171026  0.21390374  0.30802515  0.12336925]\n",
      "Loss at step 13500: 1.088498\n",
      "Minibatch accuracy: 94.2%\n",
      "Learning rate :  0.0001268822050958999\n",
      "    \n",
      "W :  [ 0.30108902  0.5554862   0.42573777  0.61262953]\n",
      "b :  [ 0.38524485  0.21536541  0.31059769  0.12285529]\n",
      "Loss at step 14000: 0.871044\n",
      "Minibatch accuracy: 95.0%\n",
      "Learning rate :  0.0001243240250500872\n",
      "    \n",
      "W :  [ 0.30379778  0.56750166  0.42500678  0.61618477]\n",
      "b :  [ 0.38754025  0.2160151   0.31520844  0.1220012 ]\n",
      "Loss at step 14500: 0.797004\n",
      "Minibatch accuracy: 94.8%\n",
      "Learning rate :  0.0001220092880225629\n",
      "    \n",
      "W :  [ 0.30288929  0.5645476   0.42503697  0.6178683 ]\n",
      "b :  [ 0.38931325  0.21962275  0.31712058  0.12354888]\n",
      "Loss at step 15000: 0.963797\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00011991482734714559\n",
      "    \n",
      "W :  [ 0.30427477  0.56710094  0.42614514  0.62112731]\n",
      "b :  [ 0.39216492  0.2209159   0.32042506  0.12243137]\n",
      "Loss at step 15500: 0.836239\n",
      "Minibatch accuracy: 95.2%\n",
      "Learning rate :  0.00011801968095742312\n",
      "    \n",
      "W :  [ 0.30481309  0.56911266  0.42831764  0.62528563]\n",
      "b :  [ 0.39514363  0.22103672  0.32258591  0.12330586]\n",
      "Loss at step 16000: 1.343521\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate :  0.00011630488159134649\n",
      "    \n",
      "W :  [ 0.30856106  0.57537085  0.42879766  0.6288535 ]\n",
      "b :  [ 0.39564943  0.22348668  0.32505795  0.12304617]\n",
      "Loss at step 16500: 0.779266\n",
      "Minibatch accuracy: 96.1%\n",
      "Learning rate :  0.00011475326696049602\n",
      "    \n",
      "W :  [ 0.30898014  0.57444549  0.42889357  0.62895906]\n",
      "b :  [ 0.39932492  0.22704938  0.32642746  0.12302902]\n",
      "Loss at step 17000: 1.076303\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00011334930798413044\n",
      "    \n",
      "W :  [ 0.30832806  0.57447547  0.42925277  0.62895685]\n",
      "b :  [ 0.40117049  0.22728099  0.329651    0.12230821]\n",
      "Loss at step 17500: 0.689044\n",
      "Minibatch accuracy: 95.8%\n",
      "Learning rate :  0.0001120789533689274\n",
      "    \n",
      "W :  [ 0.3172693   0.58374155  0.42909652  0.63279122]\n",
      "b :  [ 0.40313902  0.22755708  0.33294925  0.12204325]\n",
      "Loss at step 18000: 0.843931\n",
      "Minibatch accuracy: 94.4%\n",
      "Learning rate :  0.00011092948897891703\n",
      "    \n",
      "W :  [ 0.31449082  0.58211064  0.42799726  0.63685727]\n",
      "b :  [ 0.40393436  0.23132668  0.33504963  0.12218963]\n",
      "Loss at step 18500: 0.969568\n",
      "Minibatch accuracy: 95.0%\n",
      "Learning rate :  0.00010988941058813576\n",
      "    \n",
      "W :  [ 0.31419757  0.58224475  0.43073392  0.64065129]\n",
      "b :  [ 0.40559185  0.23198217  0.33663344  0.12287436]\n",
      "Loss at step 19000: 0.905153\n",
      "Minibatch accuracy: 94.5%\n",
      "Learning rate :  0.00010894830874246625\n",
      "    \n",
      "W :  [ 0.31531528  0.58254921  0.43363559  0.63992006]\n",
      "b :  [ 0.40883577  0.23393703  0.33787301  0.12198375]\n",
      "Loss at step 19500: 0.732137\n",
      "Minibatch accuracy: 96.2%\n",
      "Learning rate :  0.00010809676457832176\n",
      "    \n",
      "W :  [ 0.32111117  0.5916279   0.43001229  0.64447665]\n",
      "b :  [ 0.41002756  0.23460059  0.3419216   0.12285057]\n",
      "Loss at step 20000: 0.610448\n",
      "Minibatch accuracy: 97.2%\n",
      "Learning rate :  0.00010732625555549367\n",
      "    \n",
      "W :  [ 0.32031274  0.58737618  0.42974323  0.6462335 ]\n",
      "b :  [ 0.41379994  0.23644747  0.34335166  0.12312569]\n",
      "Loss at step 20500: 0.766950\n",
      "Minibatch accuracy: 94.8%\n",
      "Learning rate :  0.0001066290701607045\n",
      "    \n",
      "W :  [ 0.32043576  0.58876461  0.4295395   0.64589459]\n",
      "b :  [ 0.41569662  0.23684923  0.34531116  0.12200528]\n",
      "Loss at step 21000: 0.532706\n",
      "Minibatch accuracy: 96.7%\n",
      "Learning rate :  0.00010599823072819108\n",
      "    \n",
      "W :  [ 0.32711497  0.5926289   0.4317939   0.64664978]\n",
      "b :  [ 0.41706806  0.23664843  0.34829733  0.12173328]\n",
      "Loss at step 21500: 0.870299\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00010542742360488037\n",
      "    \n",
      "W :  [ 0.32664731  0.59392744  0.43166989  0.65021604]\n",
      "b :  [ 0.41873276  0.24006529  0.34953678  0.12209807]\n",
      "Loss at step 22000: 0.601744\n",
      "Minibatch accuracy: 96.6%\n",
      "Learning rate :  0.00010491093596122738\n",
      "    \n",
      "W :  [ 0.32610112  0.59280312  0.43166831  0.65330553]\n",
      "b :  [ 0.42157406  0.24203363  0.35056183  0.12292419]\n",
      "Loss at step 22500: 0.835114\n",
      "Minibatch accuracy: 94.1%\n",
      "Learning rate :  0.00010444359861529693\n",
      "    \n",
      "W :  [ 0.32679433  0.5927338   0.43257675  0.65283805]\n",
      "b :  [ 0.42289054  0.24341005  0.35298273  0.12238281]\n",
      "Loss at step 23000: 0.608767\n",
      "Minibatch accuracy: 96.1%\n",
      "Learning rate :  0.00010402073429785344\n",
      "    \n",
      "W :  [ 0.33352241  0.60023916  0.43264547  0.65649277]\n",
      "b :  [ 0.42564946  0.24342559  0.35711509  0.12134128]\n",
      "Loss at step 23500: 0.767980\n",
      "Minibatch accuracy: 96.1%\n",
      "Learning rate :  0.00010363811084067834\n",
      "    \n",
      "W :  [ 0.3314372   0.59871209  0.43078634  0.6582312 ]\n",
      "b :  [ 0.42621291  0.24592207  0.35820034  0.12183767]\n",
      "Loss at step 24000: 1.059489\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00010329189881960801\n",
      "    \n",
      "W :  [ 0.33118051  0.60005325  0.43245822  0.65693146]\n",
      "b :  [ 0.42867219  0.246676    0.35955158  0.12116233]\n",
      "Loss at step 24500: 0.564812\n",
      "Minibatch accuracy: 97.2%\n",
      "Learning rate :  0.00010297863322836974\n",
      "    \n",
      "W :  [ 0.33504272  0.60227537  0.43511152  0.65868348]\n",
      "b :  [ 0.43187413  0.24765421  0.36235616  0.12168499]\n",
      "Loss at step 25000: 1.102894\n",
      "Minibatch accuracy: 93.9%\n",
      "Learning rate :  0.00010269517879963419\n",
      "    \n",
      "W :  [ 0.33801201  0.60847706  0.43303666  0.66301936]\n",
      "b :  [ 0.43204051  0.24884482  0.36420521  0.12101039]\n",
      "Loss at step 25500: 0.859907\n",
      "Minibatch accuracy: 94.8%\n",
      "Learning rate :  0.00010243869862620625\n",
      "    \n",
      "W :  [ 0.33789173  0.60662347  0.43367574  0.66142386]\n",
      "b :  [ 0.43456751  0.25122145  0.3657622   0.12123427]\n",
      "Loss at step 26000: 0.740996\n",
      "Minibatch accuracy: 95.9%\n",
      "Learning rate :  0.00010220662576830431\n",
      "    \n",
      "W :  [ 0.33848223  0.60749716  0.43508646  0.66168207]\n",
      "b :  [ 0.43688729  0.25131956  0.36772078  0.12160379]\n",
      "Loss at step 26500: 0.546647\n",
      "Minibatch accuracy: 97.7%\n",
      "Learning rate :  0.00010199663756276409\n",
      "    \n",
      "W :  [ 0.34583235  0.61429149  0.43767324  0.66571903]\n",
      "b :  [ 0.43951383  0.25447601  0.37088782  0.12040838]\n",
      "Loss at step 27000: 0.572298\n",
      "Minibatch accuracy: 97.3%\n",
      "Learning rate :  0.00010180663237704508\n",
      "    \n",
      "W :  [ 0.34262142  0.613325    0.4379929   0.67157924]\n",
      "b :  [ 0.43951786  0.25500304  0.3721295   0.1211101 ]\n",
      "Loss at step 27500: 0.655249\n",
      "Minibatch accuracy: 96.6%\n",
      "Learning rate :  0.00010163470857538563\n",
      "    \n",
      "W :  [ 0.34330863  0.61593342  0.43859136  0.6711753 ]\n",
      "b :  [ 0.44239196  0.2556915   0.37372097  0.12158242]\n",
      "Loss at step 28000: 0.830080\n",
      "Minibatch accuracy: 95.2%\n",
      "Learning rate :  0.00010147914548659317\n",
      "    \n",
      "W :  [ 0.34340155  0.61469793  0.43829092  0.67081988]\n",
      "b :  [ 0.44443163  0.25785965  0.37465382  0.12105295]\n",
      "Loss at step 28500: 0.829882\n",
      "Minibatch accuracy: 95.2%\n",
      "Learning rate :  0.00010133838618298851\n",
      "    \n",
      "W :  [ 0.34951338  0.62294817  0.43911478  0.67340672]\n",
      "b :  [ 0.44673151  0.25996166  0.37890133  0.12150018]\n",
      "Loss at step 29000: 0.629516\n",
      "Minibatch accuracy: 96.4%\n",
      "Learning rate :  0.00010121102189815033\n",
      "    \n",
      "W :  [ 0.34920034  0.62135571  0.43809253  0.67341328]\n",
      "b :  [ 0.44970575  0.26018667  0.37941831  0.11984724]\n",
      "Loss at step 29500: 1.110820\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate :  0.00010109577792750736\n",
      "    \n",
      "W :  [ 0.34903529  0.62299556  0.43791628  0.67180574]\n",
      "b :  [ 0.45131391  0.2604095   0.38082686  0.11985583]\n",
      "Loss at step 30000: 0.613717\n",
      "Minibatch accuracy: 95.9%\n",
      "Learning rate :  0.00010099150087066655\n",
      "    \n",
      "W :  [ 0.35605755  0.62567443  0.43922094  0.67397583]\n",
      "b :  [ 0.45400807  0.26479152  0.38484767  0.11947874]\n",
      "Loss at step 30500: 0.700726\n",
      "Minibatch accuracy: 95.9%\n",
      "Learning rate :  0.00010089714708779433\n",
      "    \n",
      "W :  [ 0.3554717   0.62805694  0.44164488  0.6795156 ]\n",
      "b :  [ 0.45417494  0.26535258  0.3853116   0.12060372]\n",
      "Loss at step 31000: 0.543762\n",
      "Minibatch accuracy: 97.5%\n",
      "Learning rate :  0.0001008117722545183\n",
      "    \n",
      "W :  [ 0.35401648  0.62910205  0.44184446  0.6809966 ]\n",
      "b :  [ 0.45653906  0.2653088   0.38697922  0.12084045]\n",
      "Loss at step 31500: 0.574539\n",
      "Minibatch accuracy: 96.2%\n",
      "Learning rate :  0.00010073452191081157\n",
      "    \n",
      "W :  [ 0.35559615  0.63007802  0.44403681  0.68140692]\n",
      "b :  [ 0.45886466  0.2657769   0.38804892  0.12043332]\n",
      "Loss at step 32000: 0.459948\n",
      "Minibatch accuracy: 97.0%\n",
      "Learning rate :  0.00010066462290926957\n",
      "    \n",
      "W :  [ 0.36223346  0.63583893  0.44320381  0.68623447]\n",
      "b :  [ 0.46225128  0.27060026  0.39227861  0.12041166]\n",
      "Loss at step 32500: 0.829114\n",
      "Minibatch accuracy: 94.8%\n",
      "Learning rate :  0.00010060137567719104\n",
      "    \n",
      "W :  [ 0.35888976  0.63431126  0.44215935  0.68621981]\n",
      "b :  [ 0.4637292   0.27025312  0.39304152  0.12104116]\n",
      "Loss at step 33000: 0.723144\n",
      "Minibatch accuracy: 96.1%\n",
      "Learning rate :  0.00010054414721501916\n",
      "    \n",
      "W :  [ 0.35968745  0.63794667  0.44419372  0.68751144]\n",
      "b :  [ 0.46470574  0.27162912  0.39477161  0.11960393]\n",
      "Loss at step 33500: 0.563270\n",
      "Minibatch accuracy: 96.1%\n",
      "Learning rate :  0.0001004923647610694\n",
      "    \n",
      "W :  [ 0.36391798  0.6389541   0.44590178  0.68779796]\n",
      "b :  [ 0.46893832  0.27431729  0.39818132  0.12031129]\n",
      "Loss at step 34000: 0.955986\n",
      "Minibatch accuracy: 94.5%\n",
      "Learning rate :  0.00010044551005913793\n",
      "    \n",
      "W :  [ 0.3656081   0.64222908  0.44634023  0.68999749]\n",
      "b :  [ 0.46976608  0.2755717   0.39867169  0.11980155]\n",
      "Loss at step 34500: 0.545988\n",
      "Minibatch accuracy: 96.7%\n",
      "Learning rate :  0.0001004031141716194\n",
      "    \n",
      "W :  [ 0.36623451  0.64230168  0.44580218  0.68915826]\n",
      "b :  [ 0.47207743  0.27656725  0.39919519  0.12060326]\n",
      "Loss at step 35000: 0.429819\n",
      "Minibatch accuracy: 97.5%\n",
      "Learning rate :  0.00010036475278622182\n",
      "    \n",
      "W :  [ 0.36696857  0.6438182   0.44609666  0.687953  ]\n",
      "b :  [ 0.47377369  0.27709758  0.40203235  0.11950022]\n",
      "Loss at step 35500: 0.445610\n",
      "Minibatch accuracy: 97.7%\n",
      "Learning rate :  0.00010033004196930636\n",
      "    \n",
      "W :  [ 0.37382579  0.65030921  0.44521627  0.6905905 ]\n",
      "b :  [ 0.47725704  0.28168505  0.40508637  0.1197933 ]\n",
      "Loss at step 36000: 0.577288\n",
      "Minibatch accuracy: 96.6%\n",
      "Learning rate :  0.00010029863432335068\n",
      "    \n",
      "W :  [ 0.37207898  0.64795268  0.44551629  0.69209367]\n",
      "b :  [ 0.47759846  0.28235391  0.40636709  0.11985128]\n",
      "Loss at step 36500: 0.632186\n",
      "Minibatch accuracy: 96.4%\n",
      "Learning rate :  0.00010027021551007754\n",
      "    \n",
      "W :  [ 0.37081009  0.65183204  0.4465538   0.69401032]\n",
      "b :  [ 0.47935763  0.28191245  0.40750352  0.11973029]\n",
      "Loss at step 37000: 0.468416\n",
      "Minibatch accuracy: 97.2%\n",
      "Learning rate :  0.00010024450110445184\n",
      "    \n",
      "W :  [ 0.37215754  0.65401959  0.4502579   0.6966728 ]\n",
      "b :  [ 0.48211876  0.28345492  0.40775394  0.11981852]\n",
      "Loss at step 37500: 0.897549\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00010022123374805913\n",
      "    \n",
      "W :  [ 0.37712166  0.6593706   0.44817942  0.69807678]\n",
      "b :  [ 0.48622304  0.28619188  0.41141602  0.119605  ]\n",
      "Loss at step 38000: 0.553293\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate :  0.00010020018057337625\n",
      "    \n",
      "W :  [ 0.37696996  0.65681148  0.44935626  0.69983816]\n",
      "b :  [ 0.48760363  0.286919    0.41249099  0.11919015]\n",
      "Loss at step 38500: 0.656603\n",
      "Minibatch accuracy: 95.8%\n",
      "Learning rate :  0.00010018113087315473\n",
      "    \n",
      "W :  [ 0.37636587  0.65991306  0.45367691  0.69919443]\n",
      "b :  [ 0.48924562  0.28801623  0.41394502  0.11961003]\n",
      "Loss at step 39000: 0.541558\n",
      "Minibatch accuracy: 95.9%\n",
      "Learning rate :  0.00010016389399159192\n",
      "    \n",
      "W :  [ 0.3830426   0.66325921  0.45257968  0.69907045]\n",
      "b :  [ 0.49418783  0.290775    0.41773656  0.12042518]\n",
      "Loss at step 39500: 0.818325\n",
      "Minibatch accuracy: 95.5%\n",
      "Learning rate :  0.00010014829741618364\n",
      "    \n",
      "W :  [ 0.38306046  0.66225314  0.45145261  0.69962424]\n",
      "b :  [ 0.49552372  0.29153675  0.41815799  0.11957367]\n",
      "Loss at step 40000: 0.383264\n",
      "Minibatch accuracy: 97.7%\n",
      "Learning rate :  0.00010013418505116101\n",
      "    \n",
      "W :  [ 0.38160816  0.66449267  0.45351034  0.70183402]\n",
      "b :  [ 0.49676424  0.29190442  0.41932359  0.11920453]\n",
      "Loss at step 40500: 0.553473\n",
      "Minibatch accuracy: 96.6%\n",
      "Learning rate :  0.00010012141565523155\n",
      "    \n",
      "W :  [ 0.38274559  0.66455042  0.45552319  0.70099992]\n",
      "b :  [ 0.4981868   0.29306689  0.42129979  0.11983193]\n",
      "Loss at step 41000: 1.011426\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00010010986142798886\n",
      "    \n",
      "W :  [ 0.38911983  0.67164755  0.45310935  0.70546949]\n",
      "b :  [ 0.50124139  0.29684836  0.42475611  0.12044121]\n",
      "Loss at step 41500: 0.564345\n",
      "Minibatch accuracy: 96.4%\n",
      "Learning rate :  0.00010009940673084318\n",
      "    \n",
      "W :  [ 0.38781333  0.66744554  0.45327264  0.70592642]\n",
      "b :  [ 0.5028249   0.29745203  0.42541334  0.12033879]\n",
      "Loss at step 42000: 0.639807\n",
      "Minibatch accuracy: 96.7%\n",
      "Learning rate :  0.00010008994692967154\n",
      "    \n",
      "W :  [ 0.38826099  0.67180389  0.45807287  0.70619988]\n",
      "b :  [ 0.50466049  0.2979877   0.42671669  0.1206345 ]\n",
      "Loss at step 42500: 0.505313\n",
      "Minibatch accuracy: 96.2%\n",
      "Learning rate :  0.00010008138734760426\n",
      "    \n",
      "W :  [ 0.39237273  0.67312491  0.45847696  0.71008986]\n",
      "b :  [ 0.50709254  0.30060729  0.42984253  0.12136157]\n",
      "Loss at step 43000: 1.078391\n",
      "Minibatch accuracy: 93.4%\n",
      "Learning rate :  0.00010007364231746704\n",
      "    \n",
      "W :  [ 0.39460754  0.67655492  0.45854494  0.71157968]\n",
      "b :  [ 0.50797349  0.30253607  0.43104836  0.12126823]\n",
      "Loss at step 43500: 0.505362\n",
      "Minibatch accuracy: 97.5%\n",
      "Learning rate :  0.00010006663432439505\n",
      "    \n",
      "W :  [ 0.39461178  0.67831171  0.45833394  0.71127874]\n",
      "b :  [ 0.50979155  0.30322042  0.43159726  0.12045977]\n",
      "Loss at step 44000: 0.746483\n",
      "Minibatch accuracy: 96.1%\n",
      "Learning rate :  0.00010006029323003819\n",
      "    \n",
      "W :  [ 0.39485073  0.67911941  0.46396476  0.71104783]\n",
      "b :  [ 0.512761    0.30249029  0.43435955  0.12104928]\n",
      "Loss at step 44500: 0.549434\n",
      "Minibatch accuracy: 97.5%\n",
      "Learning rate :  0.00010005455557059281\n",
      "    \n",
      "W :  [ 0.40213099  0.68391156  0.46280289  0.7151264 ]\n",
      "b :  [ 0.51591009  0.30710292  0.43819755  0.12232319]\n",
      "Loss at step 45000: 0.605632\n",
      "Minibatch accuracy: 97.2%\n",
      "Learning rate :  0.00010004936392163468\n",
      "    \n",
      "W :  [ 0.40020686  0.68139613  0.46166733  0.71544713]\n",
      "b :  [ 0.51696223  0.30700332  0.43907043  0.12163156]\n",
      "Loss at step 45500: 0.467209\n",
      "Minibatch accuracy: 97.0%\n",
      "Learning rate :  0.00010004466632339606\n",
      "    \n",
      "W :  [ 0.39913705  0.68500525  0.46512645  0.71251976]\n",
      "b :  [ 0.51915282  0.30731937  0.44044864  0.12083337]\n",
      "Loss at step 46000: 0.428831\n",
      "Minibatch accuracy: 97.7%\n",
      "Learning rate :  0.00010004041576073484\n",
      "    \n",
      "W :  [ 0.40078756  0.68806666  0.46703002  0.71325868]\n",
      "b :  [ 0.52131492  0.30855137  0.44202068  0.12243712]\n",
      "Loss at step 46500: 0.978615\n",
      "Minibatch accuracy: 95.5%\n",
      "Learning rate :  0.00010003656969259128\n",
      "    \n",
      "W :  [ 0.40472502  0.69192535  0.46378419  0.71394235]\n",
      "b :  [ 0.52457309  0.31031439  0.44446528  0.12167805]\n",
      "Loss at step 47000: 0.526822\n",
      "Minibatch accuracy: 97.0%\n",
      "Learning rate :  0.00010003308962622266\n",
      "    \n",
      "W :  [ 0.40517649  0.69204831  0.4660292   0.71542448]\n",
      "b :  [ 0.5263989   0.31087321  0.44484898  0.12113249]\n",
      "Loss at step 47500: 0.360304\n",
      "Minibatch accuracy: 98.0%\n",
      "Learning rate :  0.00010002994073195509\n",
      "    \n",
      "W :  [ 0.40526891  0.69258171  0.47023553  0.71515644]\n",
      "b :  [ 0.52703333  0.3132821   0.44720218  0.12209272]\n",
      "Loss at step 48000: 0.444597\n",
      "Minibatch accuracy: 97.8%\n",
      "Learning rate :  0.00010002709149459635\n",
      "    \n",
      "W :  [ 0.41197658  0.69671828  0.46728167  0.71628243]\n",
      "b :  [ 0.53302944  0.3166647   0.45055982  0.12229232]\n",
      "Loss at step 48500: 0.717334\n",
      "Minibatch accuracy: 95.6%\n",
      "Learning rate :  0.0001000245133980213\n",
      "    \n",
      "W :  [ 0.41182765  0.69654739  0.4657222   0.71838295]\n",
      "b :  [ 0.53129601  0.31739748  0.4501099   0.12190095]\n",
      "Loss at step 49000: 0.573188\n",
      "Minibatch accuracy: 96.4%\n",
      "Learning rate :  0.00010002218063977287\n",
      "    \n",
      "W :  [ 0.41019109  0.6985901   0.46836692  0.71598208]\n",
      "b :  [ 0.53320605  0.31736648  0.45203733  0.12150677]\n",
      "Loss at step 49500: 0.400208\n",
      "Minibatch accuracy: 97.0%\n",
      "Learning rate :  0.00010002006987282247\n",
      "    \n",
      "W :  [ 0.41171867  0.69922507  0.47232625  0.72071183]\n",
      "b :  [ 0.53508222  0.31870002  0.45387346  0.12279619]\n",
      "Loss at step 50000: 0.276794\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate :  0.000100018159971905\n",
      "    \n",
      "Training Complete on SVHN Data\n",
      "Model saved in file: saved_models/box/CNN_SVHN_Box.ckpt\n"
     ]
    }
   ],
   "source": [
    "train_data = svhn_train_box_dataset\n",
    "label_data = svhn_train_box_labels\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)\n",
    "\n",
    "num_steps_1 = 50001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "#     tf.initialize_all_variables().run() \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps_1):\n",
    "        #  learning rate decay\n",
    "        max_learning_rate = 0.0005\n",
    "        min_learning_rate = 0.0001\n",
    "#         max_learning_rate = 0.000\n",
    "#         min_learning_rate = 0.000\n",
    "        decay_speed = 5000.0\n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "#         print(svhn_train_labels.shape[0])\n",
    "        offset = (step * batch_size) % (label_data.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = label_data[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "#         _, l, train_pred = session.run([train_step, cross_entropy, train_prediction], feed_dict=feed_dict)\n",
    "        t_step, l, train_pred, W, b = session.run([train_step, cross_entropy, train_prediction, W_s, b_s], feed_dict=feed_dict)\n",
    "    \n",
    "        if (step % 500 == 0):\n",
    "            print('W : ', W)\n",
    "            print('b : ', b)\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % acc(train_pred, batch_labels[:,1:6]))\n",
    "            print('Learning rate : ', learning_rate)\n",
    "            print('    ')\n",
    "            \n",
    "    print('Training Complete on SVHN Data')\n",
    "    \n",
    "    save_path = model_saver.save(session, \"saved_models/box/CNN_SVHN_Box.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph_svhn) as session:\n",
    "    model_saver.restore(session, \"saved_models/CNN_SVHN_New.ckpt\")\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized')\n",
    "    \n",
    "    _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : svhn_test_dataset[:500], Y_ : svhn_test_labels[:500], pkeep : 1.0, alpha : 0.002})\n",
    "    print('Test accuracy: ', acc(predictions, svhn_test_labels[:500,1:6]))\n",
    "    \n",
    "    del l, predictions\n",
    "    _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : svhn_valid_dataset, Y_ : svhn_valid_labels, pkeep : 1.0, alpha : 0.002})\n",
    "    print('Validation accuracy: ', acc(predictions, svhn_valid_labels[:,1:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Initialized\n",
      "Test-Accuracy  i :  0\n",
      "Test accuracy:  93.84\n",
      "       \n",
      "Test-Accuracy  i :  1\n",
      "Test accuracy:  94.0\n",
      "       \n",
      "Test-Accuracy  i :  2\n",
      "Test accuracy:  93.78\n",
      "       \n",
      "Test-Accuracy  i :  3\n",
      "Test accuracy:  93.68\n",
      "       \n",
      "Test-Accuracy  i :  4\n",
      "Test accuracy:  94.0\n",
      "       \n",
      "Test-Accuracy  i :  5\n",
      "Test accuracy:  93.66\n",
      "       \n",
      "Test-Accuracy  i :  6\n",
      "Test accuracy:  93.4\n",
      "       \n",
      "Test-Accuracy  i :  7\n",
      "Test accuracy:  92.84\n",
      "       \n",
      "Test-Accuracy  i :  8\n",
      "Test accuracy:  93.38\n",
      "       \n",
      "Test-Accuracy  i :  9\n",
      "Test accuracy:  93.1\n",
      "       \n",
      "Test-Accuracy  i :  10\n",
      "Test accuracy:  93.38\n",
      "       \n",
      "Test-Accuracy  i :  11\n",
      "Test accuracy:  93.4\n",
      "       \n",
      "-----VALIDIDATION------\n",
      "Valid-Accuracy  i :  0\n",
      "Valid accuracy:  93.96\n",
      "-------\n",
      "Valid-Accuracy  i :  1\n",
      "Valid accuracy:  94.12\n",
      "-------\n",
      "Valid-Accuracy  i :  2\n",
      "Valid accuracy:  91.88\n",
      "-------\n",
      "Valid-Accuracy  i :  3\n",
      "Valid accuracy:  92.42\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_svhn) as session:\n",
    "    model_saver.restore(session, \"saved_models/box/CNN_SVHN_Box.ckpt\")\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized')\n",
    "    \n",
    "    t_batch = 1000\n",
    "    \n",
    "    test_no = int(svhn_test_box_labels.shape[0] / t_batch)\n",
    "    for i in range(test_no - 1):\n",
    "        data = svhn_test_box_dataset[i*t_batch:(i+1)*t_batch]\n",
    "        labels = svhn_test_box_labels[i*t_batch:(i+1)*t_batch]\n",
    "        \n",
    "        print('Test-Accuracy', ' i : ', i)\n",
    "        _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : data, Y_ : labels, pkeep : 1.0, alpha : 0.002})\n",
    "        print('Test accuracy: ', acc(predictions, labels[:,1:6]))\n",
    "        print('       ')\n",
    "    \n",
    "    print('-----VALIDIDATION------')\n",
    "    valid_no = int(svhn_valid_box_labels.shape[0] /  t_batch)\n",
    "    for i in range(valid_no - 1):\n",
    "        data = svhn_valid_box_dataset[i*t_batch:(i+1)*t_batch]\n",
    "        labels = svhn_valid_box_labels[i*t_batch:(i+1)*t_batch]\n",
    "        \n",
    "        print('Valid-Accuracy', ' i : ', i)\n",
    "        _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : data, Y_ : labels, pkeep : 1.0, alpha : 0.002})\n",
    "        print('Valid accuracy: ', acc(predictions, labels[:,1:6]))\n",
    "        print('-------')\n",
    "        \n",
    "#     _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : svhn_valid_box_dataset, Y_ : svhn_valid_box_labels, pkeep : 1.0, alpha : 0.002})\n",
    "#     print('Validation accuracy: ', acc(predictions, svhn_valid_box_labels[:,1:6]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
