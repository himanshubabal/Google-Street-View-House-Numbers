{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "import gc\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_dataset(images, labels):\n",
    "    shuffle = list(zip(images, labels))\n",
    "    np.random.shuffle(shuffle)\n",
    "    i, l = zip(*shuffle)\n",
    "    i, l = np.asarray(i), np.asarray(l)\n",
    "    return i, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (80000, 32, 96, 1) (80000, 6)\n",
      "Test set (10000, 32, 96, 1) (10000, 6)\n"
     ]
    }
   ],
   "source": [
    "hdf_file = 'datasets/pickles/MNIST_multi.hdf5'\n",
    "\n",
    "hdf = h5py.File(hdf_file,'r')\n",
    "train_dataset = hdf['train_images'][:]\n",
    "train_labels = hdf['train_labels'][:]\n",
    "test_dataset = hdf['test_images'][:]\n",
    "test_labels = hdf['test_labels'][:]\n",
    "            \n",
    "hdf.close()    \n",
    "    \n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.astype(np.float32)\n",
    "test_dataset = test_dataset.astype(np.float32)\n",
    "\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "test_labels = test_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset, train_labels = randomize_dataset(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize_dataset(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_to_save = \"saved_models/mnist/CNN_SVHN_Mnist.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_svhn = tf.Graph()\n",
    "\n",
    "with graph_svhn.as_default():\n",
    "    HEIGHT = 32\n",
    "    WIDTH = 32*3\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, 1])\n",
    "    Y_ = tf.placeholder(tf.int32, [None, 6])\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Dropout Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # 5 Layers and their no of neurons\n",
    "    # 3 Convolutional Layers and a fully connected layer\n",
    "    K = 6     # First Conv Layer with depth 6\n",
    "    L = 12     # Second Conv Layer with depth 12\n",
    "    M = 24    # Third Conv layer with depth 24\n",
    "    N = 200   # Fourth Fully Connected layer with 200 neurons\n",
    "    # Last one will be softmax layer with 10 output channels\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1), name=\"W1\")    # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]), name=\"B1\")\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1), name=\"W2\")\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]), name=\"B2\")\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1), name=\"W3\")\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]), name=\"B3\")\n",
    "    \n",
    "    W5_1 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_1\")\n",
    "    B5_1 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_1\")\n",
    "    \n",
    "    W5_2 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_2\")\n",
    "    B5_2 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_2\")\n",
    "    \n",
    "    W5_3 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_3\")\n",
    "    B5_3 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_3\")\n",
    "    \n",
    "    W5_4 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_4\")\n",
    "    B5_4 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_4\")\n",
    "    \n",
    "    W5_5 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_5\")\n",
    "    B5_5 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_5\")\n",
    "    \n",
    "    # Model\n",
    "    stride = 1  # output is 32x96\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    \n",
    "    stride = 2  # output is 16x48\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    \n",
    "    stride = 2  # output is 8x24\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    shape = Y3.get_shape().as_list()\n",
    "    YY = tf.reshape(Y3, shape=[-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([shape[1] * shape[2] * shape[3], N], stddev=0.1), name=\"W4\")\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]), name=\"B4\")\n",
    "\n",
    "    Y4 = tf.sigmoid(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    \n",
    "    Ylogits_1 = tf.matmul(YY4, W5_1) + B5_1\n",
    "    Ylogits_2 = tf.matmul(YY4, W5_2) + B5_2\n",
    "    Ylogits_3 = tf.matmul(YY4, W5_3) + B5_3\n",
    "    Ylogits_4 = tf.matmul(YY4, W5_4) + B5_4\n",
    "    Ylogits_5 = tf.matmul(YY4, W5_5) + B5_5   \n",
    "    ## ('Ylogits_1 shape : ', [None, 11])\n",
    "    \n",
    "    Y_1 = tf.nn.softmax(Ylogits_1)\n",
    "    Y_2 = tf.nn.softmax(Ylogits_2)\n",
    "    Y_3 = tf.nn.softmax(Ylogits_3)\n",
    "    Y_4 = tf.nn.softmax(Ylogits_4)\n",
    "    Y_5 = tf.nn.softmax(Ylogits_5)\n",
    "   \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_1, Y_[:,1])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_2, Y_[:,2])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_3, Y_[:,3])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_4, Y_[:,4])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_5, Y_[:,5]))\n",
    "\n",
    "    train_prediction = tf.pack([Y_1, Y_2, Y_3, Y_4, Y_5])\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "    \n",
    "    W_s = tf.pack([tf.reduce_max(tf.abs(W1)),tf.reduce_max(tf.abs(W2)),tf.reduce_max(tf.abs(W3)),tf.reduce_max(tf.abs(W4))])\n",
    "    b_s = tf.pack([tf.reduce_max(tf.abs(B1)),tf.reduce_max(tf.abs(B2)),tf.reduce_max(tf.abs(B3)),tf.reduce_max(tf.abs(B4))])\n",
    "    \n",
    "    model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  (80000, 32, 96, 1)   test :  (80000, 6)\n",
      "Initialized\n",
      "W :  [ 0.19949128  0.19964908  0.19956934  0.19999897]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "Loss at step 0: 13.885118\n",
      "Minibatch accuracy: 6.4%\n",
      "Learning rate :  0.0005\n",
      "    \n",
      "W :  [ 0.24879974  0.22469039  0.22812337  0.29539117]\n",
      "b :  [ 0.12721291  0.13720784  0.19850199  0.1549781 ]\n",
      "Loss at step 500: 5.304722\n",
      "Minibatch accuracy: 66.9%\n",
      "Learning rate :  0.00046193496721438383\n",
      "    \n",
      "W :  [ 0.27584663  0.24228962  0.24755663  0.37241647]\n",
      "b :  [ 0.11623199  0.18027917  0.24038716  0.16424127]\n",
      "Loss at step 1000: 3.250563\n",
      "Minibatch accuracy: 83.4%\n",
      "Learning rate :  0.00042749230123119273\n",
      "    \n",
      "W :  [ 0.28269762  0.27191639  0.25110638  0.35960591]\n",
      "b :  [ 0.11271461  0.18738684  0.2656005   0.16846138]\n",
      "Loss at step 1500: 2.601497\n",
      "Minibatch accuracy: 85.3%\n",
      "Learning rate :  0.00039632728827268716\n",
      "    \n",
      "W :  [ 0.28678113  0.28487185  0.25926328  0.37946457]\n",
      "b :  [ 0.12079348  0.19141315  0.28827608  0.16820388]\n",
      "Loss at step 2000: 1.849728\n",
      "Minibatch accuracy: 91.2%\n",
      "Learning rate :  0.00036812801841425575\n",
      "    \n",
      "W :  [ 0.30542186  0.28986424  0.26313391  0.38292995]\n",
      "b :  [ 0.12150938  0.16916813  0.301714    0.17159571]\n",
      "Loss at step 2500: 1.633160\n",
      "Minibatch accuracy: 91.6%\n",
      "Learning rate :  0.0003426122638850534\n",
      "    \n",
      "W :  [ 0.33461094  0.34939766  0.26740068  0.40665191]\n",
      "b :  [ 0.12114688  0.16650209  0.31657225  0.17178124]\n",
      "Loss at step 3000: 1.488178\n",
      "Minibatch accuracy: 91.7%\n",
      "Learning rate :  0.00031952465443761056\n",
      "    \n",
      "W :  [ 0.34695658  0.38528255  0.27597275  0.47592551]\n",
      "b :  [ 0.12873489  0.16035573  0.33502764  0.17770642]\n",
      "Loss at step 3500: 1.463344\n",
      "Minibatch accuracy: 90.2%\n",
      "Learning rate :  0.00029863412151656383\n",
      "    \n",
      "W :  [ 0.35725185  0.39329505  0.28058046  0.48322713]\n",
      "b :  [ 0.13404462  0.1637397   0.34198174  0.17969221]\n",
      "Loss at step 4000: 1.238994\n",
      "Minibatch accuracy: 93.1%\n",
      "Learning rate :  0.00027973158564688865\n",
      "    \n",
      "W :  [ 0.36182067  0.4146848   0.28324705  0.48664024]\n",
      "b :  [ 0.14331561  0.16627465  0.34480435  0.18227258]\n",
      "Loss at step 4500: 1.006830\n",
      "Minibatch accuracy: 92.8%\n",
      "Learning rate :  0.0002626278638962397\n",
      "    \n",
      "W :  [ 0.36645558  0.42385069  0.28976378  0.48929057]\n",
      "b :  [ 0.15635756  0.16831714  0.3557336   0.17999552]\n",
      "Loss at step 5000: 1.042737\n",
      "Minibatch accuracy: 93.6%\n",
      "Learning rate :  0.00024715177646857697\n",
      "    \n",
      "W :  [ 0.36543202  0.43625745  0.29016533  0.49701896]\n",
      "b :  [ 0.1535328   0.16828802  0.3636643   0.17893454]\n",
      "Loss at step 5500: 0.826251\n",
      "Minibatch accuracy: 95.5%\n",
      "Learning rate :  0.0002331484334792318\n",
      "    \n",
      "W :  [ 0.36158869  0.44377616  0.28821778  0.49620906]\n",
      "b :  [ 0.15770581  0.16827825  0.37018701  0.18670501]\n",
      "Loss at step 6000: 0.796665\n",
      "Minibatch accuracy: 95.2%\n",
      "Learning rate :  0.00022047768476488088\n",
      "    \n",
      "W :  [ 0.36153761  0.44676253  0.28554815  0.49656552]\n",
      "b :  [ 0.16841842  0.17180952  0.37444109  0.18467627]\n",
      "Loss at step 6500: 0.843158\n",
      "Minibatch accuracy: 96.1%\n",
      "Learning rate :  0.00020901271721360503\n",
      "    \n",
      "W :  [ 0.36174873  0.45220247  0.29024431  0.4966931 ]\n",
      "b :  [ 0.16736551  0.17211209  0.3778359   0.18343283]\n",
      "Loss at step 7000: 0.892655\n",
      "Minibatch accuracy: 95.9%\n",
      "Learning rate :  0.0001986387855766426\n",
      "    \n",
      "W :  [ 0.36493385  0.45330691  0.29373896  0.49729881]\n",
      "b :  [ 0.16928473  0.17440338  0.38551292  0.18336968]\n",
      "Loss at step 7500: 0.778325\n",
      "Minibatch accuracy: 95.5%\n",
      "Learning rate :  0.00018925206405937195\n",
      "    \n",
      "W :  [ 0.367378    0.46015918  0.29268339  0.49788275]\n",
      "b :  [ 0.16819364  0.17480052  0.38925895  0.18640341]\n",
      "Loss at step 8000: 0.684373\n",
      "Minibatch accuracy: 96.4%\n",
      "Learning rate :  0.00018075860719786216\n",
      "    \n",
      "W :  [ 0.36680636  0.45859727  0.29575843  0.49824744]\n",
      "b :  [ 0.1705143   0.17262261  0.39013538  0.1882153 ]\n",
      "Loss at step 8500: 0.698404\n",
      "Minibatch accuracy: 95.9%\n",
      "Learning rate :  0.00017307340962109387\n",
      "    \n",
      "W :  [ 0.36882916  0.46028203  0.30312946  0.49869296]\n",
      "b :  [ 0.16690294  0.17362334  0.39720136  0.1870334 ]\n",
      "Loss at step 9000: 0.695391\n",
      "Minibatch accuracy: 96.7%\n",
      "Learning rate :  0.00016611955528863463\n",
      "    \n",
      "W :  [ 0.37546623  0.46176451  0.30639896  0.49898461]\n",
      "b :  [ 0.16854011  0.1788556   0.40323666  0.18599635]\n",
      "Loss at step 9500: 0.867974\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00015982744768905404\n",
      "    \n",
      "W :  [ 0.37418655  0.46414509  0.30885339  0.49935094]\n",
      "b :  [ 0.17223297  0.17994232  0.406299    0.18866754]\n",
      "Loss at step 10000: 0.638416\n",
      "Minibatch accuracy: 96.1%\n",
      "Learning rate :  0.0001541341132946451\n",
      "    \n",
      "W :  [ 0.37511635  0.46781242  0.31077611  0.49953386]\n",
      "b :  [ 0.17404929  0.18324403  0.40953499  0.188705  ]\n",
      "Loss at step 10500: 0.788208\n",
      "Minibatch accuracy: 95.8%\n",
      "Learning rate :  0.00014898257130119277\n",
      "    \n",
      "W :  [ 0.37741515  0.46947908  0.30923423  0.49528319]\n",
      "b :  [ 0.17466891  0.18399446  0.41284627  0.18911879]\n",
      "Loss at step 11000: 0.800658\n",
      "Minibatch accuracy: 95.9%\n",
      "Learning rate :  0.00014432126334493355\n",
      "    \n",
      "W :  [ 0.37939724  0.47038788  0.31345391  0.4954195 ]\n",
      "b :  [ 0.17536648  0.18645614  0.41301498  0.18832728]\n",
      "Loss at step 11500: 0.655003\n",
      "Minibatch accuracy: 96.1%\n",
      "Learning rate :  0.0001401035374891215\n",
      "    \n",
      "W :  [ 0.37831774  0.47175336  0.31155005  0.49564907]\n",
      "b :  [ 0.178212    0.18833011  0.41243967  0.18727687]\n",
      "Loss at step 12000: 0.709832\n",
      "Minibatch accuracy: 95.9%\n",
      "Learning rate :  0.00013628718131576502\n",
      "    \n",
      "W :  [ 0.37881228  0.47596753  0.30788314  0.4958103 ]\n",
      "b :  [ 0.1818186   0.19089767  0.41463187  0.19018142]\n",
      "Loss at step 12500: 0.649431\n",
      "Minibatch accuracy: 96.4%\n",
      "Learning rate :  0.00013283399944955952\n",
      "    \n",
      "W :  [ 0.37929562  0.48029462  0.30994022  0.49596187]\n",
      "b :  [ 0.18686564  0.19323348  0.41753039  0.19028814]\n",
      "Loss at step 13000: 0.600088\n",
      "Minibatch accuracy: 96.6%\n",
      "Learning rate :  0.00012970943128573357\n",
      "    \n",
      "W :  [ 0.37490615  0.48359087  0.31251413  0.49716437]\n",
      "b :  [ 0.18857266  0.1936681   0.41741055  0.19271541]\n",
      "Loss at step 13500: 0.692701\n",
      "Minibatch accuracy: 96.2%\n",
      "Learning rate :  0.0001268822050958999\n",
      "    \n",
      "W :  [ 0.37513894  0.48518234  0.31232759  0.49729279]\n",
      "b :  [ 0.18730268  0.19628437  0.42060134  0.19100413]\n",
      "Loss at step 14000: 0.587706\n",
      "Minibatch accuracy: 96.2%\n",
      "Learning rate :  0.0001243240250500872\n",
      "    \n",
      "W :  [ 0.37325162  0.48380205  0.31105876  0.49750498]\n",
      "b :  [ 0.18503387  0.19447866  0.42175683  0.19202912]\n",
      "Loss at step 14500: 0.543468\n",
      "Minibatch accuracy: 97.2%\n",
      "Learning rate :  0.0001220092880225629\n",
      "    \n",
      "W :  [ 0.37272683  0.48717022  0.31340525  0.49764627]\n",
      "b :  [ 0.1848285   0.19503103  0.42379591  0.19203469]\n",
      "Loss at step 15000: 0.443280\n",
      "Minibatch accuracy: 97.7%\n",
      "Learning rate :  0.00011991482734714559\n",
      "    \n",
      "Training Complete on MNIST Data\n",
      "Model saved in file: saved_models/mnist/CNN_SVHN_Mnist.ckpt\n"
     ]
    }
   ],
   "source": [
    "train_data = train_dataset\n",
    "label_data = train_labels\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)\n",
    "\n",
    "num_steps_1 = 15001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps_1):\n",
    "        #  learning rate decay\n",
    "        max_learning_rate = 0.0005\n",
    "        min_learning_rate = 0.0001\n",
    "\n",
    "        decay_speed = 5000.0\n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "        offset = (step * batch_size) % (label_data.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = label_data[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "        _, l, train_pred, W, b = session.run([train_step, cross_entropy, train_prediction, W_s, b_s], feed_dict=feed_dict)\n",
    "    \n",
    "        if (step % 500 == 0):\n",
    "            print('W : ', W)\n",
    "            print('b : ', b)\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % acc(train_pred, batch_labels[:,1:6]))\n",
    "            print('Learning rate : ', learning_rate)\n",
    "            print('    ')\n",
    "            \n",
    "    print('Training Complete on MNIST Data')\n",
    "    \n",
    "    save_path = model_saver.save(session, model_to_save)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "-------TEST--------\n",
      "Test-Accuracy  i :  0\n",
      "Test accuracy:  97.64\n",
      "       \n",
      "Test-Accuracy  i :  1\n",
      "Test accuracy:  97.62\n",
      "       \n",
      "Test-Accuracy  i :  2\n",
      "Test accuracy:  97.44\n",
      "       \n",
      "Test-Accuracy  i :  3\n",
      "Test accuracy:  97.34\n",
      "       \n",
      "Test-Accuracy  i :  4\n",
      "Test accuracy:  97.72\n",
      "       \n",
      "Test-Accuracy  i :  5\n",
      "Test accuracy:  97.2\n",
      "       \n",
      "Test-Accuracy  i :  6\n",
      "Test accuracy:  97.24\n",
      "       \n",
      "Test-Accuracy  i :  7\n",
      "Test accuracy:  97.54\n",
      "       \n",
      "Test-Accuracy  i :  8\n",
      "Test accuracy:  97.32\n",
      "       \n",
      "-----  FINAL  ------\n",
      "Final Test Set Accuracy :  97.45\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_svhn) as session: \n",
    "    print('Initialized')\n",
    "    batch = 1000\n",
    "    \n",
    "    test_acc = list()\n",
    "    print('-------TEST--------')\n",
    "    test_no = int(test_labels.shape[0] / batch)\n",
    "    for i in range(test_no - 1):\n",
    "        model_saver.restore(session, model_to_save)\n",
    "        data = test_dataset[i*batch:(i+1)*batch]\n",
    "        labels = test_labels[i*batch:(i+1)*batch]\n",
    "        \n",
    "        _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : data, Y_ : labels, pkeep : 1.0, alpha : 0.002})\n",
    "        accuracy = acc(predictions, labels[:,1:6])\n",
    "        test_acc.append(accuracy)\n",
    "        \n",
    "        print('Test-Accuracy', ' i : ', i)\n",
    "        print('Test accuracy: ', accuracy)\n",
    "        print('       ')\n",
    "        \n",
    "    test_avg = mean(test_acc)\n",
    "    \n",
    "    print('-----  FINAL  ------')\n",
    "    print('Final Test Set Accuracy : ',\"%.2f\" % test_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  (80000, 32, 96, 1)   test :  (80000, 6)\n",
      "Initialized\n",
      "[[[[ -3.30879651e-02  -5.12703434e-02   1.42844215e-01  -5.47783524e-02\n",
      "      8.84761661e-02   1.10916555e-01]]\n",
      "\n",
      "  [[ -2.15370674e-03   2.41771474e-01  -5.45323901e-02  -4.23194803e-02\n",
      "     -2.17045486e-01   1.27395347e-01]]\n",
      "\n",
      "  [[ -1.88058857e-02   1.46964744e-01  -8.12237486e-02   2.89021395e-02\n",
      "     -6.50788918e-02  -2.24261172e-02]]\n",
      "\n",
      "  [[  1.60844624e-02  -7.15449899e-02  -4.08643186e-02  -2.21087739e-01\n",
      "      4.56903353e-02   3.58434650e-03]]\n",
      "\n",
      "  [[ -8.40735957e-02   1.06903337e-01  -4.67394106e-02  -1.88464150e-02\n",
      "     -1.26698846e-02  -7.79727623e-02]]\n",
      "\n",
      "  [[ -1.77681237e-01  -3.68454531e-02  -1.02018006e-01   7.14501441e-02\n",
      "      1.30029410e-01   5.56466244e-02]]]\n",
      "\n",
      "\n",
      " [[[ -2.71276757e-02   3.41473408e-02  -9.29100960e-02   1.13312136e-02\n",
      "     -3.74720469e-02  -8.92096087e-02]]\n",
      "\n",
      "  [[ -1.04815975e-01  -1.64084226e-01   5.61872423e-02  -1.23486094e-01\n",
      "     -7.20127672e-02   1.17528066e-01]]\n",
      "\n",
      "  [[  1.77528933e-02  -5.94391786e-02  -1.11129582e-02  -7.14409724e-02\n",
      "      7.34919757e-02  -3.91570153e-03]]\n",
      "\n",
      "  [[  5.56371063e-02   3.04329023e-02   1.03744820e-01  -1.76192269e-01\n",
      "      6.90662414e-02  -2.29787380e-02]]\n",
      "\n",
      "  [[  6.03959225e-02  -1.73227921e-01   7.29037598e-02  -2.12766096e-01\n",
      "      1.97563823e-02  -1.18910134e-01]]\n",
      "\n",
      "  [[  6.13217466e-02   3.36743258e-02  -4.73884717e-02   1.29071400e-01\n",
      "     -8.59913677e-02   8.91362801e-02]]]\n",
      "\n",
      "\n",
      " [[[ -6.31166548e-02  -1.64733499e-01   9.71897542e-02  -5.60786211e-05\n",
      "      1.27552226e-01   3.22353430e-02]]\n",
      "\n",
      "  [[  3.94159928e-02  -1.55489847e-01  -2.77941842e-02   1.53201580e-01\n",
      "     -1.10516287e-01   1.05037630e-01]]\n",
      "\n",
      "  [[ -3.08873635e-02  -1.59230381e-01   2.22551171e-02  -5.57973795e-03\n",
      "     -8.94102156e-02   3.25445570e-02]]\n",
      "\n",
      "  [[  2.95227766e-03  -1.15795158e-01  -1.84732974e-02   9.09298062e-02\n",
      "      4.40252982e-02  -7.02771097e-02]]\n",
      "\n",
      "  [[  7.19234049e-02  -9.36971530e-02   5.92867769e-02  -1.41653299e-01\n",
      "      5.90069070e-02   5.61647816e-03]]\n",
      "\n",
      "  [[  1.65337145e-01  -2.53713428e-04   9.11103040e-02   5.65883704e-02\n",
      "     -4.46415730e-02  -1.04852520e-01]]]\n",
      "\n",
      "\n",
      " [[[  4.24676761e-02   7.09624812e-02   7.23402798e-02   1.64149076e-01\n",
      "     -1.81915015e-01   3.24755395e-03]]\n",
      "\n",
      "  [[ -1.42040655e-01  -8.66474658e-02   7.94416741e-02   1.37736112e-01\n",
      "     -6.75352663e-02   9.94944349e-02]]\n",
      "\n",
      "  [[  1.27009854e-01   2.95599233e-02   3.71642038e-02  -2.79736929e-02\n",
      "     -1.95249338e-02   2.84956712e-02]]\n",
      "\n",
      "  [[  2.31748465e-02   8.58402997e-02  -7.93243423e-02  -1.68907642e-02\n",
      "      8.27133358e-02  -2.72169635e-02]]\n",
      "\n",
      "  [[  1.44979402e-01   1.06105760e-01  -4.90322569e-03  -2.53456652e-01\n",
      "     -4.82330273e-04  -1.15886137e-01]]\n",
      "\n",
      "  [[ -1.47141945e-02  -9.36300401e-03   9.08992365e-02   5.42278737e-02\n",
      "      4.48596291e-02   4.30456623e-02]]]\n",
      "\n",
      "\n",
      " [[[ -1.50206849e-01   2.13670768e-02  -9.92232282e-03  -6.93155602e-02\n",
      "     -1.00982286e-01  -5.67884445e-02]]\n",
      "\n",
      "  [[ -5.20171151e-02   8.91446620e-02   3.72393727e-02   1.46602035e-01\n",
      "     -1.59953535e-02   5.34378178e-02]]\n",
      "\n",
      "  [[  6.34316728e-02   8.55601858e-03   2.96666138e-02   7.35782683e-02\n",
      "      9.77206975e-02   1.32541731e-02]]\n",
      "\n",
      "  [[  1.07767947e-01   6.22530980e-03   1.31328374e-01   8.42950214e-03\n",
      "      1.69212550e-01  -1.20636873e-01]]\n",
      "\n",
      "  [[  4.89224494e-02   1.18981011e-01  -2.60753068e-03  -1.70704916e-01\n",
      "      3.35501395e-02   1.53837521e-02]]\n",
      "\n",
      "  [[  6.47341907e-02   6.83763698e-02  -6.52161241e-02   1.43754572e-01\n",
      "     -7.28967935e-02   3.61728780e-02]]]\n",
      "\n",
      "\n",
      " [[[  1.83674619e-02   9.23275203e-02   3.79898614e-04  -1.94080383e-01\n",
      "      6.64391890e-02   4.35293792e-03]]\n",
      "\n",
      "  [[ -1.35843441e-01   5.12344539e-02  -6.84679206e-03  -3.72721642e-01\n",
      "      1.24084160e-01   7.87303075e-02]]\n",
      "\n",
      "  [[ -1.11750774e-02   7.08979368e-02   5.33540733e-02  -2.16453433e-01\n",
      "      1.62635818e-01  -1.42259628e-01]]\n",
      "\n",
      "  [[ -6.98466897e-02   3.96963693e-02  -2.35406719e-02  -1.20898776e-01\n",
      "     -2.74982229e-02  -1.42920032e-01]]\n",
      "\n",
      "  [[  1.06631897e-01  -4.91145663e-02   1.65753849e-02  -1.85097128e-01\n",
      "      1.43306898e-02  -1.44294560e-01]]\n",
      "\n",
      "  [[ -1.39072202e-02  -5.24736270e-02   7.55591094e-02   1.91529598e-02\n",
      "     -1.42309278e-01  -9.62895900e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_dataset\n",
    "label_data = train_labels\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)\n",
    "\n",
    "num_steps_1 = 25001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "    model_saver.restore(session, model_to_save)\n",
    "    print('Initialized')\n",
    "    \n",
    "    W1 = session.run(W1)\n",
    "    print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
