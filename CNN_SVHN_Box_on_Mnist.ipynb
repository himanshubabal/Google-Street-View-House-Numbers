{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (230070, 32, 96, 1) (230070, 6)\n",
      "Test set (13068, 32, 96, 1) (13068, 6)\n",
      "Validation set (5684, 32, 96, 1) (5684, 6)\n"
     ]
    }
   ],
   "source": [
    "hdf_file = 'datasets/pickles/SVHN_multi_box.hdf5'\n",
    "\n",
    "hdf = h5py.File(hdf_file,'r')\n",
    "train_dataset = hdf['train_images'][:]\n",
    "train_labels = hdf['train_labels'][:]\n",
    "test_dataset = hdf['test_images'][:]\n",
    "test_labels = hdf['test_labels'][:]\n",
    "valid_dataset = hdf['valid_images'][:]\n",
    "valid_labels = hdf['valid_labels'][:]\n",
    "            \n",
    "hdf.close()    \n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.astype(np.float32)\n",
    "test_dataset = test_dataset.astype(np.float32)\n",
    "valid_dataset = valid_dataset.astype(np.float32)\n",
    "\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "test_labels = test_labels.astype(np.int32)\n",
    "valid_labels = valid_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_to_save = \"saved_models/combined/box_on_mnist/CNN_SVHN_Box_on_Mnist.ckpt\"\n",
    "saved_mnist_model = \"saved_models/mnist/CNN_SVHN_Mnist.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_svhn = tf.Graph()\n",
    "\n",
    "with graph_svhn.as_default():\n",
    "    HEIGHT = 32\n",
    "    WIDTH = 32*3\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, 1])\n",
    "    Y_ = tf.placeholder(tf.int32, [None, 6])\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Dropout Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # 5 Layers and their no of neurons\n",
    "    # 3 Convolutional Layers and a fully connected layer\n",
    "    K = 6     # First Conv Layer with depth 6\n",
    "    L = 12     # Second Conv Layer with depth 12\n",
    "    M = 24    # Third Conv layer with depth 24\n",
    "    N = 200   # Fourth Fully Connected layer with 200 neurons\n",
    "    # Last one will be softmax layer with 10 output channels\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1), name=\"W1\")    # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]), name=\"B1\")\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1), name=\"W2\")\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]), name=\"B2\")\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1), name=\"W3\")\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]), name=\"B3\")\n",
    "    \n",
    "    W5_1 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_1\")\n",
    "    B5_1 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_1\")\n",
    "    \n",
    "    W5_2 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_2\")\n",
    "    B5_2 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_2\")\n",
    "    \n",
    "    W5_3 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_3\")\n",
    "    B5_3 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_3\")\n",
    "    \n",
    "    W5_4 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_4\")\n",
    "    B5_4 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_4\")\n",
    "    \n",
    "    W5_5 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_5\")\n",
    "    B5_5 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_5\")\n",
    "    \n",
    "    # Model\n",
    "    stride = 1  # output is 32x96\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    \n",
    "    stride = 2  # output is 16x48\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    \n",
    "    stride = 2  # output is 8x24\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    shape = Y3.get_shape().as_list()\n",
    "    YY = tf.reshape(Y3, shape=[-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([shape[1] * shape[2] * shape[3], N], stddev=0.1), name=\"W4\")\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]), name=\"B4\")\n",
    "\n",
    "    Y4 = tf.sigmoid(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    \n",
    "    Ylogits_1 = tf.matmul(YY4, W5_1) + B5_1\n",
    "    Ylogits_2 = tf.matmul(YY4, W5_2) + B5_2\n",
    "    Ylogits_3 = tf.matmul(YY4, W5_3) + B5_3\n",
    "    Ylogits_4 = tf.matmul(YY4, W5_4) + B5_4\n",
    "    Ylogits_5 = tf.matmul(YY4, W5_5) + B5_5   \n",
    "    ## ('Ylogits_1 shape : ', [None, 11])\n",
    "    \n",
    "    Y_1 = tf.nn.softmax(Ylogits_1)\n",
    "    Y_2 = tf.nn.softmax(Ylogits_2)\n",
    "    Y_3 = tf.nn.softmax(Ylogits_3)\n",
    "    Y_4 = tf.nn.softmax(Ylogits_4)\n",
    "    Y_5 = tf.nn.softmax(Ylogits_5)\n",
    "   \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_1, Y_[:,1])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_2, Y_[:,2])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_3, Y_[:,3])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_4, Y_[:,4])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_5, Y_[:,5]))\n",
    "\n",
    "    train_prediction = tf.pack([Y_1, Y_2, Y_3, Y_4, Y_5])\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "    \n",
    "    W_s = tf.pack([tf.reduce_max(tf.abs(W1)),tf.reduce_max(tf.abs(W2)),tf.reduce_max(tf.abs(W3)),tf.reduce_max(tf.abs(W4))])\n",
    "    b_s = tf.pack([tf.reduce_max(tf.abs(B1)),tf.reduce_max(tf.abs(B2)),tf.reduce_max(tf.abs(B3)),tf.reduce_max(tf.abs(B4))])\n",
    "    \n",
    "    model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  (230070, 32, 96, 1)   test :  (230070, 6)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_dataset\n",
    "label_data = train_labels\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "W :  [ 0.37272164  0.48718435  0.31338772  0.49764675]\n",
      "b :  [ 0.18485929  0.1950376   0.4238168   0.19206028]\n",
      "Loss at step 0: 25.621204\n",
      "Minibatch accuracy: 21.6%\n",
      "Learning rate :  0.0005\n",
      "    \n",
      "W :  [ 0.26441282  0.43882173  0.32453412  0.45159847]\n",
      "b :  [ 0.10925158  0.25092804  0.45009527  0.21947894]\n",
      "Loss at step 500: 5.465186\n",
      "Minibatch accuracy: 64.7%\n",
      "Learning rate :  0.00046193496721438383\n",
      "    \n",
      "W :  [ 0.28102431  0.42248625  0.33415824  0.45742369]\n",
      "b :  [ 0.10620536  0.27386266  0.44928122  0.22840211]\n",
      "Loss at step 1000: 4.387840\n",
      "Minibatch accuracy: 73.0%\n",
      "Learning rate :  0.00042749230123119273\n",
      "    \n",
      "W :  [ 0.2899048   0.40556544  0.34720343  0.49042353]\n",
      "b :  [ 0.12619545  0.28770003  0.4479593   0.23237409]\n",
      "Loss at step 1500: 3.684948\n",
      "Minibatch accuracy: 77.0%\n",
      "Learning rate :  0.00039632728827268716\n",
      "    \n",
      "W :  [ 0.2954677   0.3880918   0.36182961  0.5223251 ]\n",
      "b :  [ 0.13295738  0.29047504  0.44519916  0.2308701 ]\n",
      "Loss at step 2000: 3.584943\n",
      "Minibatch accuracy: 76.7%\n",
      "Learning rate :  0.00036812801841425575\n",
      "    \n",
      "W :  [ 0.2966004   0.37749857  0.37346125  0.52978075]\n",
      "b :  [ 0.13818684  0.29868427  0.44429764  0.23082253]\n",
      "Loss at step 2500: 2.858480\n",
      "Minibatch accuracy: 82.2%\n",
      "Learning rate :  0.0003426122638850534\n",
      "    \n",
      "W :  [ 0.29789674  0.38836882  0.37869215  0.53910553]\n",
      "b :  [ 0.14150278  0.30506429  0.44192046  0.22925858]\n",
      "Loss at step 3000: 2.990290\n",
      "Minibatch accuracy: 81.7%\n",
      "Learning rate :  0.00031952465443761056\n",
      "    \n",
      "W :  [ 0.29942498  0.40170631  0.38463527  0.54956901]\n",
      "b :  [ 0.13418457  0.30653986  0.43996102  0.22777396]\n",
      "Loss at step 3500: 2.447465\n",
      "Minibatch accuracy: 86.2%\n",
      "Learning rate :  0.00029863412151656383\n",
      "    \n",
      "W :  [ 0.30073735  0.41018242  0.3826836   0.54731393]\n",
      "b :  [ 0.13980198  0.30904105  0.44100052  0.22683866]\n",
      "Loss at step 4000: 2.470887\n",
      "Minibatch accuracy: 84.2%\n",
      "Learning rate :  0.00027973158564688865\n",
      "    \n",
      "W :  [ 0.30169275  0.41720253  0.37460536  0.54285109]\n",
      "b :  [ 0.1353046   0.31660134  0.44028932  0.22411685]\n",
      "Loss at step 4500: 2.050688\n",
      "Minibatch accuracy: 87.8%\n",
      "Learning rate :  0.0002626278638962397\n",
      "    \n",
      "W :  [ 0.30258316  0.42861828  0.37356576  0.56448811]\n",
      "b :  [ 0.13250947  0.3202047   0.44336513  0.22410104]\n",
      "Loss at step 5000: 2.005021\n",
      "Minibatch accuracy: 88.1%\n",
      "Learning rate :  0.00024715177646857697\n",
      "    \n",
      "W :  [ 0.30240342  0.43200362  0.37319323  0.57799011]\n",
      "b :  [ 0.13003233  0.32003596  0.44684255  0.22333397]\n",
      "Loss at step 5500: 1.827807\n",
      "Minibatch accuracy: 88.9%\n",
      "Learning rate :  0.0002331484334792318\n",
      "    \n",
      "W :  [ 0.3034406   0.43451867  0.36779794  0.59984142]\n",
      "b :  [ 0.12771903  0.32447776  0.44781771  0.22201017]\n",
      "Loss at step 6000: 2.131408\n",
      "Minibatch accuracy: 87.7%\n",
      "Learning rate :  0.00022047768476488088\n",
      "    \n",
      "W :  [ 0.30393317  0.43340141  0.36448327  0.62284803]\n",
      "b :  [ 0.12672195  0.32666042  0.45140681  0.22196881]\n",
      "Loss at step 6500: 1.815848\n",
      "Minibatch accuracy: 89.5%\n",
      "Learning rate :  0.00020901271721360503\n",
      "    \n",
      "W :  [ 0.30388364  0.43928209  0.36347932  0.6308558 ]\n",
      "b :  [ 0.12327928  0.32927272  0.45665559  0.21935657]\n",
      "Loss at step 7000: 2.313634\n",
      "Minibatch accuracy: 86.1%\n",
      "Learning rate :  0.0001986387855766426\n",
      "    \n",
      "W :  [ 0.30426666  0.43912196  0.36495498  0.63744622]\n",
      "b :  [ 0.12493005  0.33006743  0.45850334  0.2233413 ]\n",
      "Loss at step 7500: 1.779517\n",
      "Minibatch accuracy: 88.8%\n",
      "Learning rate :  0.00018925206405937195\n",
      "    \n",
      "W :  [ 0.30496669  0.44040567  0.37496197  0.65319085]\n",
      "b :  [ 0.12539551  0.33059892  0.46208891  0.22388491]\n",
      "Loss at step 8000: 1.298902\n",
      "Minibatch accuracy: 93.0%\n",
      "Learning rate :  0.00018075860719786216\n",
      "    \n",
      "W :  [ 0.30522791  0.444325    0.37565395  0.66055793]\n",
      "b :  [ 0.12365161  0.33380058  0.46602932  0.22125824]\n",
      "Loss at step 8500: 1.551679\n",
      "Minibatch accuracy: 89.8%\n",
      "Learning rate :  0.00017307340962109387\n",
      "    \n",
      "W :  [ 0.30460379  0.44498548  0.37774095  0.66715723]\n",
      "b :  [ 0.12200427  0.33339626  0.46990985  0.2237809 ]\n",
      "Loss at step 9000: 1.878839\n",
      "Minibatch accuracy: 90.0%\n",
      "Learning rate :  0.00016611955528863463\n",
      "    \n",
      "W :  [ 0.30654937  0.44625786  0.37925556  0.67200708]\n",
      "b :  [ 0.12311269  0.33513439  0.47129938  0.22518265]\n",
      "Loss at step 9500: 1.369168\n",
      "Minibatch accuracy: 90.8%\n",
      "Learning rate :  0.00015982744768905404\n",
      "    \n",
      "W :  [ 0.30658436  0.44513255  0.38140449  0.6802789 ]\n",
      "b :  [ 0.12352873  0.33449429  0.47393134  0.22464788]\n",
      "Loss at step 10000: 1.707848\n",
      "Minibatch accuracy: 89.7%\n",
      "Learning rate :  0.0001541341132946451\n",
      "    \n",
      "W :  [ 0.30719978  0.44948414  0.38434678  0.68824011]\n",
      "b :  [ 0.12415467  0.33620462  0.47684097  0.22152725]\n",
      "Loss at step 10500: 1.418866\n",
      "Minibatch accuracy: 91.7%\n",
      "Learning rate :  0.00014898257130119277\n",
      "    \n",
      "W :  [ 0.30711627  0.44908771  0.38473633  0.6948275 ]\n",
      "b :  [ 0.12369554  0.33590877  0.47876841  0.22383831]\n",
      "Loss at step 11000: 1.134342\n",
      "Minibatch accuracy: 93.3%\n",
      "Learning rate :  0.00014432126334493355\n",
      "    \n",
      "W :  [ 0.30788046  0.44938967  0.3878113   0.69779336]\n",
      "b :  [ 0.12514812  0.33655709  0.48026085  0.22527716]\n",
      "Loss at step 11500: 1.483116\n",
      "Minibatch accuracy: 90.8%\n",
      "Learning rate :  0.0001401035374891215\n",
      "    \n",
      "W :  [ 0.30823123  0.45011789  0.38495982  0.70032281]\n",
      "b :  [ 0.12432011  0.33736449  0.48276344  0.22380516]\n",
      "Loss at step 12000: 1.384844\n",
      "Minibatch accuracy: 91.1%\n",
      "Learning rate :  0.00013628718131576502\n",
      "    \n",
      "W :  [ 0.30772907  0.45157689  0.38578516  0.70371604]\n",
      "b :  [ 0.12358519  0.33846587  0.48594934  0.22249982]\n",
      "Loss at step 12500: 1.822462\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate :  0.00013283399944955952\n",
      "    \n",
      "W :  [ 0.30854076  0.45328146  0.38527185  0.7044223 ]\n",
      "b :  [ 0.12565112  0.33761764  0.48704377  0.22728859]\n",
      "Loss at step 13000: 1.508036\n",
      "Minibatch accuracy: 90.9%\n",
      "Learning rate :  0.00012970943128573357\n",
      "    \n",
      "W :  [ 0.30900881  0.45242187  0.38961226  0.70860112]\n",
      "b :  [ 0.12499138  0.33894184  0.48832619  0.22695208]\n",
      "Loss at step 13500: 1.195781\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate :  0.0001268822050958999\n",
      "    \n",
      "W :  [ 0.30955064  0.4550471   0.38834721  0.71221441]\n",
      "b :  [ 0.12637566  0.34074867  0.49117672  0.22641189]\n",
      "Loss at step 14000: 1.091391\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate :  0.0001243240250500872\n",
      "    \n",
      "W :  [ 0.30970189  0.45525762  0.39133942  0.71717733]\n",
      "b :  [ 0.12591688  0.33983761  0.49194866  0.22204153]\n",
      "Loss at step 14500: 1.163506\n",
      "Minibatch accuracy: 92.8%\n",
      "Learning rate :  0.0001220092880225629\n",
      "    \n",
      "W :  [ 0.31035089  0.45510548  0.39341792  0.71744937]\n",
      "b :  [ 0.12728232  0.33994031  0.49301875  0.22252509]\n",
      "Loss at step 15000: 1.118377\n",
      "Minibatch accuracy: 92.5%\n",
      "Learning rate :  0.00011991482734714559\n",
      "    \n",
      "W :  [ 0.31104955  0.45256206  0.39141357  0.7217676 ]\n",
      "b :  [ 0.12814726  0.34150419  0.49459213  0.22265846]\n",
      "Loss at step 15500: 1.091277\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate :  0.00011801968095742312\n",
      "    \n",
      "W :  [ 0.31091422  0.45529187  0.39247054  0.71872157]\n",
      "b :  [ 0.12669389  0.34178171  0.49751619  0.22556315]\n",
      "Loss at step 16000: 1.395697\n",
      "Minibatch accuracy: 89.8%\n",
      "Learning rate :  0.00011630488159134649\n",
      "    \n",
      "W :  [ 0.31117323  0.45545039  0.39266345  0.72252643]\n",
      "b :  [ 0.12819716  0.34254345  0.49696842  0.22559869]\n",
      "Loss at step 16500: 1.228318\n",
      "Minibatch accuracy: 93.0%\n",
      "Learning rate :  0.00011475326696049602\n",
      "    \n",
      "W :  [ 0.31147614  0.45464185  0.39391235  0.72527558]\n",
      "b :  [ 0.1290216   0.34256604  0.49882779  0.22456843]\n",
      "Loss at step 17000: 0.944814\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00011334930798413044\n",
      "    \n",
      "W :  [ 0.31157893  0.45629466  0.39454237  0.72536993]\n",
      "b :  [ 0.12949017  0.34447688  0.50085783  0.22354443]\n",
      "Loss at step 17500: 0.830926\n",
      "Minibatch accuracy: 94.5%\n",
      "Learning rate :  0.0001120789533689274\n",
      "    \n",
      "W :  [ 0.31166649  0.45748451  0.39889714  0.73028427]\n",
      "b :  [ 0.12790695  0.34469849  0.50292265  0.22378425]\n",
      "Loss at step 18000: 1.209359\n",
      "Minibatch accuracy: 91.9%\n",
      "Learning rate :  0.00011092948897891703\n",
      "    \n",
      "W :  [ 0.31265873  0.45927182  0.39818847  0.72894168]\n",
      "b :  [ 0.13001826  0.34408998  0.50289226  0.22441132]\n",
      "Loss at step 18500: 0.964900\n",
      "Minibatch accuracy: 95.2%\n",
      "Learning rate :  0.00010988941058813576\n",
      "    \n",
      "W :  [ 0.31286904  0.45824102  0.39885715  0.73117483]\n",
      "b :  [ 0.13117039  0.34480757  0.50391245  0.22436504]\n",
      "Loss at step 19000: 0.952495\n",
      "Minibatch accuracy: 94.1%\n",
      "Learning rate :  0.00010894830874246625\n",
      "    \n",
      "W :  [ 0.31303924  0.46141958  0.39801887  0.7303412 ]\n",
      "b :  [ 0.13109338  0.34596992  0.50593925  0.22108337]\n",
      "Loss at step 19500: 0.777805\n",
      "Minibatch accuracy: 95.5%\n",
      "Learning rate :  0.00010809676457832176\n",
      "    \n",
      "W :  [ 0.31323665  0.46159276  0.39697734  0.73205614]\n",
      "b :  [ 0.13101967  0.3462688   0.50686604  0.22235879]\n",
      "Loss at step 20000: 0.941204\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00010732625555549367\n",
      "    \n",
      "W :  [ 0.31345755  0.460823    0.39927557  0.73397553]\n",
      "b :  [ 0.13211933  0.34730515  0.50804847  0.22185665]\n",
      "Loss at step 20500: 0.704692\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate :  0.0001066290701607045\n",
      "    \n",
      "W :  [ 0.3139894   0.46177596  0.39935097  0.73610628]\n",
      "b :  [ 0.13248448  0.34839791  0.50964338  0.22203481]\n",
      "Loss at step 21000: 1.104269\n",
      "Minibatch accuracy: 94.4%\n",
      "Learning rate :  0.00010599823072819108\n",
      "    \n",
      "W :  [ 0.31334767  0.46384341  0.39969507  0.73956585]\n",
      "b :  [ 0.13117751  0.35093877  0.5121153   0.22212943]\n",
      "Loss at step 21500: 0.876464\n",
      "Minibatch accuracy: 93.9%\n",
      "Learning rate :  0.00010542742360488037\n",
      "    \n",
      "W :  [ 0.3138794   0.46357214  0.40289605  0.73755676]\n",
      "b :  [ 0.1332615   0.34888107  0.51070988  0.22409563]\n",
      "Loss at step 22000: 0.965344\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate :  0.00010491093596122738\n",
      "    \n",
      "W :  [ 0.31438056  0.46488935  0.40327683  0.73759419]\n",
      "b :  [ 0.13364206  0.34926391  0.51235938  0.22134809]\n",
      "Loss at step 22500: 0.741848\n",
      "Minibatch accuracy: 95.6%\n",
      "Learning rate :  0.00010444359861529693\n",
      "    \n",
      "W :  [ 0.31444135  0.4659273   0.40257004  0.73573679]\n",
      "b :  [ 0.13361099  0.35090056  0.51448703  0.22062559]\n",
      "Loss at step 23000: 0.880633\n",
      "Minibatch accuracy: 94.4%\n",
      "Learning rate :  0.00010402073429785344\n",
      "    \n",
      "W :  [ 0.31429499  0.46697783  0.4029679   0.7382493 ]\n",
      "b :  [ 0.1332925   0.35122624  0.51518244  0.2209394 ]\n",
      "Loss at step 23500: 0.892730\n",
      "Minibatch accuracy: 94.5%\n",
      "Learning rate :  0.00010363811084067834\n",
      "    \n",
      "W :  [ 0.31481028  0.46915334  0.40544575  0.73616993]\n",
      "b :  [ 0.13441625  0.35104963  0.51595104  0.22127192]\n",
      "Loss at step 24000: 0.957823\n",
      "Minibatch accuracy: 94.7%\n",
      "Learning rate :  0.00010329189881960801\n",
      "    \n",
      "W :  [ 0.31558165  0.46865469  0.40705153  0.73546499]\n",
      "b :  [ 0.13539064  0.35300481  0.51727778  0.22081046]\n",
      "Loss at step 24500: 0.848933\n",
      "Minibatch accuracy: 95.8%\n",
      "Learning rate :  0.00010297863322836974\n",
      "    \n",
      "W :  [ 0.31493953  0.47088644  0.40761155  0.73635745]\n",
      "b :  [ 0.13409339  0.35369703  0.51956576  0.22093345]\n",
      "Loss at step 25000: 1.591357\n",
      "Minibatch accuracy: 90.2%\n",
      "Learning rate :  0.00010269517879963419\n",
      "    \n",
      "W :  [ 0.31556049  0.46986836  0.40884954  0.73816782]\n",
      "b :  [ 0.13540961  0.3534407   0.51921481  0.22122267]\n",
      "Loss at step 25500: 0.920659\n",
      "Minibatch accuracy: 93.9%\n",
      "Learning rate :  0.00010243869862620625\n",
      "    \n",
      "W :  [ 0.3163023   0.47135064  0.40940732  0.73782343]\n",
      "b :  [ 0.13572261  0.35318482  0.5209105   0.22095828]\n",
      "Loss at step 26000: 0.696950\n",
      "Minibatch accuracy: 96.7%\n",
      "Learning rate :  0.00010220662576830431\n",
      "    \n",
      "W :  [ 0.31601107  0.4709222   0.40889999  0.73566937]\n",
      "b :  [ 0.13653336  0.35501888  0.52175224  0.22169492]\n",
      "Loss at step 26500: 0.792578\n",
      "Minibatch accuracy: 95.6%\n",
      "Learning rate :  0.00010199663756276409\n",
      "    \n",
      "W :  [ 0.3161459   0.4729057   0.41129959  0.7411738 ]\n",
      "b :  [ 0.13524774  0.355479    0.52336299  0.2221193 ]\n",
      "Loss at step 27000: 0.710812\n",
      "Minibatch accuracy: 95.6%\n",
      "Learning rate :  0.00010180663237704508\n",
      "    \n",
      "W :  [ 0.31701562  0.47372025  0.41100779  0.73771793]\n",
      "b :  [ 0.1366386   0.35468796  0.52389359  0.22213113]\n",
      "Loss at step 27500: 0.788832\n",
      "Minibatch accuracy: 95.5%\n",
      "Learning rate :  0.00010163470857538563\n",
      "    \n",
      "W :  [ 0.31698203  0.47279939  0.41308826  0.73740876]\n",
      "b :  [ 0.13863152  0.35555193  0.52563888  0.22179031]\n",
      "Loss at step 28000: 1.018768\n",
      "Minibatch accuracy: 93.9%\n",
      "Learning rate :  0.00010147914548659317\n",
      "    \n",
      "W :  [ 0.3169277   0.47466275  0.41279906  0.74007303]\n",
      "b :  [ 0.13724582  0.35665935  0.52644426  0.22212367]\n",
      "Loss at step 28500: 0.693450\n",
      "Minibatch accuracy: 95.8%\n",
      "Learning rate :  0.00010133838618298851\n",
      "    \n",
      "W :  [ 0.31716457  0.47508636  0.41289592  0.74185055]\n",
      "b :  [ 0.13762732  0.35806763  0.52803242  0.22267772]\n",
      "Loss at step 29000: 1.010429\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate :  0.00010121102189815033\n",
      "    \n",
      "W :  [ 0.31813619  0.47621489  0.41374344  0.74411947]\n",
      "b :  [ 0.13938759  0.35869074  0.52830625  0.22294553]\n",
      "Loss at step 29500: 0.733931\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate :  0.00010109577792750736\n",
      "    \n",
      "W :  [ 0.31813723  0.47756305  0.41438183  0.74341893]\n",
      "b :  [ 0.13969998  0.35882199  0.52949095  0.22330207]\n",
      "Loss at step 30000: 0.927485\n",
      "Minibatch accuracy: 94.1%\n",
      "Learning rate :  0.00010099150087066655\n",
      "    \n",
      "Training Complete on MNIST Data\n",
      "Model saved in file: saved_models/combined/box_on_mnist/CNN_SVHN_Box_on_Mnist.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps_1 = 30001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "#     tf.global_variables_initializer().run()\n",
    "    model_saver.restore(session, saved_mnist_model)\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps_1):\n",
    "        #  learning rate decay\n",
    "        max_learning_rate = 0.0005\n",
    "        min_learning_rate = 0.0001\n",
    "\n",
    "        decay_speed = 5000.0\n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "        offset = (step * batch_size) % (label_data.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = label_data[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "        _, l, train_pred, W, b = session.run([train_step, cross_entropy, train_prediction, W_s, b_s], feed_dict=feed_dict)\n",
    "    \n",
    "        if (step % 500 == 0):\n",
    "            print('W : ', W)\n",
    "            print('b : ', b)\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % acc(train_pred, batch_labels[:,1:6]))\n",
    "            print('Learning rate : ', learning_rate)\n",
    "            print('    ')\n",
    "            \n",
    "    print('Training Complete on MNIST Data')\n",
    "    \n",
    "    save_path = model_saver.save(session, model_to_save)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "-------TEST--------\n",
      "Test-Accuracy  i :  0\n",
      "Test accuracy:  93.3\n",
      "       \n",
      "Test-Accuracy  i :  1\n",
      "Test accuracy:  93.5\n",
      "       \n",
      "Test-Accuracy  i :  2\n",
      "Test accuracy:  93.46\n",
      "       \n",
      "Test-Accuracy  i :  3\n",
      "Test accuracy:  93.54\n",
      "       \n",
      "Test-Accuracy  i :  4\n",
      "Test accuracy:  93.62\n",
      "       \n",
      "Test-Accuracy  i :  5\n",
      "Test accuracy:  93.8\n",
      "       \n",
      "Test-Accuracy  i :  6\n",
      "Test accuracy:  93.24\n",
      "       \n",
      "Test-Accuracy  i :  7\n",
      "Test accuracy:  93.5\n",
      "       \n",
      "Test-Accuracy  i :  8\n",
      "Test accuracy:  93.14\n",
      "       \n",
      "Test-Accuracy  i :  9\n",
      "Test accuracy:  93.48\n",
      "       \n",
      "Test-Accuracy  i :  10\n",
      "Test accuracy:  93.34\n",
      "       \n",
      "Test-Accuracy  i :  11\n",
      "Test accuracy:  92.72\n",
      "       \n",
      "-----VALIDIDATION------\n",
      "Valid-Accuracy  i :  0\n",
      "Valid accuracy:  93.8\n",
      "        \n",
      "Valid-Accuracy  i :  1\n",
      "Valid accuracy:  93.56\n",
      "        \n",
      "Valid-Accuracy  i :  2\n",
      "Valid accuracy:  92.36\n",
      "        \n",
      "Valid-Accuracy  i :  3\n",
      "Valid accuracy:  91.84\n",
      "        \n",
      "-----  FINAL  ------\n",
      "Final Test Set Accuracy :  93.39\n",
      "Final Validation Set Accuracy :  92.89\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_svhn) as session: \n",
    "    print('Initialized')\n",
    "    batch = 1000\n",
    "    \n",
    "    test_acc = list()\n",
    "    print('-------TEST--------')\n",
    "    test_no = int(test_labels.shape[0] / batch)\n",
    "    for i in range(test_no - 1):\n",
    "        model_saver.restore(session, model_to_save)\n",
    "        data = test_dataset[i*batch:(i+1)*batch]\n",
    "        labels = test_labels[i*batch:(i+1)*batch]\n",
    "        \n",
    "        _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : data, Y_ : labels, pkeep : 1.0, alpha : 0.002})\n",
    "        accuracy = acc(predictions, labels[:,1:6])\n",
    "        test_acc.append(accuracy)\n",
    "        \n",
    "        print('Test-Accuracy', ' i : ', i)\n",
    "        print('Test accuracy: ', accuracy)\n",
    "        print('       ')\n",
    "        \n",
    "        \n",
    "    valid_acc = list()\n",
    "    print('-----VALIDIDATION------')\n",
    "    valid_no = int(valid_labels.shape[0] /  batch)\n",
    "    for i in range(valid_no - 1):\n",
    "        model_saver.restore(session, \"saved_models/box/CNN_SVHN_Box.ckpt\")\n",
    "        data = valid_dataset[i*batch:(i+1)*batch]\n",
    "        labels = valid_labels[i*batch:(i+1)*batch]\n",
    "        \n",
    "        _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : data, Y_ : labels, pkeep : 1.0, alpha : 0.002})\n",
    "        accuracy = acc(predictions, labels[:,1:6])\n",
    "        valid_acc.append(accuracy)\n",
    "        \n",
    "        print('Valid-Accuracy', ' i : ', i)\n",
    "        print('Valid accuracy: ', accuracy)\n",
    "        print('        ')\n",
    "        \n",
    "        \n",
    "    test_avg = mean(test_acc)\n",
    "    valid_avg = mean(valid_acc)\n",
    "    \n",
    "    print('-----  FINAL  ------')\n",
    "    print('Final Test Set Accuracy : ',\"%.2f\" % test_avg)\n",
    "    print('Final Validation Set Accuracy : ',\"%.2f\" % valid_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  (230070, 32, 96, 1)   test :  (230070, 6)\n",
      "Initialized\n",
      "[[[[ -1.50522776e-02  -2.64132079e-02   1.52027056e-01   2.12814603e-02\n",
      "      1.38093442e-01   5.17821796e-02]]\n",
      "\n",
      "  [[  4.74680550e-02   2.50265330e-01  -7.72794187e-02  -3.27843428e-02\n",
      "     -1.74948499e-01   9.23548341e-02]]\n",
      "\n",
      "  [[  3.13280448e-02   1.62439570e-01  -1.15763634e-01   2.56980397e-02\n",
      "     -5.72252236e-02  -2.33353879e-02]]\n",
      "\n",
      "  [[  6.30227774e-02  -3.36757153e-02  -7.43347034e-02  -1.85744002e-01\n",
      "      1.70991588e-02   3.83118913e-02]]\n",
      "\n",
      "  [[ -6.02321662e-02   1.58646643e-01  -7.95831308e-02   3.92488986e-02\n",
      "     -5.92272915e-02  -4.05822210e-02]]\n",
      "\n",
      "  [[ -1.90515310e-01   1.36464490e-02  -1.43729493e-01   1.98645294e-01\n",
      "      9.47273299e-02   6.18795007e-02]]]\n",
      "\n",
      "\n",
      " [[[ -1.71603207e-02   3.16025503e-02  -8.37688297e-02   1.30627722e-01\n",
      "     -8.66929814e-02  -2.93678679e-02]]\n",
      "\n",
      "  [[ -4.22022454e-02  -1.76658720e-01   3.39092501e-02  -7.32576922e-02\n",
      "     -1.08213857e-01   1.77022755e-01]]\n",
      "\n",
      "  [[  8.91070664e-02  -6.98873773e-02  -5.86782806e-02  -5.82379997e-02\n",
      "      3.45929787e-02   6.91867545e-02]]\n",
      "\n",
      "  [[  1.20496906e-01   2.86542121e-02   4.11486290e-02  -1.64497942e-01\n",
      "      3.48755531e-02   6.72794133e-02]]\n",
      "\n",
      "  [[  8.64261761e-02  -1.68108955e-01  -2.93250429e-03  -1.99721590e-01\n",
      "      1.20899621e-02  -4.90165912e-02]]\n",
      "\n",
      "  [[  3.70610878e-02   3.49003747e-02  -1.41954333e-01   1.90248758e-01\n",
      "     -4.75699864e-02   9.48348269e-02]]]\n",
      "\n",
      "\n",
      " [[[ -9.10845026e-02  -1.66397288e-01   1.13889657e-01   8.62659737e-02\n",
      "     -6.71118777e-03   1.39712512e-01]]\n",
      "\n",
      "  [[  7.31104389e-02  -1.80880234e-01  -2.92718448e-02   1.60600945e-01\n",
      "     -1.91423640e-01   1.80988342e-01]]\n",
      "\n",
      "  [[  2.41741333e-02  -2.00079262e-01   1.07808469e-03  -5.72535358e-02\n",
      "     -1.31281659e-01   9.11629498e-02]]\n",
      "\n",
      "  [[  5.93286455e-02  -1.67174324e-01  -5.58652207e-02   1.81038994e-02\n",
      "      3.87017205e-02  -1.95705164e-02]]\n",
      "\n",
      "  [[  1.01102717e-01  -1.48282751e-01   5.42477192e-03  -2.04921469e-01\n",
      "      9.68073756e-02   1.75222196e-02]]\n",
      "\n",
      "  [[  1.65188253e-01  -5.86618185e-02   1.80602930e-02   4.70443517e-02\n",
      "      4.43432629e-02  -1.61597997e-01]]]\n",
      "\n",
      "\n",
      " [[[ -3.86365466e-02   9.96280387e-02   1.04298815e-01   2.25921109e-01\n",
      "     -2.98802584e-01   7.12745711e-02]]\n",
      "\n",
      "  [[ -1.71972796e-01  -8.72757658e-02   1.11775361e-01   1.44780263e-01\n",
      "     -9.38841179e-02   1.07005320e-01]]\n",
      "\n",
      "  [[  1.25943080e-01   8.94138869e-03   6.27964586e-02  -6.83166534e-02\n",
      "      1.16953207e-02  -7.54172041e-04]]\n",
      "\n",
      "  [[  4.57447432e-02   4.86523844e-02  -6.48006350e-02  -7.93234184e-02\n",
      "      1.50332585e-01  -7.41755590e-02]]\n",
      "\n",
      "  [[  1.74965814e-01   6.28683791e-02  -7.10371765e-04  -3.18121284e-01\n",
      "      9.24335420e-02  -1.90456405e-01]]\n",
      "\n",
      "  [[  2.68897284e-02  -5.29207401e-02   8.42594355e-02   1.98647287e-02\n",
      "      1.64441913e-01  -7.86706060e-02]]]\n",
      "\n",
      "\n",
      " [[[ -2.41705060e-01   6.26763403e-02   2.82434896e-02   1.50872814e-03\n",
      "     -1.77555874e-01  -6.61528558e-02]]\n",
      "\n",
      "  [[ -1.30465820e-01   1.15237132e-01   7.97797069e-02   2.04860926e-01\n",
      "     -2.83998065e-03  -1.13187116e-02]]\n",
      "\n",
      "  [[  5.26645361e-03   3.20783779e-02   6.59102425e-02   9.92287174e-02\n",
      "      1.48806930e-01  -7.01042041e-02]]\n",
      "\n",
      "  [[  8.56411830e-02   2.58011706e-02   1.57450348e-01   1.15791131e-02\n",
      "      2.34226376e-01  -1.83306649e-01]]\n",
      "\n",
      "  [[  5.95365092e-02   1.38560221e-01   1.80454813e-02  -2.01232716e-01\n",
      "      1.08999871e-01  -3.49126831e-02]]\n",
      "\n",
      "  [[  1.13162749e-01   9.01031494e-02  -4.99503724e-02   8.85821059e-02\n",
      "      1.52682811e-02  -4.59962897e-02]]]\n",
      "\n",
      "\n",
      " [[[ -5.16281724e-02   7.91989788e-02   3.01214289e-02  -2.78368089e-02\n",
      "      1.33696338e-02   2.09373366e-02]]\n",
      "\n",
      "  [[ -2.39746869e-01   4.17759754e-02   1.52391484e-02  -1.32862970e-01\n",
      "      1.27421558e-01   5.55030927e-02]]\n",
      "\n",
      "  [[ -1.15654960e-01   7.69115686e-02   6.21484369e-02   2.34499457e-04\n",
      "      1.68798789e-01  -1.50750056e-01]]\n",
      "\n",
      "  [[ -1.43289730e-01   5.82314134e-02  -2.77730543e-02   8.59381780e-02\n",
      "     -3.02675422e-02  -8.75433460e-02]]\n",
      "\n",
      "  [[  6.86383322e-02  -2.00416669e-02   5.90279605e-03  -7.19943270e-02\n",
      "      1.35192089e-02  -6.21126145e-02]]\n",
      "\n",
      "  [[ -8.19058344e-03  -1.77788045e-02   6.28340393e-02   4.71853353e-02\n",
      "     -1.30259141e-01  -6.33599758e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_dataset\n",
    "label_data = train_labels\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)\n",
    "\n",
    "num_steps_1 = 25001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "    model_saver.restore(session, model_to_save)\n",
    "    print('Initialized')\n",
    "    \n",
    "    W1 = session.run(W1)\n",
    "    print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
