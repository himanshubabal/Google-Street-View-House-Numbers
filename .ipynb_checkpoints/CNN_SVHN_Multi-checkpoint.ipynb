{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "import gc\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pylab as pl\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def acc(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_dataset(images, labels):\n",
    "    shuffle = list(zip(images, labels))\n",
    "    np.random.shuffle(shuffle)\n",
    "    i, l = zip(*shuffle)\n",
    "    i, l = np.asarray(i), np.asarray(l)\n",
    "    return i, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (229089, 32, 96, 1) (229089, 6)\n",
      "Test set (13068, 32, 96, 1) (13068, 6)\n",
      "Validation set (6666, 32, 96, 1) (6666, 6)\n"
     ]
    }
   ],
   "source": [
    "hdf_file = 'datasets/pickles/SVHN_multi.hdf5'\n",
    "\n",
    "hdf = h5py.File(hdf_file,'r')\n",
    "test_dataset = hdf['test_images'][:]\n",
    "test_labels = hdf['test_labels'][:]\n",
    "train_dataset = hdf['train_images'][:]\n",
    "train_labels = hdf['train_labels'][:]\n",
    "valid_dataset = hdf['valid_images'][:]\n",
    "valid_labels = hdf['valid_labels'][:]\n",
    "\n",
    "hdf.close()       \n",
    "    \n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_img(im, label):\n",
    "    if len(im.shape) >= 3:\n",
    "        im = im[:,:,0]\n",
    "    plt.imshow(im)\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "    \n",
    "def plot(i, d=0):\n",
    "    if d == 2:\n",
    "        plot_img(valid_dataset[i], valid_labels[i])\n",
    "    elif d == 1:\n",
    "        plot_img(test_dataset[i], test_labels[i])\n",
    "    else :\n",
    "        plot_img(train_dataset[i], train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACeCAYAAAAiy/EDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efebae797f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACeCAYAAAAiy/EDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhG4YhpESbEI3DMNICf8fH2YqmL1YXRQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efebae797b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACeCAYAAAAiy/EDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efebae9f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACeCAYAAAAiy/EDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHLEbEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efebae9f8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACeCAYAAAAiy/EDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wdo7C0BuRdQqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe02471fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACeCAYAAAAiy/EDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8P8BAq61ed4m+MYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe023cafd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(0)\n",
    "plot(123)\n",
    "\n",
    "plot(0,1)\n",
    "plot(123,1)\n",
    "\n",
    "plot(0,2)\n",
    "plot(123,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label_dist(labels_data):\n",
    "    ddict = {}\n",
    "    for i in range(labels_data.shape[0]):\n",
    "        l = labels_data[i][0]\n",
    "        if not l in ddict:\n",
    "            ddict[l] = 0\n",
    "        ddict[l] = ddict[l] + 1\n",
    "    return ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hist_dict(dict) :\n",
    "    dict = OrderedDict(sorted(dict.items()))\n",
    "    X = np.arange(len(dict))\n",
    "    pl.bar(X, dict.values(), align='center', width=0.5)\n",
    "    pl.xticks(X, dict.keys())\n",
    "    ymax = max(dict.values()) + 1000\n",
    "    pl.ylim(0, ymax)\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label_dict = get_label_dist(train_labels)\n",
    "test_label_dict = get_label_dist(test_labels)\n",
    "valid_label_dict = get_label_dist(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBcNgwzQzUwA/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe023a8a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_dict(train_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADg5JREFUeJzt3FuMXdV9x/HvrzbkQi5AmCBqWx1XsVKRSC1oBLRIeYCI\nu2IeguSqTdzIkl9oS9pKCfQFNQkSSFUgkRokC1NBiuIgSIUVUKjFRRVSuYyBkBgHYQEFFxpPZENC\noyQ1+fdhFjBGY88Zezx76Pp+pNHsvc46Z9Y+D/Ods2efk6pCktSf3xl6AZKkYRgASeqUAZCkThkA\nSeqUAZCkThkASeqUAZCkThkASeqUAZCkTi0fegGHctJJJ9X4+PjQy5Ckd5Xt27f/rKrG5pq3pAMw\nPj7O5OTk0MuQpHeVJP85yjxPAUlSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhS\npwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyAJHXKAEhSpwyA\nJHVq+dAL0OIYv/LuoZcAwAvXXjz0EiQ1vgKQpE4ZAEnqlAGQpE6NFIAkf5NkR5IfJ/lOkvcmWZ3k\nkSTPJvlukmPb3Pe0/V3t9vEZj3NVG38myflH55AkSaOYMwBJVgB/DUxU1SeBZcA64Drg+qpaA+wD\nNrS7bAD2VdXHgOvbPJKc2u73CeAC4FtJli3s4UiSRjXqKaDlwPuSLAfeD7wCnAPc0W6/Bbi0ba9t\n+7Tbz02SNr6lqn5dVc8Du4AzjvwQJEmHY84AVNV/Af8IvMj0L/7XgO3Aq1W1v03bDaxo2yuAl9p9\n97f5H5k5Pst9JEmLbJRTQCcw/df7auB3geOAC2eZWm/e5SC3HWz8nT9vY5LJJJNTU1NzLU+SdJhG\nOQX0aeD5qpqqqv8Fvgf8CXB8OyUEsBJ4uW3vBlYBtNs/DOydOT7Lfd5SVZuqaqKqJsbGxg7jkCRJ\noxglAC8CZyV5fzuXfy7wNPAA8Nk2Zz1wV9ve2vZpt99fVdXG17WrhFYDa4BHF+YwJEnzNedHQVTV\nI0nuAB4H9gNPAJuAu4EtSb7Wxja3u2wGvp1kF9N/+a9rj7Mjye1Mx2M/cHlVvbHAxyNJGtFInwVU\nVVcDV79j+DlmuYqnqn4FXHaQx7kGuGaea5QkHQW+E1iSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlT\nBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCS\nOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUA\nJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTIwUgyfFJ7kjykyQ7k/xxkhOTbEvybPt+Qpub\nJN9MsivJU0lOn/E469v8Z5OsP1oHJUma26ivAL4B/KCq/gD4Q2AncCVwX1WtAe5r+wAXAmva10bg\nRoAkJwJXA2cCZwBXvxkNSdLimzMAST4EfArYDFBVv6mqV4G1wC1t2i3ApW17LXBrTXsYOD7JKcD5\nwLaq2ltV+4BtwAULejSSpJGN8grg94Ep4J+TPJHkpiTHASdX1SsA7ftH2/wVwEsz7r+7jR1s/ABJ\nNiaZTDI5NTU17wOSJI1mlAAsB04Hbqyq04D/4e3TPbPJLGN1iPEDB6o2VdVEVU2MjY2NsDxJ0uEY\nJQC7gd1V9Ujbv4PpIPy0ndqhfd8zY/6qGfdfCbx8iHFJ0gDmDEBV/TfwUpKPt6FzgaeBrcCbV/Ks\nB+5q21uBz7ergc4CXmuniO4FzktyQvvn73ltTJI0gOUjzvsr4LYkxwLPAV9gOh63J9kAvAhc1ube\nA1wE7AJ+2eZSVXuTfBV4rM37SlXtXZCjkCTN20gBqKongYlZbjp3lrkFXH6Qx7kZuHk+C5QkHR2+\nE1iSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCS\nOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUA\nJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOjVyAJIs\nS/JEku+3/dVJHknybJLvJjm2jb+n7e9qt4/PeIyr2vgzSc5f6IORJI1uPq8ArgB2zti/Dri+qtYA\n+4ANbXwDsK+qPgZc3+aR5FRgHfAJ4ALgW0mWHdnyJUmHa6QAJFkJXAzc1PYDnAPc0abcAlzatte2\nfdrt57b5a4EtVfXrqnoe2AWcsRAHIUmav1FfAdwAfAn4bdv/CPBqVe1v+7uBFW17BfASQLv9tTb/\nrfFZ7iNJWmRzBiDJJcCeqto+c3iWqTXHbYe6z8yftzHJZJLJqampuZYnSTpMo7wCOBv4TJIXgC1M\nn/q5ATg+yfI2ZyXwctveDawCaLd/GNg7c3yW+7ylqjZV1URVTYyNjc37gCRJo5kzAFV1VVWtrKpx\npv+Je39V/RnwAPDZNm09cFfb3tr2abffX1XVxte1q4RWA2uARxfsSCRJ87J87ikH9WVgS5KvAU8A\nm9v4ZuDbSXYx/Zf/OoCq2pHkduBpYD9weVW9cQQ/X5J0BOYVgKp6EHiwbT/HLFfxVNWvgMsOcv9r\ngGvmu0hJ0sLzncCS1CkDIEmdMgCS1CkDIEmdMgCS1CkDIEmdMgCS1CkDIEmdMgCS1CkDIEmdMgCS\n1CkDIEmdMgCS1CkDIEmdMgCS1CkDIEmdMgCS1CkDIEmdMgCS1CkDIEmdMgCS1CkDIEmdMgCS1CkD\nIEmdMgCS1CkDIEmdMgCS1CkDIEmdMgCS1CkDIEmdWj70Ao6m8SvvHnoJvHDtxUMvQZJm5SsASeqU\nAZCkThkASeqUAZCkThkASeqUAZCkThkASerUnAFIsirJA0l2JtmR5Io2fmKSbUmebd9PaONJ8s0k\nu5I8leT0GY+1vs1/Nsn6o3dYkqS5jPJGsP3A31XV40k+CGxPsg34C+C+qro2yZXAlcCXgQuBNe3r\nTOBG4MwkJwJXAxNAtcfZWlX7FvqgpLkshTcJgm8U1LDmfAVQVa9U1eNt+xfATmAFsBa4pU27Bbi0\nba8Fbq1pDwPHJzkFOB/YVlV72y/9bcAFC3o0kqSRzet/AEnGgdOAR4CTq+oVmI4E8NE2bQXw0oy7\n7W5jBxt/58/YmGQyyeTU1NR8lidJmoeRA5DkA8CdwBer6ueHmjrLWB1i/MCBqk1VNVFVE2NjY6Mu\nT5I0TyMFIMkxTP/yv62qvteGf9pO7dC+72nju4FVM+6+Enj5EOOSpAGMchVQgM3Azqr6+oybtgJv\nXsmzHrhrxvjn29VAZwGvtVNE9wLnJTmhXTF0XhuTJA1glKuAzgY+B/woyZNt7O+Ba4Hbk2wAXgQu\na7fdA1wE7AJ+CXwBoKr2Jvkq8Fib95Wq2rsgRyFJmrc5A1BVDzH7+XuAc2eZX8DlB3msm4Gb57NA\nSdLR4TuBJalTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCS\nOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUA\nJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlTBkCSOmUAJKlT\nyxf7Bya5APgGsAy4qaquXew1SHrb+JV3D70EAF649uKhl9CdRX0FkGQZ8E/AhcCpwJ8mOXUx1yBJ\nmrbYp4DOAHZV1XNV9RtgC7B2kdcgSWLxA7ACeGnG/u42JklaZIv9P4DMMlYHTEg2Ahvb7utJnjnq\nqzq4k4CfHckD5LoFWsnS4PNxIJ+Pt/lcHOiIn48j9HujTFrsAOwGVs3YXwm8PHNCVW0CNi3mog4m\nyWRVTQy9jqXC5+NAPh9v87k40Lvl+VjsU0CPAWuSrE5yLLAO2LrIa5AkscivAKpqf5K/BO5l+jLQ\nm6tqx2KuQZI0bdHfB1BV9wD3LPbPPUxL4lTUEuLzcSCfj7f5XBzoXfF8pKrmniVJ+n/Hj4KQpE4Z\ngFkkuTnJniQ/HnotS0GSVUkeSLIzyY4kVwy9pqEkeW+SR5P8sD0X/zD0mpaCJMuSPJHk+0OvZWhJ\nXkjyoyRPJpkcej2H4imgWST5FPA6cGtVfXLo9QwtySnAKVX1eJIPAtuBS6vq6YGXtuiSBDiuql5P\ncgzwEHBFVT088NIGleRvgQngQ1V1ydDrGVKSF4CJqhryfQAj8RXALKrq34G9Q69jqaiqV6rq8bb9\nC2Annb6Du6a93naPaV9d/xWVZCVwMXDT0GvR/BgAzUuSceA04JFhVzKcdrrjSWAPsK2qun0umhuA\nLwG/HXohS0QB/5Zke/tkgyXLAGhkST4A3Al8sap+PvR6hlJVb1TVHzH9TvYzknR7mjDJJcCeqto+\n9FqWkLOr6nSmP/X48nZKeUkyABpJO999J3BbVX1v6PUsBVX1KvAgcMHASxnS2cBn2nnvLcA5Sf5l\n2CUNq6pebt/3AP/K9KcgL0kGQHNq//jcDOysqq8PvZ4hJRlLcnzbfh/waeAnw65qOFV1VVWtrKpx\npj/a5f6q+vOBlzWYJMe1CyVIchxwHrBkryY0ALNI8h3gP4CPJ9mdZMPQaxrY2cDnmP7r7sn2ddHQ\nixrIKcADSZ5i+rOttlVV95c+6i0nAw8l+SHwKHB3Vf1g4DUdlJeBSlKnfAUgSZ0yAJLUKQMgSZ0y\nAJLUKQMgSZ0yAJLUKQMgSZ0yAJLUqf8Dy/BzUN5FgI8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe022804a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_dict(test_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe022cf3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_dict(valid_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.astype(np.float32)\n",
    "test_dataset = test_dataset.astype(np.float32)\n",
    "valid_dataset = valid_dataset.astype(np.float32)\n",
    "\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "test_labels = test_labels.astype(np.int32)\n",
    "valid_labels = valid_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_to_save = \"saved_models/multi/CNN_SVHN_Multi.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_svhn = tf.Graph()\n",
    "\n",
    "with graph_svhn.as_default():\n",
    "    HEIGHT = 32\n",
    "    WIDTH = 32*3\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, 1], name=\"X\")\n",
    "    Y_ = tf.placeholder(tf.int32, [None, 6], name=\"Y\")\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Dropout Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # 5 Layers and their no of neurons\n",
    "    # 3 Convolutional Layers and a fully connected layer\n",
    "    K = 6     # First Conv Layer with depth 6\n",
    "    L = 12     # Second Conv Layer with depth 12\n",
    "    M = 24    # Third Conv layer with depth 24\n",
    "    N = 200   # Fourth Fully Connected layer with 200 neurons\n",
    "    # Last one will be softmax layer with 10 output channels\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1), name=\"W1\")    # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]), name=\"B1\")\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1), name=\"W2\")\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]), name=\"B2\")\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1), name=\"W3\")\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]), name=\"B3\")\n",
    "    \n",
    "    W5_1 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_1\")\n",
    "    B5_1 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_1\")\n",
    "    \n",
    "    W5_2 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_2\")\n",
    "    B5_2 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_2\")\n",
    "    \n",
    "    W5_3 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_3\")\n",
    "    B5_3 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_3\")\n",
    "    \n",
    "    W5_4 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_4\")\n",
    "    B5_4 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_4\")\n",
    "    \n",
    "    W5_5 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_5\")\n",
    "    B5_5 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_5\")\n",
    "    \n",
    "    # Model\n",
    "    stride = 1  # output is 32x96\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    \n",
    "    stride = 2  # output is 16x48\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    \n",
    "    stride = 2  # output is 8x24\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    shape = Y3.get_shape().as_list()\n",
    "    YY = tf.reshape(Y3, shape=[-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([shape[1] * shape[2] * shape[3], N], stddev=0.1), name=\"W4\")\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]), name=\"B4\")\n",
    "\n",
    "    Y4 = tf.sigmoid(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    \n",
    "    Ylogits_1 = tf.matmul(YY4, W5_1) + B5_1\n",
    "    Ylogits_2 = tf.matmul(YY4, W5_2) + B5_2\n",
    "    Ylogits_3 = tf.matmul(YY4, W5_3) + B5_3\n",
    "    Ylogits_4 = tf.matmul(YY4, W5_4) + B5_4\n",
    "    Ylogits_5 = tf.matmul(YY4, W5_5) + B5_5   \n",
    "    ## ('Ylogits_1 shape : ', [None, 11])\n",
    "    \n",
    "    Y_1 = tf.nn.softmax(Ylogits_1)\n",
    "    Y_2 = tf.nn.softmax(Ylogits_2)\n",
    "    Y_3 = tf.nn.softmax(Ylogits_3)\n",
    "    Y_4 = tf.nn.softmax(Ylogits_4)\n",
    "    Y_5 = tf.nn.softmax(Ylogits_5)\n",
    "\n",
    "#     tf.summary.histogram(\"W1_summ\", W1)\n",
    "#     tf.summary.histogram(\"W2_summ\", W2)\n",
    "#     tf.summary.histogram(\"W3_summ\", W3)\n",
    "#     tf.summary.histogram(\"W4_summ\", W4)\n",
    "#     tf.summary.histogram(\"W5_1_summ\", W5_1)\n",
    "#     tf.summary.histogram(\"W5_2_summ\", W5_2)\n",
    "#     tf.summary.histogram(\"W5_3_summ\", W5_3)\n",
    "#     tf.summary.histogram(\"W5_4_summ\", W5_4)\n",
    "#     tf.summary.histogram(\"W5_5_summ\", W5_5)\n",
    "\n",
    "#     tf.summary.scalar('W1', W1)\n",
    "#     tf.summary.scalar('W2', W2)\n",
    "#     tf.summary.scalar('W3', W3)\n",
    "#     tf.summary.scalar('W4', W4)\n",
    "    \n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_1, Y_[:,1])) +\\\n",
    "        tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_2, Y_[:,2])) +\\\n",
    "        tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_3, Y_[:,3])) +\\\n",
    "        tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_4, Y_[:,4])) +\\\n",
    "        tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_5, Y_[:,5]))\n",
    "#     tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    \n",
    "    train_prediction = tf.pack([Y_1, Y_2, Y_3, Y_4, Y_5])\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "    \n",
    "    W_s = tf.pack([tf.reduce_max(tf.abs(W1)),tf.reduce_max(tf.abs(W2)),tf.reduce_max(tf.abs(W3)),tf.reduce_max(tf.abs(W4))])\n",
    "    b_s = tf.pack([tf.reduce_max(tf.abs(B1)),tf.reduce_max(tf.abs(B2)),tf.reduce_max(tf.abs(B3)),tf.reduce_max(tf.abs(B4))])\n",
    "    \n",
    "#     merged = tf.summary.merge_all()\n",
    "#     train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + '/train',sess.graph)\n",
    "    \n",
    "    model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  (229089, 32, 96, 1)   test :  (229089, 6)\n",
      "Initialized\n",
      "epoch :  1  =>  Loss at step 0: 14.640172\n",
      "epoch :  1  =>  Minibatch accuracy: 6.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 1: 14.453615\n",
      "epoch :  1  =>  Minibatch accuracy: 6.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 2: 14.438289\n",
      "epoch :  1  =>  Minibatch accuracy: 5.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 3: 14.648836\n",
      "epoch :  1  =>  Minibatch accuracy: 5.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 4: 14.366284\n",
      "epoch :  1  =>  Minibatch accuracy: 5.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 5: 14.734524\n",
      "epoch :  1  =>  Minibatch accuracy: 6.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 6: 14.588514\n",
      "epoch :  1  =>  Minibatch accuracy: 5.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 7: 14.683742\n",
      "epoch :  1  =>  Minibatch accuracy: 5.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 8: 14.839526\n",
      "epoch :  1  =>  Minibatch accuracy: 7.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 9: 14.558135\n",
      "epoch :  1  =>  Minibatch accuracy: 7.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 10: 14.422233\n",
      "epoch :  1  =>  Minibatch accuracy: 6.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 11: 14.554212\n",
      "epoch :  1  =>  Minibatch accuracy: 6.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 12: 14.690644\n",
      "epoch :  1  =>  Minibatch accuracy: 4.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 13: 14.599206\n",
      "epoch :  1  =>  Minibatch accuracy: 5.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 14: 13.990891\n",
      "epoch :  1  =>  Minibatch accuracy: 8.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 15: 14.587775\n",
      "epoch :  1  =>  Minibatch accuracy: 6.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 16: 14.216275\n",
      "epoch :  1  =>  Minibatch accuracy: 6.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 17: 14.784056\n",
      "epoch :  1  =>  Minibatch accuracy: 4.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 18: 14.348070\n",
      "epoch :  1  =>  Minibatch accuracy: 7.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 19: 14.528382\n",
      "epoch :  1  =>  Minibatch accuracy: 6.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 20: 14.676830\n",
      "epoch :  1  =>  Minibatch accuracy: 5.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 21: 14.403255\n",
      "epoch :  1  =>  Minibatch accuracy: 7.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 22: 14.535438\n",
      "epoch :  1  =>  Minibatch accuracy: 6.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 23: 14.559296\n",
      "epoch :  1  =>  Minibatch accuracy: 7.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 24: 14.757269\n",
      "epoch :  1  =>  Minibatch accuracy: 5.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 25: 14.497860\n",
      "epoch :  1  =>  Minibatch accuracy: 6.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 26: 14.602815\n",
      "epoch :  1  =>  Minibatch accuracy: 6.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 27: 14.387984\n",
      "epoch :  1  =>  Minibatch accuracy: 7.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 28: 14.346991\n",
      "epoch :  1  =>  Minibatch accuracy: 6.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 29: 14.694167\n",
      "epoch :  1  =>  Minibatch accuracy: 5.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 30: 14.865326\n",
      "epoch :  1  =>  Minibatch accuracy: 4.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 31: 14.420681\n",
      "epoch :  1  =>  Minibatch accuracy: 4.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 32: 14.502714\n",
      "epoch :  1  =>  Minibatch accuracy: 5.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 33: 14.854828\n",
      "epoch :  1  =>  Minibatch accuracy: 5.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 34: 14.475489\n",
      "epoch :  1  =>  Minibatch accuracy: 6.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 35: 14.809471\n",
      "epoch :  1  =>  Minibatch accuracy: 5.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 36: 14.839937\n",
      "epoch :  1  =>  Minibatch accuracy: 6.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 37: 14.596677\n",
      "epoch :  1  =>  Minibatch accuracy: 4.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 38: 14.789835\n",
      "epoch :  1  =>  Minibatch accuracy: 5.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 39: 14.630226\n",
      "epoch :  1  =>  Minibatch accuracy: 6.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 40: 14.624040\n",
      "epoch :  1  =>  Minibatch accuracy: 5.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 41: 14.526784\n",
      "epoch :  1  =>  Minibatch accuracy: 5.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 42: 14.580121\n",
      "epoch :  1  =>  Minibatch accuracy: 6.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 43: 14.458125\n",
      "epoch :  1  =>  Minibatch accuracy: 7.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 44: 14.789309\n",
      "epoch :  1  =>  Minibatch accuracy: 5.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 45: 14.490585\n",
      "epoch :  1  =>  Minibatch accuracy: 6.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 46: 14.607164\n",
      "epoch :  1  =>  Minibatch accuracy: 5.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 47: 14.759956\n",
      "epoch :  1  =>  Minibatch accuracy: 5.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 48: 14.631957\n",
      "epoch :  1  =>  Minibatch accuracy: 5.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 49: 14.242823\n",
      "epoch :  1  =>  Minibatch accuracy: 6.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 50: 14.616109\n",
      "epoch :  1  =>  Minibatch accuracy: 5.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 51: 14.479662\n",
      "epoch :  1  =>  Minibatch accuracy: 6.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 52: 14.553226\n",
      "epoch :  1  =>  Minibatch accuracy: 5.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 53: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 6.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [ 0.19496706  0.19928116  0.19977403  0.1999999 ]\n",
      "b :  [ 0.1  0.1  0.1  0.1]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 54: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [       -inf        -inf  0.19694112        -inf]\n",
      "b :  [-inf -inf  0.1 -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 55: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 56: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 57: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 58: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 59: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 60: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 61: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 62: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 63: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 64: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 65: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 66: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 67: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 68: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 69: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 70: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 71: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 72: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 73: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 74: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 75: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 76: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 77: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 78: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 79: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 80: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 81: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 82: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 83: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 84: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 85: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 86: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 87: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 88: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 89: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 90: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 91: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 92: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 93: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 94: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 95: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 96: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 97: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 98: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 99: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 100: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 101: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 102: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 103: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 104: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 105: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 106: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 107: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 108: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 109: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 110: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 111: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 112: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 113: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 114: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 115: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 116: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 117: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 118: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 119: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 120: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 121: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 122: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 123: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 124: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 125: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 126: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 127: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 128: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 129: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 130: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 131: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 132: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 133: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 134: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 135: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 136: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 137: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 138: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 139: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 140: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 141: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 142: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 143: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 144: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 145: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 146: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 147: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 148: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 149: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 150: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 151: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 152: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 153: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 154: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 155: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 156: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 157: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 158: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 159: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 160: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 161: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 162: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 163: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 164: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 165: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 166: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 167: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 168: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 169: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 170: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 171: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 172: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 173: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 174: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 175: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 176: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 177: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 178: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 179: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 180: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 181: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 182: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 183: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 184: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 185: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 186: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 187: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 188: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 189: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 190: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 191: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 192: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 193: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 194: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 195: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 196: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 197: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 198: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 199: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 200: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 201: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 202: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 203: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 204: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 205: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 206: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 207: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 208: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 209: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 210: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 211: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 212: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 213: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 214: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 215: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 216: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 217: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 218: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 219: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 220: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 221: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 222: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 223: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 224: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 225: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 226: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 227: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 228: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 229: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 230: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 231: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 232: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 233: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 234: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 235: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 236: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 237: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 238: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 239: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 240: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 241: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 242: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 243: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 244: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 245: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 246: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 247: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 248: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 249: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 250: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 251: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 252: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 253: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 254: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 255: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 256: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 257: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 258: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 259: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 260: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 261: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 262: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 263: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 264: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 265: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 266: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 267: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 268: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 269: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 270: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 271: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 272: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 273: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 274: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 275: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 276: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 277: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 278: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 279: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 280: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 281: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 282: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 283: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 284: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 285: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 286: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 287: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 288: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 289: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 290: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 291: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 292: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 293: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 294: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 295: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 296: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 297: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 298: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 299: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 300: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 301: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 302: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 303: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 304: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 305: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 306: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 307: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 308: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 309: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 310: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 311: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 312: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 313: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 314: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 315: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 316: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 317: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 318: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 319: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 320: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 321: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 322: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 323: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 324: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 325: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 326: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 327: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 328: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 329: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 330: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 331: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 332: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 333: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 334: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 335: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 336: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 337: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 338: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 339: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 340: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 341: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 342: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 343: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 344: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 345: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 346: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 347: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 348: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 349: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 350: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 351: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 352: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 353: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 354: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 355: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 356: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 357: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 358: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 359: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 360: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 361: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 362: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 363: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 364: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 365: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 366: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 367: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 368: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 369: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 370: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 371: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 372: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 373: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 374: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 375: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 376: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 377: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 378: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 379: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 380: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 381: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 382: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 383: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 384: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 385: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 386: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 387: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 388: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 389: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 390: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 391: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 392: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 393: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 394: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 395: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 396: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 397: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 398: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 399: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 400: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 401: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 402: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 403: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 404: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 405: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 406: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 52.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 407: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 408: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 409: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 410: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 411: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 412: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 413: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 414: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 415: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 416: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 417: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 418: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 419: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 420: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 421: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 422: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 423: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 424: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 425: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 426: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 427: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 428: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 429: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 430: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 431: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 432: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 433: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 434: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 435: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 436: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 437: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 438: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 439: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 440: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 441: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 442: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 443: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 444: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 445: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 446: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 447: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 448: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 449: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 450: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 451: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 452: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 453: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 454: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 455: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 456: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 457: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 458: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 459: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 460: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 461: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 462: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 463: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 464: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 465: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 466: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 467: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 468: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 469: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 470: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 45.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 471: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 472: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 473: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 474: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 475: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 476: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 477: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 478: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 52.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 479: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 480: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 481: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 482: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 483: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 484: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 485: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 486: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 487: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 488: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 489: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 490: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 491: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 492: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 493: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 494: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 495: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 496: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 497: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 498: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 499: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 500: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 501: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 502: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 503: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 504: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 505: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 506: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 507: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 508: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 509: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 510: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 511: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 52.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 512: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 513: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 514: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 515: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 516: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 517: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 518: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 519: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 520: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 521: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 522: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 523: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 524: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 525: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 526: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 527: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 528: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 529: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 530: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 531: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 532: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 533: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 534: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 535: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 536: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 537: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 538: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 539: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 540: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 541: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 542: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 543: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 544: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 545: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 546: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 547: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 548: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 549: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 550: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 551: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 552: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 553: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 554: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 555: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 556: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 557: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 558: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 559: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 560: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 561: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 562: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 563: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 564: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 51.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 565: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 566: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 567: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 568: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 569: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 570: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 571: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 572: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 52.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 573: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.6%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 574: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 575: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 576: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 577: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 578: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.5%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 579: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 580: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 581: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 582: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 583: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.2%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 584: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 585: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 586: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 587: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 588: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 46.9%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 589: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 590: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.4%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 591: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.0%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 592: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 49.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 593: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 50.3%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 594: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.7%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 595: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 47.8%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n",
      "epoch :  1  =>  Loss at step 596: nan\n",
      "epoch :  1  =>  Minibatch accuracy: 48.1%\n",
      "epoch :  1  =>  Learning rate :  0.0\n",
      "W :  [-inf -inf -inf -inf]\n",
      "b :  [-inf -inf -inf -inf]\n",
      "    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c566aca30247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkeep\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0maccu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = train_dataset\n",
    "label_data = train_labels\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_steps = int(label_data.shape[0] / batch_size)\n",
    "\n",
    "# num_epochs = int(train_labels.shape[0] / batch_size)\n",
    "num_epochs = 15\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "#     model_saver.restore(session, saved_model)\n",
    "    print('Initialized')\n",
    "    \n",
    "    for epoch in range(num_epochs-1) :\n",
    "        test_d = test_dataset[epoch*batch_size:(epoch+1)*batch_size,:,:,:]\n",
    "        test_l = test_labels[epoch*batch_size:(epoch+1)*batch_size,:]\n",
    "        \n",
    "        for step in range(num_steps-1):\n",
    "            \n",
    "            #  learning rate decay\n",
    "#             max_learning_rate = 0.00005\n",
    "#             min_learning_rate = 0.00001\n",
    "\n",
    "            max_learning_rate = 0.0000\n",
    "            min_learning_rate = 0.0000\n",
    "\n",
    "            decay_speed = 5000.0\n",
    "            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "            \n",
    "            batch_data = train_data[step*batch_size:(step + 1)*batch_size, :, :, :]\n",
    "            batch_labels = label_data[step*batch_size:(step + 1)*batch_size, :]\n",
    "\n",
    "            feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "            _, l, train_pred, W, b = session.run([train_step, cross_entropy, train_prediction, W_s, b_s], feed_dict=feed_dict)\n",
    "            accu = acc(train_pred, batch_labels[:,1:6])\n",
    "            \n",
    "            if (step % 1 == 0):\n",
    "                print('epoch : ',epoch+1, ' => ', 'Loss at step %d: %f' % (step, l))\n",
    "                print('epoch : ',epoch+1, ' => ', 'Minibatch accuracy: %.1f%%' % accu)\n",
    "                print('epoch : ',epoch+1, ' => ', 'Learning rate : ', learning_rate)\n",
    "                print('W : ', W)\n",
    "                print('b : ', b)\n",
    "                print('    ')\n",
    "        print('------------------------------------------')\n",
    "        feed_dict = {X : test_d, Y_ : test_l, pkeep : 1.00, alpha : 0.0005}\n",
    "        _, l, test_pred, W, b = session.run([train_step, cross_entropy, train_prediction, W_s, b_s], feed_dict=feed_dict)\n",
    "        \n",
    "        print('Epoch Test accuracy: %.1f%%' % acc(test_pred, test_l[:,1:6]))\n",
    "        print('Epoch W_s : ', W)\n",
    "        print('Epoch b_s : ', b)\n",
    "        print('------------------------------------------')\n",
    "        print('      ')\n",
    "                \n",
    "    print('Training Complete on MNIST Data')    \n",
    "    save_path = model_saver.save(session, model_to_save)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('train : ', train_data.shape, '  test : ', label_data.shape)\n",
    "\n",
    "batch_size = 128\n",
    "num_steps = int(label_data.shape[0] / batch_size)\n",
    "print(num_steps)\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "#     train_writer = tf.summary.FileWriter(\"tf_logs/\", graph_svhn)\n",
    "    \n",
    "    for step in range(num_steps-1):\n",
    "        #  learning rate decay\n",
    "        max_learning_rate = 0.0005\n",
    "        min_learning_rate = 0.0001\n",
    "\n",
    "        decay_speed = 5000.0\n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "        \n",
    "#         offset = (step * batch_size) % (label_data.shape[0] - batch_size)\n",
    "#         batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "#         batch_labels = label_data[offset:(offset + batch_size), :]\n",
    "        batch_data = train_data[step*batch_size:(step + 1)*batch_size, :, :, :]\n",
    "        batch_labels = label_data[step*batch_size:(step + 1)*batch_size, :]\n",
    "        \n",
    "\n",
    "        feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "        _, l, train_pred, W, b = session.run([train_step, cross_entropy, train_prediction, W_s, b_s], feed_dict=feed_dict)\n",
    "    \n",
    "#         train_writer.add_summary(summary, step)\n",
    "    \n",
    "        if (step % 500 == 0):\n",
    "            print('W : ', W)\n",
    "            print('b : ', b)\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % acc(train_pred, batch_labels[:,1:6]))\n",
    "            print('Learning rate : ', learning_rate)\n",
    "            print('    ')\n",
    "            \n",
    "    print('Training Complete on MNIST Data')\n",
    "    \n",
    "    save_path = model_saver.save(session, model_to_save)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph_svhn) as session: \n",
    "    print('Initialized')\n",
    "    batch = 1000\n",
    "    \n",
    "    valid_acc = list()\n",
    "    print('-----VALIDIDATION------')\n",
    "    valid_no = int(valid_labels.shape[0] /  batch)\n",
    "    for i in range(valid_no - 1):\n",
    "        model_saver.restore(session, \"saved_models/box/CNN_SVHN_Box.ckpt\")\n",
    "        data = valid_dataset[i*batch:(i+1)*batch]\n",
    "        labels = valid_labels[i*batch:(i+1)*batch]\n",
    "        \n",
    "        _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : data, Y_ : labels, pkeep : 1.0, alpha : 0.002})\n",
    "        accuracy = acc(predictions, labels[:,1:6])\n",
    "        valid_acc.append(accuracy)\n",
    "        \n",
    "        print('Valid-Accuracy', ' i : ', i)\n",
    "        print('Valid accuracy: ', accuracy)\n",
    "        print('        ')\n",
    "        \n",
    "        \n",
    "    valid_avg = mean(valid_acc)\n",
    "    \n",
    "    print('-----  FINAL  ------')\n",
    "    print('Final Validation Set Accuracy : ',\"%.2f\" % valid_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_dataset\n",
    "label_data = train_labels\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)\n",
    "\n",
    "num_steps_1 = 25001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "    model_saver.restore(session, model_to_save)\n",
    "    print('Initialized')\n",
    "    \n",
    "    W1 = session.run(W1)\n",
    "    print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
