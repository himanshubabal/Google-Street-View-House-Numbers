{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from download_helper.path import data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])\n",
    "\n",
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf_file = data_dir + 'MNIST/MNIST_manufactured_data.hdf5'\n",
    "\n",
    "hdf = h5py.File(hdf_file,'r')\n",
    "\n",
    "train_images = hdf['train_images'][:]\n",
    "train_labels = hdf['train_labels'][:]\n",
    "train_bboxes = hdf['train_bboxes'][:]\n",
    "\n",
    "test_images = hdf['test_images'][:]\n",
    "test_labels = hdf['test_labels'][:]\n",
    "test_bboxes = hdf['test_bboxes'][:]\n",
    "            \n",
    "hdf.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_inputs = 5000\n",
    "l = len(train_labels)\n",
    "\n",
    "valid_images = train_images[l-valid_inputs:]\n",
    "valid_labels = train_labels[l-valid_inputs:]\n",
    "valid_bboxes = train_bboxes[l-valid_inputs:]\n",
    "\n",
    "train_images = train_images[:l-valid_inputs]\n",
    "train_labels = train_labels[:l-valid_inputs]\n",
    "train_bboxes = train_bboxes[:l-valid_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)\n",
    "valid_images = valid_images.astype(np.float32)\n",
    "\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "test_labels = test_labels.astype(np.int32)\n",
    "valid_labels = valid_labels.astype(np.int32)\n",
    "\n",
    "train_bboxes = train_bboxes.astype(np.int32)\n",
    "test_bboxes = test_bboxes.astype(np.int32)\n",
    "valid_bboxes = valid_bboxes.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (120000, 64, 64), (120000, 11), (120000, 40))\n",
      "('Test set', (25000, 64, 64), (25000, 11), (25000, 40))\n",
      "('Valid set', (5000, 64, 64), (5000, 11), (5000, 40))\n"
     ]
    }
   ],
   "source": [
    "print('Training set', train_images.shape, train_labels.shape, train_bboxes.shape)\n",
    "print('Test set', test_images.shape, test_labels.shape, test_bboxes.shape)\n",
    "print('Valid set', valid_images.shape, valid_labels.shape, valid_bboxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 45  2 61 40 23 23 40 61  2 45 18  0  0  5  5  0  0  5  5  0  0  5  5  0\n",
      "  0  5  5  0  0  5  5  0  0  5  5  0  0  5  5]\n",
      "[ 3  1  1  8 10 10 10 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "print(train_bboxes[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12249aad0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEd1JREFUeJzt3XuQXGWZx/HvbyZDLuRuYBySLIE1FQ2sBp0NsLBbkIAV\nLytobbFYqxu3YsU/XAt3vYW9WKV/uPyjq3stsorGEtR4waQoVyuMsJbKBhIJlxBCEANMmFwIxHDZ\nJHN59o85Od1nmEk6mT7dM3l/n6quft7zvjP9ZDLPvO85ffocRQRmlp6WZidgZs3h4jdLlIvfLFEu\nfrNEufjNEuXiN0uUi98sUaMqfkkrJO2U9KSkNfVKyszKp9M9yUdSK/AEcC3QDTwAvD8iHqtfemZW\nlgmj+NqlwJMR8RSApO8A1wEjFv9ZmhiTOHsUL2lmJ3KEVzgWR1XL2NEU/1zg2ap2N3Dpib5gEmdz\nqZaP4iXN7EQ2R1fNY0dT/DWRtBpYDTCJKWW/nJnVaDQH/PYA86va87JtBRGxNiI6I6KzjYmjeDkz\nq6fRFP8DwEJJF0g6C7gR2FiftMysbKe97I+IPkl/DfwUaAVui4jtdcvMzEo1qn3+iPgx8OM65WJm\nDeQz/MwS5eI3S5SL3yxRLn6zRLn4zRLl4jdLlIvfLFEufrNEufjNEuXiN0tU6R/pNRt3VLwWRuuc\nOXnc+8Z5hb7uqyfn8czL9+Xxiy8VP74+Z32lPXXDg4W+6D12+rmOgmd+s0S5+M0S5WW/GUBLax7G\n5X9Q6HryYwN5/PlLflTo+9Hzl+TxjgPtefy+hQ8Vxh341LQ8fuaZRcXXvv+RU8+3DjzzmyXKxW+W\nKBe/WaK8z28G9F5T2Xe/+AvF/fUPTP1tHv/DxhsLfW+443Aez32mJ4+3nVvcr+/+p0qpTXjb1ELf\nOfefRsJ14JnfLFEufrNEnfa9+k7HdM0O37HHxoSqt/YA2n52bh4fOjK50HfWl2ZX4nseLvTVenZe\ny5LFeXxwyYxC35z7D+Zx/2NP1PT9RrI5ujgcL9R0uy7P/GaJcvGbJcrFb5Yov9VnBhz8v8qn7t43\nf1uh79YP/XEetyx7W6Hvwu9W3upr2f1cHsfvvb4wbuffTMrjj7z17kLf9758TR6/bsQb3NffSWd+\nSbdJ2i/p0aptsyVtkrQre55VbppmVm+1LPu/AawYsm0N0BURC4GurG1m40hNb/VJWgDcFREXZ+2d\nwFUR0SOpA7g3Ihad4FsAfqvPxq7WhRfm8QtLzy30HXjH0TxeNHdfoW9qW6Xv4efOy+NVi39VGDcQ\nlXn2m9++ttC34OtP5XFfz95TSfs1GvFWX3tEHD+XcS/QfqLBZjb2jPpofwwuHUZcPkhaLWmLpC29\nHB1pmJk12Oke7d8nqaNq2b9/pIERsRZYC4PL/tN8PbNS9e+qLL1nVMUAM26varzhgkLfM/9SeZdg\n/dL/yuPzWvsL4y77zicr3+KLWwt9fUebMyme7sy/EViZxSuBDfVJx8wapZa3+r4N3AcsktQtaRVw\nC3CtpF3ANVnbzMaRky77I+L9I3T5sL3ZOOYz/MyGGvKJvwnzK2/h/eYLxQtxXD57dx7/5Rf/No+n\nv7unMK5/Vm8eq7X4/Zt1IMzn9pslysVvligv+82GmNBRPGftt1+sXHzjzs5bC303/FvlLbz2R47k\n8fy/eL4w7sB9HXkc/cW3AZvFM79Zolz8Zoly8Zslyvv8ZkM8++cLCu3rf/8Xefxnt36y0Df3V6/k\n8a4Pt+Xxc93nF8Zd8K+P53F/k07nHcozv1miXPxmifKy3wxomVL5dN6Ua4ofUr3r6YvyuG9q8Xy8\nnk9VztybuensPO743tOFcf0HX6hLnvXkmd8sUS5+s0R52W8GDBypHIHf1128GPWHLv1lHq87fFmh\n77xbK0v9iXdXbrfb39dX7xTrzjO/WaJc/GaJcvGbJcr7/GYAA5VP2i3+/HOFrv+d+eY8XvRk8X5a\nA0cqn+Qbb1en9cxvligXv1mivOw3G6Kve09xQ3dz8iibZ36zRLn4zRLl4jdLlIvfLFG13K5rvqR7\nJD0mabukm7LtsyVtkrQre551su9lZmNHLTN/H/CJiFgMXAZ8VNJiYA3QFRELga6sbWbjxEmLPyJ6\nIuLXWfwSsAOYC1wHrMuGrQOuLytJM6u/U9rnl7QAuATYDLRHxPEbku0F2kf4MjMbg2oufklTgR8A\nH4+Iw9V9ERGMcGqzpNWStkja0svYuGqpmdVY/JLaGCz82yPih9nmfZI6sv4OYP9wXxsRayOiMyI6\n25hYj5zNrA5qOdov4GvAjoj4UlXXRmBlFq8ENtQ/PTMrSy3n9l8BfBB4RNK2bNvfAbcA6yWtAp4G\nbignRTMrw0mLPyJ+AWiE7uX1TcfMGsVn+JklysVvligXv1miXPxmiXLxmyXKxW+WKBe/WaJc/GaJ\ncvGbJcrFb5YoF79Zolz8Zoly8ZslysVvlijfq89eo2XKlDzuX7Iwj1tfLl6GbeDhxxuWk9WfZ36z\nRLn4zRLlZb/ROnNGob3jlkV5/M/L78jjf3z0PYVx57233LysXJ75zRLl4jdLlJf9hqZNK7SXLXks\nj68/++U8XjvzUGFcf7lpWck885slysVvligXv1mivM9vr9GiYe+5ysvHivdanNyIZKw0tdyrb5Kk\n+yU9JGm7pM9l22dL2iRpV/Y8q/x0zaxealn2HwWWRcRbgCXACkmXAWuArohYCHRlbTMbJ2q5V18A\nx9/vacseAVwHXJVtXwfcC3ym7hla6QYOvlBo37fngjzunffzPD7W31oY52X/+FbTAT9JrdkdevcD\nmyJiM9AeET3ZkL1Ae0k5mlkJair+iOiPiCXAPGCppIuH9AeDq4HXkLRa0hZJW3o5OtwQM2uCU3qr\nLyIOAfcAK4B9kjoAsuf9I3zN2ojojIjONiYON8TMmqCWo/3nSJqZxZOBa4HHgY3AymzYSmBDWUla\nuaK/v/Do7W3NH9VmT3618LDxrZb3+TuAdZJaGfxjsT4i7pJ0H7Be0irgaeCGEvM0szqr5Wj/w8Al\nw2w/CCwvIykzK5/P8DOit6/Q7huy3D9uINSIdKxBfG6/WaJc/GaJ8rLfmHDunEL7ovk9edxCZan/\nxDOvL4xbyJ5yE7NSeeY3S5SL3yxRLn6zRHmf36CtrdBsn/RiHg9UfWRjQs9ZDUvJyueZ3yxRLn6z\nRHnZbyf0ahzL4/N+3neCkTbeeOY3S5SL3yxRLn6zRHmf306ou2o3v+2l3uYlYnXnmd8sUS5+s0R5\n2W/Eq8Xr8W3dNy+Pv3nW5Xnctu9wYZxv0T2+eeY3S5SL3yxRXvYb/S+8WGjP+krldl3/veSP8nje\nizsblpOVzzO/WaJc/GaJcvGbJcr7/AZRvMfqhK6tedzRVdnut/bOLDXP/Nltuh+UdFfWni1pk6Rd\n2fOs8tI0s3o7lWX/TcCOqvYaoCsiFgJdWdvMxomail/SPOBdwFerNl8HrMvidcD19U3NzMpU68z/\nZeDTwEDVtvaIOH53h71Aez0TM7NynbT4Jb0b2B8RW0caExEBxHB9klZL2iJpSy9HTz9TM6urWo72\nXwG8R9I7gUnAdEnfAvZJ6oiIHkkdwP7hvjgi1gJrAaZr9rB/IMys8U4680fEzRExLyIWADcCP4uI\nDwAbgZXZsJXAhtKyNLO6G81JPrcA10raBVyTtc1snDilk3wi4l7g3iw+CCyvf0pm1gg+vdcsUS5+\ns0S5+M0S5eI3S5SL3yxRLn6zRLn4zRLl4jdLlIvfLFEufrNEufjNEuXiN0uUi98sUS5+s0S5+M0S\n5eI3S5SL3yxRSd6uq3X69DzWrBmFvr5nuiuN8PVG7czlmd8sUS5+s0Qls+zXxIl53P3hi/O47ern\nC+PaP1jZJeg/9LvyEzNrEs/8Zoly8ZslysVvlqhk9vlbqvb5X15yJI8/suDBwrj/aVvQqJTMmqqm\n4pe0G3gJ6Af6IqJT0mzgu8ACYDdwQ0S8WE6aZlZvp7LsvzoilkREZ9ZeA3RFxEKgK2ub2TgxmmX/\ndcBVWbyOwXv4fWaU+ZSnrfJPnTHj1SYmYjY21DrzB3C3pK2SVmfb2iOiJ4v3Au11z87MSlPrzH9l\nROyRdC6wSdLj1Z0REZKGPRE++2OxGmASU0aVrJnVT00zf0TsyZ73A3cCS4F9kjoAsuf9I3zt2ojo\njIjONiYON8TMmuCkM7+ks4GWiHgpi98OfB7YCKwEbsmeN5SZ6Ghp2tQ8ftf525uYidnYUMuyvx24\nU9Lx8XdExE8kPQCsl7QKeBq4obw0zazeTlr8EfEU8JZhth8ElpeRlJmV78w9w29wpZLbv2xuHr93\nxvfz+O6XL2pYSmZjic/tN0uUi98sUS5+s0Sdsfv8rdOmFdrP/2F/Hu865pMRzTzzmyXKxW+WqDN2\n2T9w9Gih/YY7evP4s7P+NI9XXXRfw3IyG0s885slysVvlqgzdtkfQ5b9Lb98OI8nd16ax0fe1Naw\nnMzGEs/8Zoly8ZslysVvlqgzdp//NQYqZ/i9bvuxPD5wrHgm4MC8cyuNAwdKT8usWTzzmyXKxW+W\nqHSW/VUm76os51s0UOh7btmMPD7vkeKPJ/r6yk3MrIE885slysVvligXv1miktznjxcP5fFTL88p\n9A1c8bs8br2j2NfXs7fcxMwayDO/WaJc/GaJSnLZ33+osrT/3X+8qdA358OVpf2hK88v9E39npf9\nduaoaeaXNFPS9yU9LmmHpMslzZa0SdKu7HlW2cmaWf3Uuuz/CvCTiHgjg7fu2gGsAboiYiHQlbXN\nbJxQRJx4gDQD2AZcGFWDJe0EroqInuwW3fdGxKITfa/pmh2Xaozd3q+ltdicPCmP41hvoS96j2E2\nlm2OLg7HCzr5yNpm/guAA8DXJT0o6avZrbrbI6InG7OXwbv5mtk4UUvxTwDeCvxnRFwCvMKQJX62\nIhh2CSFptaQtkrb0cnS4IWbWBLUUfzfQHRGbs/b3GfxjsC9b7pM97x/uiyNibUR0RkRnGxPrkbOZ\n1cFJiz8i9gLPSjq+P78ceAzYCKzMtq0ENpSSYdkG+guPgVdeyR/Re6zwMDuT1Po+/8eA2yWdBTwF\n/BWDfzjWS1oFPA3cUE6KZlaGmoo/IrYBncN0jbFD92ZWK5/ea5YoF79Zolz8Zoly8ZslysVvligX\nv1miXPxmiTrpp/rq+mLSAQZPCJoDPN+wFx6Z8yhyHkVjIY9TzeH8iDinloENLf78RaUtETHcSUPO\nw3k4jwbl4GW/WaJc/GaJalbxr23S6w7lPIqcR9FYyKO0HJqyz29mzedlv1miGlr8klZI2inpSUkN\nu9qvpNsk7Zf0aNW2hl96XNJ8SfdIekzSdkk3NSMXSZMk3S/poSyPzzUjj6p8WrPrQ97VrDwk7Zb0\niKRtkrY0MY+GXSa/YcUvqRX4d+AdwGLg/ZIWN+jlvwGsGLKtGZce7wM+ERGLgcuAj2Y/g0bnchRY\nFhFvAZYAKyRd1oQ8jruJwcvBH9esPK6OiCVVb601I4/GXSY/IhryAC4HflrVvhm4uYGvvwB4tKq9\nE+jI4g5gZ6NyqcphA3BtM3MBpgC/Bi5tRh7AvOwXehlwV7P+b4DdwJwh2xqaBzAD+C3Zsbiy82jk\nsn8u8GxVuzvb1ixNvfS4pAXAJcDmZuSSLbW3MXjh1U0xeIHWZvxMvgx8Ghio2taMPAK4W9JWSaub\nlEdDL5PvA36c+NLjZZA0FfgB8PGIONyMXCKiPyKWMDjzLpV0caPzkPRuYH9EbD1Bno36v7ky+3m8\ng8HdsT9pQh6jukz+qWpk8e8B5le152XbmqWmS4/Xm6Q2Bgv/9oj4YTNzAYiIQ8A9DB4TaXQeVwDv\nkbQb+A6wTNK3mpAHEbEne94P3AksbUIeo7pM/qlqZPE/ACyUdEF2FeAbGbz8d7M0/NLjkgR8DdgR\nEV9qVi6SzpE0M4snM3jc4fFG5xERN0fEvIhYwODvw88i4gONzkPS2ZKmHY+BtwOPNjqPaPRl8ss+\nkDLkwMU7gSeA3wB/38DX/TbQA/Qy+Nd1FfA6Bg807QLuBmY3II8rGVyyPczg/Q+3ZT+ThuYCvBl4\nMMvjUeCz2faG/0yqcrqKygG/Rv88LgQeyh7bj/9uNul3ZAmwJfu/+REwq6w8fIafWaJ8wM8sUS5+\ns0S5+M0S5eI3S5SL3yxRLn6zRLn4zRLl4jdL1P8DEekE61mwLBMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120bbfa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rectangle\n",
    "# (x1, y1) & (a1, b2):\n",
    "#          Top Left Corner for\n",
    "#          Rectangle 1 and 2 respectively\n",
    "# (x2, y2) & (a2, b2):\n",
    "#          Top Left Corner for\n",
    "#          Rectangle 1 and 2 respectively\n",
    "def get_iou(x1, y1, x2, y2, a1, b1, a2, b2):\n",
    "    x_overlap = max(0, min(x1 + x2, a1 + a2) - max(x1, a1))\n",
    "    y_overlap = max(0, min(y1 + y2, b1 + b2) - max(y1, b1))\n",
    "    intersection_area = x_overlap * y_overlap\n",
    "    \n",
    "    # areas of both rectangles\n",
    "    area_1 = abs(x2 - x1) * abs(y2 - y1)\n",
    "    area_2 = abs(a2 - a1) * abs(b2 - b1)\n",
    "    # Total (Union) area\n",
    "    union_area = area_1 + area_2 - intersection_area\n",
    "        \n",
    "    iou = (intersection_area * 1.0) / union_area\n",
    "    return(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(x, is_training, iteration, conv=False, offset=0.0, scale=1.0):\n",
    "    \"\"\"\n",
    "    Given some logits `x`, apply batch normalization to them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "    is_training\n",
    "    iteration\n",
    "    conv:      (boolean)(default=False)\n",
    "        Applying it to a convolutional layer?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Credits\n",
    "    -------\n",
    "    This code is based on code written by Martin Gorner:\n",
    "    - https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_4.2_batchnorm_convolutional.py\n",
    "    https://www.youtube.com/watch?v=vq2nnJ4g6N0\n",
    "    \"\"\"\n",
    "    # adding the iteration prevents from averaging across non-existing iterations\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.9999, iteration)\n",
    "    bnepsilon = 1e-5\n",
    "\n",
    "    # calculate mean and variance for batch of logits\n",
    "    if conv:\n",
    "        mean, variance = tf.nn.moments(x, [0, 1, 2])\n",
    "    else:\n",
    "        # mean and variance along the batch\n",
    "        mean, variance = tf.nn.moments(x, [0])\n",
    "\n",
    "    update_moving_averages = exp_moving_avg.apply([mean, variance])\n",
    "    tf.add_to_collection(\"update_moving_averages\", update_moving_averages)\n",
    "\n",
    "    # Mean and Variance (how it get it is dependent on whether it is training)\n",
    "    # TODO: Change the following to use the `is_trianing` directly without logical_not()\n",
    "    #       to make it more intuitive.\n",
    "    m = tf.cond(tf.logical_not(is_training),\n",
    "                lambda: exp_moving_avg.average(mean),\n",
    "                lambda: mean)\n",
    "    v = tf.cond(tf.logical_not(is_training),\n",
    "                lambda: exp_moving_avg.average(variance),\n",
    "                lambda: variance)\n",
    "\n",
    "    # Offset\n",
    "    param_shape = mean.get_shape().as_list()\n",
    "    beta_init = tf.constant_initializer(offset)\n",
    "    beta = tf.Variable(initial_value=beta_init(param_shape), name=\"beta\")\n",
    "\n",
    "    # Scale\n",
    "    gamma_init = tf.constant_initializer(scale)\n",
    "    gamma = tf.Variable(initial_value=gamma_init(param_shape), name=\"gamma\")\n",
    "\n",
    "    # Apply Batch Norm\n",
    "    Ybn = tf.nn.batch_normalization(x, m, v, offset=beta, scale=gamma,\n",
    "                                    variance_epsilon=bnepsilon)\n",
    "    return Ybn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_to_save = \"saved_models/MNIST/MNIST.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, rate=0.01, name=\"leaky_relu\"):\n",
    "    \"\"\"Leaky Rectified Linear Activation Unit\n",
    "\n",
    "    Args:\n",
    "        x:    preactivation tensor\n",
    "        rate: Amount of leakiness\n",
    "        name: name for this op\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name) as scope:\n",
    "        leak_rate = tf.multiply(x, rate, name=\"leak_rate\")\n",
    "        activation = tf.maximum(x, leak_rate, name=scope)\n",
    "        # activation_summary(activation)\n",
    "        # tf.histogram_summary(scope + '/activation', activation)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_pipeline(X_in, in_width, out_width, fltr_conv, fltr_maxpool, stride_conv, \n",
    "                  stride_maxpool, is_train, iteration, pkeep, conv=True):\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([fltr_conv, fltr_conv, in_width, out_width], stddev=0.1))    \n",
    "    B = tf.Variable(tf.constant(0.1, tf.float32, [out_width]))\n",
    "#     print('W : ', W.shape.as_list())\n",
    "#     print('B : ', B.shape.as_list())\n",
    "\n",
    "    Y_conv = tf.nn.conv2d(X_in, W, strides=[1, stride_conv, stride_conv, 1], padding='SAME') + B\n",
    "#     print('Y_conv : ', Y_conv.shape.as_list())\n",
    "    Y_bnorm = batchnorm(Y_conv, is_train, iteration, conv)\n",
    "#     print('Y_bnorm : ', Y_bnorm.shape.as_list())\n",
    "    Y_drop = tf.nn.dropout(Y_bnorm, pkeep)\n",
    "#     print('Y_drop : ', Y_drop.shape.as_list())\n",
    "    Y_pool = tf.nn.max_pool(Y_drop, ksize=[1, fltr_maxpool, fltr_maxpool, 1], strides=[1, stride_maxpool, stride_maxpool, 1], padding='SAME')\n",
    "#     print('Y_pool : ', Y_pool.shape.as_list())\n",
    "    Y_relu = leaky_relu(Y_pool)\n",
    "    return(Y_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(x, name=\"flatten\"):\n",
    "    \"\"\"Given a tensor whose first dimension is the batch_size, and all other\n",
    "    dimensions are elements of data, then it flattens the data so you get a\n",
    "    [batch_size, num_elements] sized tensor\"\"\"\n",
    "    with tf.name_scope(name) as scope:\n",
    "        num_elements = np.product(x.get_shape().as_list()[1:])\n",
    "        x = tf.reshape(x, [-1, num_elements], name=scope)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_pipeline(X_in, num_nodes, is_train, iteration, pkeep, conv=False):\n",
    "    in_nodes = int(X_in.shape.as_list()[-1])\n",
    "\n",
    "    W = tf.Variable(tf.truncated_normal([in_nodes, num_nodes], stddev=0.1))    \n",
    "    B = tf.Variable(tf.constant(0.1, tf.float32, [num_nodes]))\n",
    "#     print('W : ', W.shape.as_list())\n",
    "#     print('B : ', B.shape.as_list())\n",
    "    \n",
    "    Y_fc = tf.matmul(X_in, W) + B\n",
    "#     print('Y_fc : ', Y_fc.shape.as_list())\n",
    "    \n",
    "    Y_bn = batchnorm(Y_fc, is_train, iteration, conv)\n",
    "#     print('Y_bn : ', Y_bn.shape.as_list())\n",
    "    \n",
    "    Y_dp = tf.nn.dropout(Y_bn, keep_prob=pkeep)\n",
    "#     print('Y_dp : ', Y_dp.shape.as_list())\n",
    "    \n",
    "    Y_lr = leaky_relu(Y_dp, rate=0.01, name=\"relu\")\n",
    "#     print('Y_lr : ', Y_lr.shape.as_list())\n",
    "    \n",
    "    return(Y_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_digit_loss(logits_list, Y_, max_digits=10, name=\"loss\"):\n",
    "    \"\"\" Calculates the loss for the multi-digit recognition task,\n",
    "        given a list of the logits for each digit, and the correct\n",
    "        labels.\n",
    "\n",
    "    Args:\n",
    "        logits:         (list of tensors) list of the logits from each of the\n",
    "                        branches\n",
    "        Y:              (tensor) correct labels, shaped as [n_batch, max_digits]\n",
    "        name:           (str) Name for the scope of this loss.\n",
    "\n",
    "    Returns:\n",
    "        (tensor) the loss\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name) as scope:\n",
    "        # LOSSES FOR EACH DIGIT BRANCH\n",
    "        losses = [None] * (max_digits)\n",
    "        for i in range(max_digits):\n",
    "            losses[i] = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_list[i], labels=Y_[:, i+1])\n",
    "            \n",
    "        # AVERAGE LOSS\n",
    "        loss = sum(losses) / float(max_digits)\n",
    "        loss = tf.reduce_mean(loss, name=scope)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HEIGHT = 64\n",
    "WIDTH = 64\n",
    "no_of_digits = 10\n",
    "    \n",
    "graph_svhn = tf.Graph()\n",
    "\n",
    "with graph_svhn.as_default():    \n",
    "    # Image\n",
    "    X = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH])\n",
    "    X = X / 255.0\n",
    "    X = tf.reshape(X, shape=[-1, HEIGHT, WIDTH, 1])\n",
    "    # Label\n",
    "    Y_ = tf.placeholder(tf.int32, [None, no_of_digits + 1])\n",
    "    # Bounding Box\n",
    "    Z_ = tf.placeholder(tf.int32, [None, no_of_digits * 4])\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    # Dropout (or better : 1 - toDropOut) Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    # Model trainig or testing\n",
    "    is_train = tf.placeholder(tf.bool)\n",
    "    # Iteration\n",
    "    iteration = tf.placeholder(tf.int32)\n",
    "    \n",
    "    # Depth   # Filter   Stride\n",
    "    K = 6     # 3        1\n",
    "    L = 24    # 3        1\n",
    "    M = 96    # 5        1\n",
    "    N = 256   # 5        2\n",
    "    P = 384   # 3        2\n",
    "    Q = 256   # 3        1\n",
    "    \n",
    "    # Fully Connected / Dense\n",
    "    R = 16384\n",
    "    S = 4096\n",
    "    T = 512\n",
    "    U = 64\n",
    "    V = 256\n",
    "    \n",
    "    \n",
    "#     print('X : ', X.shape.as_list())\n",
    "#     print(' Y_ :', Y_.shape.as_list())\n",
    "#     print(' Z_ : ', Z_.shape.as_list())\n",
    "#     print('')\n",
    "    \n",
    "    Y1 = conv_pipeline(X,  in_width=1, out_width=K, fltr_conv=3, fltr_maxpool=3, stride_conv=1, stride_maxpool=1, is_train=is_train, iteration=iteration, pkeep=pkeep)    \n",
    "    Y2 = conv_pipeline(Y1, in_width=K, out_width=L, fltr_conv=3, fltr_maxpool=3, stride_conv=1, stride_maxpool=1, is_train=is_train, iteration=iteration, pkeep=pkeep)    \n",
    "    Y3 = conv_pipeline(Y2, in_width=L, out_width=M, fltr_conv=5, fltr_maxpool=5, stride_conv=2, stride_maxpool=1, is_train=is_train, iteration=iteration, pkeep=pkeep)    \n",
    "    Y4 = conv_pipeline(Y3, in_width=M, out_width=N, fltr_conv=5, fltr_maxpool=5, stride_conv=1, stride_maxpool=1, is_train=is_train, iteration=iteration, pkeep=pkeep)    \n",
    "    Y5 = conv_pipeline(Y4, in_width=N, out_width=P, fltr_conv=3, fltr_maxpool=3, stride_conv=2, stride_maxpool=1, is_train=is_train, iteration=iteration, pkeep=pkeep)    \n",
    "    Y6 = conv_pipeline(Y5, in_width=P, out_width=Q, fltr_conv=3, fltr_maxpool=3, stride_conv=1, stride_maxpool=1, is_train=is_train, iteration=iteration, pkeep=pkeep)\n",
    "\n",
    "    Y_flat = flatten_layer(Y6)\n",
    "    \n",
    "    Y7 = fc_pipeline(Y_flat, R, is_train, iteration, pkeep)    \n",
    "    Y8 = fc_pipeline(Y7, S, is_train, iteration, pkeep)    \n",
    "    Y9 = fc_pipeline(Y8, T, is_train, iteration, pkeep)\n",
    "    \n",
    "    Y_digits = fc_pipeline(Y9, U, is_train, iteration, pkeep=1.0)\n",
    "    Y_bboxes = fc_pipeline(Y9, V, is_train, iteration, pkeep=1.0)\n",
    "    \n",
    "#     print('Y_digits : ', Y_digits.shape.as_list(), ' || Y_bboxes : ', Y_bboxes.shape.as_list())\n",
    "    \n",
    "    d_logits = [None] * no_of_digits\n",
    "    for i in range(no_of_digits):\n",
    "        # 11 digits possible for each place -> 0,1,2,3,4,5,6,7,8,9,10\n",
    "        d_logits[i] = fc_pipeline(Y_digits, 11, is_train, iteration, pkeep=1.0)\n",
    "    digits_logits = tf.stack(d_logits, axis=0)\n",
    "#     print('digits_logits : ', digits_logits.shape.as_list())\n",
    "    \n",
    "    bboxes_logits = fc_pipeline(Y_bboxes, 40, is_train, iteration, pkeep=1.0)\n",
    "#     print('bboxes_logits : ', bboxes_logits.shape.as_list())\n",
    "    \n",
    "    loss_digits = multi_digit_loss(digits_logits, Y_, max_digits=no_of_digits, name=\"loss\")\n",
    "    \n",
    "    loss_bboxes = tf.sqrt(tf.reduce_mean(tf.square(1 * (bboxes_logits - tf.to_float(Z_)))), name=\"bbox_loss\")\n",
    "    \n",
    "    loss_total = tf.add(loss_bboxes, loss_digits, name=\"loss\")\n",
    "    \n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=alpha,\n",
    "                               beta1=0.9, beta2=0.999,\n",
    "                               epsilon=1e-08,\n",
    "                               name=\"optimizer\").minimize(loss_total)\n",
    "    \n",
    "    \n",
    "    digits_preds = tf.transpose(tf.argmax(digits_logits, axis=2))\n",
    "    # digits_preds = tf.transpose(tf.argmax(digits_logits, dimension=2))\n",
    "    digits_preds = tf.to_int32(digits_preds, name=\"digit_predictions\")\n",
    "#     print(digits_preds.shape.as_list())\n",
    "    \n",
    "    bboxes_preds = tf.to_int32(bboxes_logits, name='box_predictions')\n",
    "#     print(bboxes_preds.shape.as_list())\n",
    "\n",
    "    model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svhn_train_box_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3aa953771a28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvhn_train_box_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvhn_train_box_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'  test : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbox_train_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svhn_train_box_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print('Training set', train_images.shape, train_labels.shape, train_bboxes.shape)\n",
    "\n",
    "img = train_images[:10]\n",
    "label = train_labels[:10]\n",
    "box = train_bboxes[:10]\n",
    "\n",
    "box_train_dict = {}\n",
    "batch_size = 2\n",
    "num_steps = int(label_data.shape[0] / batch_size)\n",
    "num_epochs = 1\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs - 1):\n",
    "        for step in range(num_steps - 1):\n",
    "#             max_learning_rate = 0.0005\n",
    "#             min_learning_rate = 0.0001\n",
    "\n",
    "#             decay_speed = 5000.0\n",
    "#             learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "            learning_rate = 0.001\n",
    "            \n",
    "            batch_data = train_data[step*batch_size:(step + 1)*batch_size, :, :, :]\n",
    "            batch_labels = label_data[step*batch_size:(step + 1)*batch_size, :]\n",
    "\n",
    "            feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "            _, l, train_pred, W, b = session.run([train_step, cross_entropy, train_prediction, W_s, b_s], feed_dict=feed_dict)\n",
    "            accuracy = float(acc(train_pred, batch_labels[:,1:6]))\n",
    "\n",
    "            if (step % 500 == 0):\n",
    "                minibatch = {}\n",
    "                minibatch['loss'] = l\n",
    "                minibatch['W'] = W\n",
    "                minibatch['B'] = b\n",
    "                minibatch['accuracy'] = \"%.2f\" % accuracy\n",
    "\n",
    "                res_epoch[int(step/500)] = minibatch\n",
    "                print('Loss at step %d: %f' % (step, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % acc(train_pred, batch_labels[:,1:6]))\n",
    "                print('    ')\n",
    "                \n",
    "        box_train_dict[epoch+1] = res_epoch\n",
    "\n",
    "        epoch_acc = 0\n",
    "        for f in res_epoch:\n",
    "            minibatch = res_epoch[f]\n",
    "            epoch_acc += float(minibatch['accuracy'])\n",
    "        epoch_acc = float(epoch_acc/len(res_epoch))\n",
    "        \n",
    "        _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : t_data, Y_ : t_label, pkeep : 1.0, alpha : 0.002})\n",
    "        accuracy = float(acc(predictions, t_label[:,1:6]))\n",
    "        test_acc.append(accuracy)\n",
    "\n",
    "        print('------------------------------------')\n",
    "        print('Epoch',epoch+1,' Complete with accuracy: %.2f%%' % epoch_acc)\n",
    "        print('Epoch',epoch+1,' Test Accuracy : %.2f%%' % accuracy)\n",
    "        print('------------------------------------')\n",
    "        print('        ')\n",
    "            \n",
    "    print('Training Complete on MNIST Data')\n",
    "    print('Test Accuracy : ', mean(test_acc))\n",
    "    \n",
    "    save_path = model_saver.save(session, model_to_save)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
