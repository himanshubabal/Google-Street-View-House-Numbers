{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import scipy.io\n",
    "import idx2numpy\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = scipy.io.loadmat('mnist_default_train_28x28.mat')\n",
    "test = scipy.io.loadmat('mnist_default_test_28x28.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = train['y']\n",
    "train_data = train['X']\n",
    "\n",
    "test_labels = test['y']\n",
    "test_data = test['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 28, 28, 1), (60000, 10))\n",
      "((10000, 28, 28, 1), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIDTH = 28\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, [None, WIDTH, WIDTH, 1])\n",
    "    Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Dropout Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # 5 Layers and their no of neurons\n",
    "    # 3 Convolutional Layers and a fully connected layer\n",
    "    K = 6     # First Conv Layer with depth 4\n",
    "    L = 12     # Second Conv Layer with depth 8\n",
    "    M = 24    # Third Conv layer with depth 12\n",
    "    N = 200   # Fully Connected layer with 200 neurons\n",
    "    # Last one will be softmax layer with 10 output channels\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))     # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n",
    "    \n",
    "    W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "    B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n",
    "    \n",
    "    # Model\n",
    "    stride = 1  # output is 28x28\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    \n",
    "    stride = 2  # output is 14x14\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    \n",
    "    stride = 2  # output is 7x7\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "    Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    \n",
    "    Ylogits = tf.matmul(YY4, W5) + B5\n",
    "    \n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "    \n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(Ylogits, Y_)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "    # accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "    \n",
    "    model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 10469.605469\n",
      "('accuracy : ', 0.1)\n",
      "('Learning rate : ', 0.003)\n",
      "    \n",
      "Loss at step 500: 14.001122\n",
      "('accuracy : ', 0.94999999)\n",
      "('Learning rate : ', 0.002358522270907074)\n",
      "    \n",
      "Loss at step 1000: 15.418262\n",
      "('accuracy : ', 0.95999998)\n",
      "('Learning rate : ', 0.0018589389131666372)\n",
      "    \n",
      "Loss at step 1500: 3.168375\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.0014698630029489428)\n",
      "    \n",
      "Loss at step 2000: 5.354420\n",
      "('accuracy : ', 0.99000001)\n",
      "('Learning rate : ', 0.0011668503793971828)\n",
      "    \n",
      "Loss at step 2500: 2.573733\n",
      "('accuracy : ', 0.99000001)\n",
      "('Learning rate : ', 0.0009308639108945514)\n",
      "    \n",
      "Loss at step 3000: 8.073564\n",
      "('accuracy : ', 0.99000001)\n",
      "('Learning rate : ', 0.0007470774644304465)\n",
      "    \n",
      "Loss at step 3500: 1.364063\n",
      "('accuracy : ', 0.99000001)\n",
      "('Learning rate : ', 0.000603944436006291)\n",
      "    \n",
      "Loss at step 4000: 0.162217\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.0004924723213861769)\n",
      "    \n",
      "Loss at step 4500: 1.023254\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.00040565775122940656)\n",
      "    \n",
      "Loss at step 5000: 4.098165\n",
      "('accuracy : ', 0.98000002)\n",
      "('Learning rate : ', 0.00033804649600930654)\n",
      "    \n",
      "Loss at step 5500: 4.571881\n",
      "('accuracy : ', 0.99000001)\n",
      "('Learning rate : ', 0.00028539079749945195)\n",
      "    \n",
      "Loss at step 6000: 0.091796\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.00024438249826680544)\n",
      "    \n",
      "Loss at step 6500: 0.004305\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.00021244520271199385)\n",
      "    \n",
      "Loss at step 7000: 0.228078\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.00018757241192472366)\n",
      "    \n",
      "Loss at step 7500: 0.157955\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.00016820146298242642)\n",
      "    \n",
      "Loss at step 8000: 1.007049\n",
      "('accuracy : ', 0.99000001)\n",
      "('Learning rate : ', 0.00015311535277732913)\n",
      "    \n",
      "Loss at step 8500: 0.063367\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.00014136627833609785)\n",
      "    \n",
      "Loss at step 9000: 0.048018\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.0001322160899609027)\n",
      "    \n",
      "Loss at step 9500: 0.031372\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.00012508991608904985)\n",
      "    \n",
      "Loss at step 10000: 0.070976\n",
      "('accuracy : ', 1.0)\n",
      "('Learning rate : ', 0.00011954004629734786)\n",
      "    \n",
      "('Test accuracy: ', 0.98549998)\n",
      "Model saved in file: CNN_MNIST_DEFAULT.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        #  learning rate decay\n",
    "        max_learning_rate = 0.003\n",
    "        min_learning_rate = 0.0001\n",
    "        decay_speed = 2000.0\n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "        _, l, predictions = session.run([train_step, cross_entropy, accuracy], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0): \n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('accuracy : ', predictions )\n",
    "            print('Learning rate : ', learning_rate)\n",
    "            print('    ')\n",
    "    \n",
    "    _, l, predictions = session.run([train_step, cross_entropy, accuracy], feed_dict={X : test_data, Y_ : test_labels, pkeep : 1.0, alpha : 0.002})\n",
    "    print('Test accuracy: ', predictions)\n",
    "    \n",
    "    save_path = model_saver.save(session, \"CNN_MNIST_DEFAULT.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 28\n",
    "\n",
    "grapha = tf.Graph()\n",
    "\n",
    "with grapha.as_default():\n",
    "    X = tf.placeholder(tf.float32, [None, WIDTH, WIDTH, 1])\n",
    "    Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Dropout Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # 5 Layers and their no of neurons\n",
    "    # 3 Convolutional Layers and a fully connected layer\n",
    "    K = 6     # First Conv Layer with depth 4\n",
    "    L = 12     # Second Conv Layer with depth 8\n",
    "    M = 24    # Third Conv layer with depth 12\n",
    "    N = 200   # Fully Connected layer with 200 neurons\n",
    "    # Last one will be softmax layer with 10 output channels\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))     # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n",
    "    \n",
    "    W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "    B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n",
    "    \n",
    "    # Model\n",
    "    stride = 1  # output is 28x28\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    \n",
    "    stride = 2  # output is 14x14\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    \n",
    "    stride = 2  # output is 7x7\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "    Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    \n",
    "    Ylogits = tf.matmul(YY4, W5) + B5\n",
    "    \n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "    \n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(Ylogits, Y_)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "    # accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "    \n",
    "    model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img_name = ['1', '0']\n",
    "tags = np.array([[0,1,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, 28, 28, 1), (2, 10))\n"
     ]
    }
   ],
   "source": [
    "images_ = []\n",
    "for img in img_name :\n",
    "    n = '_' + img + '_.jpg'\n",
    "    images_.append(Image.open(n).convert('L'))\n",
    "    \n",
    "for img in images_ :\n",
    "    img = img.convert('L')\n",
    "    \n",
    "images_array_ = []\n",
    "for i in images_ :\n",
    "    images_array_.append(np.array(i))\n",
    "    \n",
    "for i in range(len(images_array_)) :\n",
    "    images_array_[i] = images_array_[i][:,:,np.newaxis]\n",
    "    \n",
    "images_array_ = np.array(images_array_)\n",
    "\n",
    "images_array_ = (images_array_.astype(np.float32))\n",
    "tage = (tags.astype(np.int32))\n",
    "\n",
    "print(images_array_.shape, tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "#     tf.initialize_all_variables().run()\n",
    "    model_saver.restore(session, \"CNN_MNIST_DEFAULT.ckpt\")\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized')\n",
    "    \n",
    "    _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : svhn_test_dataset, Y_ : svhn_test_labels, pkeep : 1.0, alpha : 0.002})\n",
    "    print('Test accuracy: ', acc(predictions, svhn_test_labels[:,1:6]))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
