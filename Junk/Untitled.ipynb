{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/svhn_raw/train.tar.gz\n",
      "datasets/svhn_raw/train already present - Skipping extraction of datasets/svhn_raw/train.tar.gz.\n",
      "datasets/svhn_raw/train\n",
      "datasets/svhn_raw/test already present - Skipping extraction of datasets/svhn_raw/test.tar.gz.\n",
      "datasets/svhn_raw/test\n",
      "datasets/svhn_raw/extra already present - Skipping extraction of datasets/svhn_raw/extra.tar.gz.\n",
      "datasets/svhn_raw/extra\n",
      "train dataset shapes :  (230070, 32, 96, 1) (230070, 6)\n",
      "test dataset shapes :  (13068, 32, 96, 1) (13068, 6)\n",
      "validation dataset shapes :  (5684, 32, 96, 1) (5684, 6)\n",
      "Unable to save data to datasets/pickles/SVHN_multi_box.pickle : \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-01cd99c2f7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m'test_labels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         }\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import random\n",
    "import h5py\n",
    "from PIL import Image\n",
    "\n",
    "url = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "last_percent_reported = None\n",
    "\n",
    "data_location = 'datasets/svhn_raw/'\n",
    "pickle_location = 'datasets/pickles/'\n",
    "\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "....slow internet connections. Reports every 1% change in download progress.\n",
    "....\"\"\"\n",
    "\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write('%s%%' % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        last_percent_reported = percent\n",
    "\n",
    "\n",
    "def maybe_download(filename, force=False):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "\n",
    "    if force or not os.path.exists(filename):\n",
    "        print('Attempting to download:', filename)\n",
    "        (filename, _) = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "        statinfo = os.stat(filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "train_filename = maybe_download(data_location + 'train.tar.gz')\n",
    "test_filename = maybe_download(data_location + 'test.tar.gz')\n",
    "extra_filename = maybe_download(data_location + 'extra.tar.gz')\n",
    "\n",
    "np.random.seed(133)\n",
    "\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root = filename[:-7]  # remove .tar.gz\n",
    "    if os.path.isdir(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    data_folders = root\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "print(train_filename)\n",
    "\n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n",
    "extra_folders = maybe_extract(extra_filename)\n",
    "\n",
    "\n",
    "# The DigitStructFile is just a wrapper around the h5py data.  It basically references\n",
    "#    inf:              The input h5 matlab file\n",
    "#    digitStructName   The h5 ref to all the file names\n",
    "#    digitStructBbox   The h5 ref to all struc data\n",
    "\n",
    "class DigitStructFile:\n",
    "\n",
    "    def __init__(self, inf):\n",
    "        self.inf = h5py.File(inf, 'r')\n",
    "        self.digitStructName = self.inf['digitStruct']['name']\n",
    "        self.digitStructBbox = self.inf['digitStruct']['bbox']\n",
    "\n",
    "# getName returns the 'name' string for for the n(th) digitStruct.\n",
    "\n",
    "    def getName(self, n):\n",
    "        return ''.join([chr(c[0]) for c in\n",
    "                       self.inf[self.digitStructName[n][0]].value])\n",
    "\n",
    "# bboxHelper handles the coding difference when there is exactly one bbox or an array of bbox.\n",
    "\n",
    "    def bboxHelper(self, attr):\n",
    "        if len(attr) > 1:\n",
    "            attr = [self.inf[attr.value[j].item()].value[0][0] for j in\n",
    "                    range(len(attr))]\n",
    "        else:\n",
    "            attr = [attr.value[0][0]]\n",
    "        return attr\n",
    "\n",
    "# getBbox returns a dict of data for the n(th) bbox.\n",
    "\n",
    "    def getBbox(self, n):\n",
    "        bbox = {}\n",
    "        bb = self.digitStructBbox[n].item()\n",
    "        bbox['height'] = self.bboxHelper(self.inf[bb]['height'])\n",
    "        bbox['label'] = self.bboxHelper(self.inf[bb]['label'])\n",
    "        bbox['left'] = self.bboxHelper(self.inf[bb]['left'])\n",
    "        bbox['top'] = self.bboxHelper(self.inf[bb]['top'])\n",
    "        bbox['width'] = self.bboxHelper(self.inf[bb]['width'])\n",
    "        return bbox\n",
    "\n",
    "    def getDigitStructure(self, n):\n",
    "        s = self.getBbox(n)\n",
    "        s['name'] = self.getName(n)\n",
    "        return s\n",
    "\n",
    "# getAllDigitStructure returns all the digitStruct from the input file.\n",
    "\n",
    "    def getAllDigitStructure(self):\n",
    "        return [self.getDigitStructure(i) for i in\n",
    "                range(len(self.digitStructName))]\n",
    "\n",
    "# Return a restructured version of the dataset (one structure by boxed digit).\n",
    "#\n",
    "#   Return a list of such dicts :\n",
    "#      'filename' : filename of the samples\n",
    "#      'boxes' : list of such dicts (one by digit) :\n",
    "#          'label' : 1 to 9 corresponding digits. 10 for digit '0' in image.\n",
    "#          'left', 'top' : position of bounding box\n",
    "#          'width', 'height' : dimension of bounding box\n",
    "#\n",
    "# Note: We may turn this to a generator, if memory issues arise.\n",
    "\n",
    "    def getAllDigitStructure_ByDigit(self):\n",
    "        pictDat = self.getAllDigitStructure()\n",
    "        result = []\n",
    "        structCnt = 1\n",
    "        for i in range(len(pictDat)):\n",
    "            item = {'filename': pictDat[i]['name']}\n",
    "            figures = []\n",
    "            for j in range(len(pictDat[i]['height'])):\n",
    "                figure = {}\n",
    "                figure['height'] = pictDat[i]['height'][j]\n",
    "                figure['label'] = pictDat[i]['label'][j]\n",
    "                figure['left'] = pictDat[i]['left'][j]\n",
    "                figure['top'] = pictDat[i]['top'][j]\n",
    "                figure['width'] = pictDat[i]['width'][j]\n",
    "                figures.append(figure)\n",
    "            structCnt = structCnt + 1\n",
    "            item['boxes'] = figures\n",
    "            result.append(item)\n",
    "        return result\n",
    "\n",
    "\n",
    "train_folders = data_location + 'train'\n",
    "test_folders = data_location + 'test'\n",
    "extra_folders = data_location + 'extra'\n",
    "\n",
    "struct_pickle_file = pickle_location + 'SVHN_new_data_struct.pickle'\n",
    "\n",
    "if not os.path.exists(struct_pickle_file):\n",
    "    fin = os.path.join(train_folders, 'digitStruct.mat')\n",
    "    dsf = DigitStructFile(fin)\n",
    "    train_data = dsf.getAllDigitStructure_ByDigit()\n",
    "\n",
    "    fin = os.path.join(test_folders, 'digitStruct.mat')\n",
    "    dsf = DigitStructFile(fin)\n",
    "    test_data = dsf.getAllDigitStructure_ByDigit()\n",
    "\n",
    "    fin = os.path.join(extra_folders, 'digitStruct.mat')\n",
    "    dsf = DigitStructFile(fin)\n",
    "    extra_data = dsf.getAllDigitStructure_ByDigit()\n",
    "\n",
    "else:\n",
    "    with open(struct_pickle_file, 'rb') as f:\n",
    "        save = pickle.load(f, encoding='latin1')\n",
    "        test_data = save['test_data']\n",
    "        extra_data = save['extra_data']\n",
    "        train_data = save['train_data']\n",
    "        del save\n",
    "\n",
    "train_imsize = np.ndarray([len(train_data), 2])\n",
    "for i in np.arange(len(train_data)):\n",
    "    filename = train_data[i]['filename']\n",
    "    fullname = os.path.join(train_folders, filename)\n",
    "    im = Image.open(fullname)\n",
    "    train_imsize[i, :] = im.size[:]\n",
    "\n",
    "test_imsize = np.ndarray([len(test_data), 2])\n",
    "for i in np.arange(len(test_data)):\n",
    "    filename = test_data[i]['filename']\n",
    "    fullname = os.path.join(test_folders, filename)\n",
    "    im = Image.open(fullname)\n",
    "    test_imsize[i, :] = im.size[:]\n",
    "\n",
    "extra_imsize = np.ndarray([len(extra_data), 2])\n",
    "for i in np.arange(len(extra_data)):\n",
    "    filename = extra_data[i]['filename']\n",
    "    fullname = os.path.join(extra_folders, filename)\n",
    "    im = Image.open(fullname)\n",
    "    extra_imsize[i, :] = im.size[:]\n",
    "\n",
    "\n",
    "def generate_dataset(data, folder):\n",
    "\n",
    "    dataset = np.ndarray([len(data), 32, 96, 1], dtype='float32')\n",
    "    labels = np.ones([len(data), 6], dtype=int) * 10\n",
    "    for i in np.arange(len(data)):\n",
    "        filename = data[i]['filename']\n",
    "        fullname = os.path.join(folder, filename)\n",
    "        im = Image.open(fullname)\n",
    "        boxes = data[i]['boxes']\n",
    "        num_digit = len(boxes)\n",
    "        labels[i, 0] = num_digit\n",
    "        top = np.ndarray([num_digit], dtype='float32')\n",
    "        left = np.ndarray([num_digit], dtype='float32')\n",
    "        height = np.ndarray([num_digit], dtype='float32')\n",
    "        width = np.ndarray([num_digit], dtype='float32')\n",
    "        for j in np.arange(num_digit):\n",
    "            if j < 5:\n",
    "                labels[i, j + 1] = boxes[j]['label']\n",
    "                if boxes[j]['label'] == 10:\n",
    "                    labels[i, j + 1] = 0\n",
    "\n",
    "            top[j] = boxes[j]['top']\n",
    "            left[j] = boxes[j]['left']\n",
    "            height[j] = boxes[j]['height']\n",
    "            width[j] = boxes[j]['width']\n",
    "\n",
    "        im_top = np.amin(top)\n",
    "        im_left = np.amin(left)\n",
    "        im_height = np.amax(top) + height[np.argmax(top)] - im_top\n",
    "        im_width = np.amax(left) + width[np.argmax(left)] - im_left\n",
    "\n",
    "        im_top = np.floor(im_top - 0.1 * im_height)\n",
    "        im_left = np.floor(im_left - 0.1 * im_width)\n",
    "        im_bottom = np.amin([np.ceil(im_top + 1.2 * im_height), im.size[1]])\n",
    "        im_right = np.amin([np.ceil(im_left + 1.2 * im_width), im.size[0]])\n",
    "\n",
    "        im = im.crop((im_left, im_top, im_right, im_bottom)).resize([96, 32], Image.ANTIALIAS)\n",
    "        im = np.dot(np.array(im, dtype='float32'), [[0.2989], [0.5870], [0.1140]])\n",
    "        mean = np.mean(im, dtype='float32')\n",
    "        std = np.std(im, dtype='float32', ddof=1)\n",
    "        if std < 1e-4:\n",
    "            std = 1.\n",
    "        im = (im - mean) / std\n",
    "        dataset[i, :, :, :] = im[:, :, :]\n",
    "\n",
    "    return (dataset, labels)\n",
    "\n",
    "\n",
    "(train_dataset, train_labels) = generate_dataset(train_data, train_folders)\n",
    "(test_dataset, test_labels) = generate_dataset(test_data, test_folders)\n",
    "(extra_dataset, extra_labels) = generate_dataset(extra_data, extra_folders)\n",
    "\n",
    "train_dataset = np.delete(train_dataset, 29929, axis=0)\n",
    "train_labels = np.delete(train_labels, 29929, axis=0)\n",
    "\n",
    "random.seed()\n",
    "\n",
    "n_labels = 10\n",
    "valid_index = []\n",
    "valid_index2 = []\n",
    "train_index = []\n",
    "train_index2 = []\n",
    "for i in np.arange(n_labels):\n",
    "    valid_index.extend((np.where(train_labels[:, 1] == i)[0])[:400].tolist())\n",
    "    train_index.extend((np.where(train_labels[:, 1] == i)[0])[400:].tolist())\n",
    "    valid_index2.extend((np.where(extra_labels[:, 1] == i)[0])[:200].tolist())\n",
    "    train_index2.extend((np.where(extra_labels[:, 1] == i)[0])[200:].tolist())\n",
    "\n",
    "random.shuffle(valid_index)\n",
    "random.shuffle(train_index)\n",
    "random.shuffle(valid_index2)\n",
    "random.shuffle(train_index2)\n",
    "\n",
    "valid_dataset = np.concatenate((extra_dataset[valid_index2, :, :, :], train_dataset[valid_index, :, :, :]), axis=0)\n",
    "valid_labels = np.concatenate((extra_labels[valid_index2, :], train_labels[valid_index, :]), axis=0)\n",
    "train_dataset_t = np.concatenate((extra_dataset[train_index2, :, :, :], train_dataset[train_index, :, :, :]), axis=0)\n",
    "train_labels_t = np.concatenate((extra_labels[train_index2, :], train_labels[train_index, :]), axis=0)\n",
    "\n",
    "print('train dataset shapes : ', train_dataset_t.shape, train_labels_t.shape)\n",
    "print('test dataset shapes : ', test_dataset.shape, test_labels.shape)\n",
    "print('validation dataset shapes : ', valid_dataset.shape, valid_labels.shape)\n",
    "\n",
    "\n",
    "# pickle_file = data_location + 'SVHN_multi_box.pickle'\n",
    "pickle_file = pickle_location + 'SVHN_multi_box.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset_t,\n",
    "        'train_labels': train_labels_t,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "        }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "\n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)\n",
    "print('SVHN Datasets ready in SVHN_multi.pickle')\n",
    "\n",
    "\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
