{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (230255, 32, 96, 1) (230255, 6)\n",
      "Test set (13068, 32, 96, 1) (13068, 6)\n",
      "Validation set (5500, 32, 96, 1) (5500, 6)\n"
     ]
    }
   ],
   "source": [
    "hdf_file = 'datasets/pickles/SVHN_multi.hdf5'\n",
    "\n",
    "hdf = h5py.File(hdf_file,'r')\n",
    "train_dataset = hdf['train_images'][:]\n",
    "train_labels = hdf['train_labels'][:]\n",
    "test_dataset = hdf['test_images'][:]\n",
    "test_labels = hdf['test_labels'][:]\n",
    "valid_dataset = hdf['valid_images'][:]\n",
    "valid_labels = hdf['valid_labels'][:]\n",
    "            \n",
    "hdf.close()    \n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.astype(np.float32)\n",
    "test_dataset = test_dataset.astype(np.float32)\n",
    "valid_dataset = valid_dataset.astype(np.float32)\n",
    "\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "test_labels = test_labels.astype(np.int32)\n",
    "valid_labels = valid_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_to_save = \"saved_models/combined/multi_on_mnist/CNN_SVHN_Multi_on_Mnist.ckpt\"\n",
    "saved_mnist_model = \"saved_models/combined/box_on_mnist/CNN_SVHN_Box_on_Mnist.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_svhn = tf.Graph()\n",
    "\n",
    "with graph_svhn.as_default():\n",
    "    HEIGHT = 32\n",
    "    WIDTH = 32*3\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, 1])\n",
    "    Y_ = tf.placeholder(tf.int32, [None, 6])\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Dropout Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # 5 Layers and their no of neurons\n",
    "    # 3 Convolutional Layers and a fully connected layer\n",
    "    K = 6     # First Conv Layer with depth 6\n",
    "    L = 12     # Second Conv Layer with depth 12\n",
    "    M = 24    # Third Conv layer with depth 24\n",
    "    N = 200   # Fourth Fully Connected layer with 200 neurons\n",
    "    # Last one will be softmax layer with 10 output channels\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1), name=\"W1\")    # 6x6 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]), name=\"B1\")\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1), name=\"W2\")\n",
    "    B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]), name=\"B2\")\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1), name=\"W3\")\n",
    "    B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]), name=\"B3\")\n",
    "    \n",
    "    W5_1 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_1\")\n",
    "    B5_1 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_1\")\n",
    "    \n",
    "    W5_2 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_2\")\n",
    "    B5_2 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_2\")\n",
    "    \n",
    "    W5_3 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_3\")\n",
    "    B5_3 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_3\")\n",
    "    \n",
    "    W5_4 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_4\")\n",
    "    B5_4 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_4\")\n",
    "    \n",
    "    W5_5 = tf.Variable(tf.truncated_normal([N, 11], stddev=0.1), name=\"W5_5\")\n",
    "    B5_5 = tf.Variable(tf.constant(0.1, tf.float32, [11]), name=\"B5_5\")\n",
    "    \n",
    "    # Model\n",
    "    stride = 1  # output is 32x96\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    \n",
    "    stride = 2  # output is 16x48\n",
    "    Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    \n",
    "    stride = 2  # output is 8x24\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "    # reshape the output from the third convolution for the fully connected layer\n",
    "    shape = Y3.get_shape().as_list()\n",
    "    YY = tf.reshape(Y3, shape=[-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([shape[1] * shape[2] * shape[3], N], stddev=0.1), name=\"W4\")\n",
    "    B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]), name=\"B4\")\n",
    "\n",
    "    Y4 = tf.sigmoid(tf.matmul(YY, W4) + B4)\n",
    "    YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "    \n",
    "    Ylogits_1 = tf.matmul(YY4, W5_1) + B5_1\n",
    "    Ylogits_2 = tf.matmul(YY4, W5_2) + B5_2\n",
    "    Ylogits_3 = tf.matmul(YY4, W5_3) + B5_3\n",
    "    Ylogits_4 = tf.matmul(YY4, W5_4) + B5_4\n",
    "    Ylogits_5 = tf.matmul(YY4, W5_5) + B5_5   \n",
    "    ## ('Ylogits_1 shape : ', [None, 11])\n",
    "    \n",
    "    Y_1 = tf.nn.softmax(Ylogits_1)\n",
    "    Y_2 = tf.nn.softmax(Ylogits_2)\n",
    "    Y_3 = tf.nn.softmax(Ylogits_3)\n",
    "    Y_4 = tf.nn.softmax(Ylogits_4)\n",
    "    Y_5 = tf.nn.softmax(Ylogits_5)\n",
    "   \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_1, Y_[:,1])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_2, Y_[:,2])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_3, Y_[:,3])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_4, Y_[:,4])) +\\\n",
    "    tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(Ylogits_5, Y_[:,5]))\n",
    "\n",
    "    train_prediction = tf.pack([Y_1, Y_2, Y_3, Y_4, Y_5])\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "    \n",
    "    W_s = tf.pack([tf.reduce_max(tf.abs(W1)),tf.reduce_max(tf.abs(W2)),tf.reduce_max(tf.abs(W3)),tf.reduce_max(tf.abs(W4))])\n",
    "    b_s = tf.pack([tf.reduce_max(tf.abs(B1)),tf.reduce_max(tf.abs(B2)),tf.reduce_max(tf.abs(B3)),tf.reduce_max(tf.abs(B4))])\n",
    "    \n",
    "    model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  (125000, 32, 96, 1)   test :  (125000, 6)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_dataset[0:125000]\n",
    "label_data = train_labels[0:125000]\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "W :  [ 0.31812128  0.47755289  0.41438857  0.74337363]\n",
      "b :  [ 0.13971798  0.35880855  0.5294922   0.22335321]\n",
      "Loss at step 0: 34.898956\n",
      "Minibatch accuracy: 20.2%\n",
      "Learning rate :  0.0005\n",
      "    \n",
      "W :  [ 0.30540377  0.49145553  0.42058745  0.74134821]\n",
      "b :  [ 0.13995735  0.35920218  0.53238881  0.22562999]\n",
      "Loss at step 500: 7.441952\n",
      "Minibatch accuracy: 51.9%\n",
      "Learning rate :  0.00046193496721438383\n",
      "    \n",
      "W :  [ 0.30543074  0.49143091  0.42063224  0.74134821]\n",
      "b :  [ 0.13995619  0.35911319  0.53237379  0.22562999]\n",
      "Loss at step 1000: 7.230564\n",
      "Minibatch accuracy: 51.7%\n",
      "Learning rate :  0.00042749230123119273\n",
      "    \n",
      "W :  [ 0.30382398  0.49153945  0.42064172  0.74134821]\n",
      "b :  [ 0.1399387   0.35905415  0.53229761  0.22562999]\n",
      "Loss at step 1500: 7.203030\n",
      "Minibatch accuracy: 53.9%\n",
      "Learning rate :  0.00039632728827268716\n",
      "    \n",
      "W :  [ 0.30152917  0.49106887  0.42058113  0.74134821]\n",
      "b :  [ 0.13992423  0.35911414  0.53209585  0.22463393]\n",
      "Loss at step 2000: 7.498715\n",
      "Minibatch accuracy: 51.6%\n",
      "Learning rate :  0.00036812801841425575\n",
      "    \n",
      "W :  [ 0.30018577  0.49102813  0.42058     0.74134821]\n",
      "b :  [ 0.13991924  0.35908899  0.53209764  0.22463393]\n",
      "Loss at step 2500: 6.960279\n",
      "Minibatch accuracy: 54.4%\n",
      "Learning rate :  0.0003426122638850534\n",
      "    \n",
      "W :  [ 0.3017889   0.49149173  0.42065009  0.74134821]\n",
      "b :  [ 0.13993353  0.35917044  0.53206933  0.22463393]\n",
      "Loss at step 3000: 7.167845\n",
      "Minibatch accuracy: 53.0%\n",
      "Learning rate :  0.00031952465443761056\n",
      "    \n",
      "W :  [ 0.29824355  0.49259007  0.42065722  0.74134821]\n",
      "b :  [ 0.13985518  0.35969925  0.53238177  0.22463393]\n",
      "Loss at step 3500: 6.964263\n",
      "Minibatch accuracy: 54.1%\n",
      "Learning rate :  0.00029863412151656383\n",
      "    \n",
      "W :  [ 0.30346423  0.49284041  0.42053252  0.74134821]\n",
      "b :  [ 0.13985343  0.36025074  0.53279829  0.22463393]\n",
      "Loss at step 4000: 6.835378\n",
      "Minibatch accuracy: 55.6%\n",
      "Learning rate :  0.00027973158564688865\n",
      "    \n",
      "W :  [ 0.2999903   0.49304283  0.42024177  0.74134821]\n",
      "b :  [ 0.1398353   0.35996154  0.53285003  0.22463393]\n",
      "Loss at step 4500: 6.573709\n",
      "Minibatch accuracy: 56.4%\n",
      "Learning rate :  0.0002626278638962397\n",
      "    \n",
      "W :  [ 0.29593652  0.49191576  0.41885492  0.74134821]\n",
      "b :  [ 0.13987982  0.35899401  0.53255874  0.22463393]\n",
      "Loss at step 5000: 6.909734\n",
      "Minibatch accuracy: 55.0%\n",
      "Learning rate :  0.00024715177646857697\n",
      "    \n",
      "W :  [ 0.29995295  0.49508163  0.41910943  0.74134821]\n",
      "b :  [ 0.13996418  0.3588939   0.53263831  0.22463393]\n",
      "Loss at step 5500: 6.997267\n",
      "Minibatch accuracy: 54.2%\n",
      "Learning rate :  0.0002331484334792318\n",
      "    \n",
      "W :  [ 0.30083308  0.4946914   0.41761777  0.74134821]\n",
      "b :  [ 0.13996282  0.35891685  0.532664    0.22463393]\n",
      "Loss at step 6000: 6.907149\n",
      "Minibatch accuracy: 54.7%\n",
      "Learning rate :  0.00022047768476488088\n",
      "    \n",
      "W :  [ 0.30081767  0.49472067  0.41761944  0.74134821]\n",
      "b :  [ 0.13996442  0.35891268  0.53266358  0.22463393]\n",
      "Loss at step 6500: 6.702001\n",
      "Minibatch accuracy: 55.6%\n",
      "Learning rate :  0.00020901271721360503\n",
      "    \n",
      "W :  [ 0.30103508  0.49461034  0.41761944  0.74134821]\n",
      "b :  [ 0.13995627  0.35910925  0.53272933  0.22463393]\n",
      "Loss at step 7000: 7.023553\n",
      "Minibatch accuracy: 53.3%\n",
      "Learning rate :  0.0001986387855766426\n",
      "    \n",
      "W :  [ 0.30103526  0.49461034  0.41761944  0.74134821]\n",
      "b :  [ 0.13995627  0.35910925  0.53272933  0.22463393]\n",
      "Loss at step 7500: 7.001530\n",
      "Minibatch accuracy: 55.5%\n",
      "Learning rate :  0.00018925206405937195\n",
      "    \n",
      "W :  [ 0.30103898  0.49462363  0.4176192   0.74134821]\n",
      "b :  [ 0.13995545  0.35911041  0.53273064  0.22463393]\n",
      "Loss at step 8000: 6.945157\n",
      "Minibatch accuracy: 54.4%\n",
      "Learning rate :  0.00018075860719786216\n",
      "    \n",
      "W :  [ 0.30296841  0.4945358   0.41796786  0.74134821]\n",
      "b :  [ 0.13985561  0.35934001  0.53270048  0.22463393]\n",
      "Loss at step 8500: 7.057916\n",
      "Minibatch accuracy: 55.2%\n",
      "Learning rate :  0.00017307340962109387\n",
      "    \n",
      "W :  [ 0.30296227  0.494535    0.41796929  0.74134821]\n",
      "b :  [ 0.13985553  0.35934001  0.5327003   0.22463393]\n",
      "Loss at step 9000: 6.701381\n",
      "Minibatch accuracy: 55.9%\n",
      "Learning rate :  0.00016611955528863463\n",
      "    \n",
      "W :  [ 0.30296859  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 9500: 6.947968\n",
      "Minibatch accuracy: 53.1%\n",
      "Learning rate :  0.00015982744768905404\n",
      "    \n",
      "W :  [ 0.30296859  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 10000: 6.794528\n",
      "Minibatch accuracy: 54.1%\n",
      "Learning rate :  0.0001541341132946451\n",
      "    \n",
      "W :  [ 0.30296838  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 10500: 6.660719\n",
      "Minibatch accuracy: 55.5%\n",
      "Learning rate :  0.00014898257130119277\n",
      "    \n",
      "W :  [ 0.30296838  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 11000: 7.237292\n",
      "Minibatch accuracy: 52.3%\n",
      "Learning rate :  0.00014432126334493355\n",
      "    \n",
      "W :  [ 0.30296838  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 11500: 6.752375\n",
      "Minibatch accuracy: 53.3%\n",
      "Learning rate :  0.0001401035374891215\n",
      "    \n",
      "W :  [ 0.30296838  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 12000: 6.847499\n",
      "Minibatch accuracy: 55.2%\n",
      "Learning rate :  0.00013628718131576502\n",
      "    \n",
      "W :  [ 0.30296838  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 12500: 6.874578\n",
      "Minibatch accuracy: 53.8%\n",
      "Learning rate :  0.00013283399944955952\n",
      "    \n",
      "W :  [ 0.30296838  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 13000: 6.642741\n",
      "Minibatch accuracy: 56.4%\n",
      "Learning rate :  0.00012970943128573357\n",
      "    \n",
      "W :  [ 0.30296838  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 13500: 7.097761\n",
      "Minibatch accuracy: 53.9%\n",
      "Learning rate :  0.0001268822050958999\n",
      "    \n",
      "W :  [ 0.30296838  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 14000: 6.505353\n",
      "Minibatch accuracy: 57.5%\n",
      "Learning rate :  0.0001243240250500872\n",
      "    \n",
      "W :  [ 0.30296835  0.49453327  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 14500: 6.729436\n",
      "Minibatch accuracy: 56.6%\n",
      "Learning rate :  0.0001220092880225629\n",
      "    \n",
      "W :  [ 0.30296835  0.4945333   0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 15000: 6.776422\n",
      "Minibatch accuracy: 56.1%\n",
      "Learning rate :  0.00011991482734714559\n",
      "    \n",
      "W :  [ 0.30296826  0.4945333   0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 15500: 6.954148\n",
      "Minibatch accuracy: 53.1%\n",
      "Learning rate :  0.00011801968095742312\n",
      "    \n",
      "W :  [ 0.30296826  0.49453333  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 16000: 7.150990\n",
      "Minibatch accuracy: 54.1%\n",
      "Learning rate :  0.00011630488159134649\n",
      "    \n",
      "W :  [ 0.30296811  0.49453333  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 16500: 7.148120\n",
      "Minibatch accuracy: 52.3%\n",
      "Learning rate :  0.00011475326696049602\n",
      "    \n",
      "W :  [ 0.30296811  0.49453339  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 17000: 6.781863\n",
      "Minibatch accuracy: 55.9%\n",
      "Learning rate :  0.00011334930798413044\n",
      "    \n",
      "W :  [ 0.30296797  0.49453339  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 17500: 6.925751\n",
      "Minibatch accuracy: 53.8%\n",
      "Learning rate :  0.0001120789533689274\n",
      "    \n",
      "W :  [ 0.30296797  0.49453348  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 18000: 6.815847\n",
      "Minibatch accuracy: 54.4%\n",
      "Learning rate :  0.00011092948897891703\n",
      "    \n",
      "W :  [ 0.30296797  0.49453348  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 18500: 6.831025\n",
      "Minibatch accuracy: 54.1%\n",
      "Learning rate :  0.00010988941058813576\n",
      "    \n",
      "W :  [ 0.30296797  0.49453348  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 19000: 6.932391\n",
      "Minibatch accuracy: 54.8%\n",
      "Learning rate :  0.00010894830874246625\n",
      "    \n",
      "W :  [ 0.30296758  0.49453348  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 19500: 7.001542\n",
      "Minibatch accuracy: 54.2%\n",
      "Learning rate :  0.00010809676457832176\n",
      "    \n",
      "W :  [ 0.30296755  0.49453375  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 20000: 6.896663\n",
      "Minibatch accuracy: 55.3%\n",
      "Learning rate :  0.00010732625555549367\n",
      "    \n",
      "W :  [ 0.30296749  0.49453375  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 20500: 6.837519\n",
      "Minibatch accuracy: 54.4%\n",
      "Learning rate :  0.0001066290701607045\n",
      "    \n",
      "W :  [ 0.30296749  0.49453375  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934225  0.53270143  0.22463393]\n",
      "Loss at step 21000: 6.845109\n",
      "Minibatch accuracy: 52.5%\n",
      "Learning rate :  0.00010599823072819108\n",
      "    \n",
      "W :  [ 0.30296823  0.49453375  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934201  0.53270143  0.22463393]\n",
      "Loss at step 21500: 6.965177\n",
      "Minibatch accuracy: 53.4%\n",
      "Learning rate :  0.00010542742360488037\n",
      "    \n",
      "W :  [ 0.30296823  0.49453375  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934201  0.53270143  0.22463393]\n",
      "Loss at step 22000: 7.092766\n",
      "Minibatch accuracy: 53.1%\n",
      "Learning rate :  0.00010491093596122738\n",
      "    \n",
      "W :  [ 0.3029722   0.49453375  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934055  0.53270119  0.22463393]\n",
      "Loss at step 22500: 6.797048\n",
      "Minibatch accuracy: 55.6%\n",
      "Learning rate :  0.00010444359861529693\n",
      "    \n",
      "W :  [ 0.3029722   0.49453375  0.41797051  0.74134821]\n",
      "b :  [ 0.13985568  0.35934055  0.53270119  0.22463393]\n",
      "Loss at step 23000: 6.779710\n",
      "Minibatch accuracy: 56.4%\n",
      "Learning rate :  0.00010402073429785344\n",
      "    \n",
      "W :  [ 0.30321169  0.49505931  0.4179489   0.74134821]\n",
      "b :  [ 0.13985783  0.35870439  0.53255492  0.22463393]\n",
      "Loss at step 23500: 7.012480\n",
      "Minibatch accuracy: 54.2%\n",
      "Learning rate :  0.00010363811084067834\n",
      "    \n",
      "W :  [ 0.30321169  0.49505931  0.4179489   0.74134821]\n",
      "b :  [ 0.13985783  0.35870439  0.53255492  0.22463393]\n",
      "Loss at step 24000: 6.960969\n",
      "Minibatch accuracy: 55.3%\n",
      "Learning rate :  0.00010329189881960801\n",
      "    \n",
      "W :  [ 0.29600355  0.49931097  0.42118663  0.74134821]\n",
      "b :  [ 0.13622434  0.35882354  0.53257132  0.22463393]\n",
      "Loss at step 24500: 6.810053\n",
      "Minibatch accuracy: 54.8%\n",
      "Learning rate :  0.00010297863322836974\n",
      "    \n",
      "W :  [ 0.29876482  0.49792111  0.41830274  0.74134821]\n",
      "b :  [ 0.13912618  0.35543624  0.53006148  0.22463393]\n",
      "Loss at step 25000: 6.934011\n",
      "Minibatch accuracy: 54.7%\n",
      "Learning rate :  0.00010269517879963419\n",
      "    \n",
      "W :  [ 0.30101195  0.49463099  0.42155468  0.74134821]\n",
      "b :  [ 0.13482258  0.35802358  0.53239334  0.22463393]\n",
      "Loss at step 25500: 7.421713\n",
      "Minibatch accuracy: 52.2%\n",
      "Learning rate :  0.00010243869862620625\n",
      "    \n",
      "W :  [ 0.30099949  0.49463111  0.42155468  0.74134821]\n",
      "b :  [ 0.13483238  0.35803759  0.53240776  0.22463393]\n",
      "Loss at step 26000: 6.905084\n",
      "Minibatch accuracy: 54.5%\n",
      "Learning rate :  0.00010220662576830431\n",
      "    \n",
      "W :  [ 0.30306324  0.49461982  0.4215551   0.74134821]\n",
      "b :  [ 0.13599895  0.35856938  0.53240049  0.22463393]\n",
      "Loss at step 26500: 6.863058\n",
      "Minibatch accuracy: 53.1%\n",
      "Learning rate :  0.00010199663756276409\n",
      "    \n",
      "W :  [ 0.30448049  0.49492326  0.42142704  0.74134821]\n",
      "b :  [ 0.13682635  0.3597964   0.53341538  0.22463393]\n",
      "Loss at step 27000: 6.957403\n",
      "Minibatch accuracy: 52.2%\n",
      "Learning rate :  0.00010180663237704508\n",
      "    \n",
      "W :  [ 0.305783    0.49488616  0.42029116  0.74134821]\n",
      "b :  [ 0.13374704  0.36263615  0.53231758  0.22463393]\n",
      "Loss at step 27500: 6.997765\n",
      "Minibatch accuracy: 54.4%\n",
      "Learning rate :  0.00010163470857538563\n",
      "    \n",
      "W :  [ 0.3058103   0.4948867   0.42029116  0.74134821]\n",
      "b :  [ 0.13374631  0.36263752  0.53231913  0.22463393]\n",
      "Loss at step 28000: 7.000897\n",
      "Minibatch accuracy: 54.2%\n",
      "Learning rate :  0.00010147914548659317\n",
      "    \n",
      "W :  [ 0.31178775  0.49461374  0.42040494  0.74134821]\n",
      "b :  [ 0.13965413  0.36135224  0.53096086  0.22463393]\n",
      "Loss at step 28500: 7.022632\n",
      "Minibatch accuracy: 52.7%\n",
      "Learning rate :  0.00010133838618298851\n",
      "    \n",
      "W :  [ 0.30926746  0.49433091  0.42038918  0.74134821]\n",
      "b :  [ 0.1366576   0.35864559  0.53326058  0.22463393]\n",
      "Loss at step 29000: 6.533269\n",
      "Minibatch accuracy: 56.1%\n",
      "Learning rate :  0.00010121102189815033\n",
      "    \n",
      "W :  [ 0.30919141  0.49511406  0.42022395  0.74134821]\n",
      "b :  [ 0.13458569  0.35944027  0.53383362  0.22463393]\n",
      "Loss at step 29500: 6.957428\n",
      "Minibatch accuracy: 52.7%\n",
      "Learning rate :  0.00010109577792750736\n",
      "    \n",
      "W :  [ 0.30914608  0.49424225  0.42022243  0.74134821]\n",
      "b :  [ 0.13449872  0.35971382  0.53411561  0.22463393]\n",
      "Loss at step 30000: 7.185672\n",
      "Minibatch accuracy: 54.1%\n",
      "Learning rate :  0.00010099150087066655\n",
      "    \n",
      "Training Complete on MNIST Data\n",
      "Model saved in file: saved_models/combined/multi_on_mnist/CNN_SVHN_Multi_on_Mnist.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps_1 = 30001\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session(graph=graph_svhn) as session:\n",
    "#     tf.global_variables_initializer().run()\n",
    "    model_saver.restore(session, saved_mnist_model)\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps_1):\n",
    "        #  learning rate decay\n",
    "        max_learning_rate = 0.0005\n",
    "        min_learning_rate = 0.0001\n",
    "\n",
    "        decay_speed = 5000.0\n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "        offset = (step * batch_size) % (label_data.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = label_data[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "        _, l, train_pred, W, b = session.run([train_step, cross_entropy, train_prediction, W_s, b_s], feed_dict=feed_dict)\n",
    "    \n",
    "        if (step % 500 == 0):\n",
    "            print('W : ', W)\n",
    "            print('b : ', b)\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % acc(train_pred, batch_labels[:,1:6]))\n",
    "            print('Learning rate : ', learning_rate)\n",
    "            print('    ')\n",
    "            \n",
    "    print('Training Complete on MNIST Data')\n",
    "    \n",
    "    save_path = model_saver.save(session, model_to_save)\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "-------TEST--------\n",
      "Test-Accuracy  i :  0\n",
      "Test accuracy:  63.72\n",
      "       \n",
      "Test-Accuracy  i :  1\n",
      "Test accuracy:  64.14\n",
      "       \n",
      "Test-Accuracy  i :  2\n",
      "Test accuracy:  63.8\n",
      "       \n",
      "Test-Accuracy  i :  3\n",
      "Test accuracy:  64.22\n",
      "       \n",
      "Test-Accuracy  i :  4\n",
      "Test accuracy:  64.4\n",
      "       \n",
      "Test-Accuracy  i :  5\n",
      "Test accuracy:  63.8\n",
      "       \n",
      "Test-Accuracy  i :  6\n",
      "Test accuracy:  63.78\n",
      "       \n",
      "Test-Accuracy  i :  7\n",
      "Test accuracy:  64.52\n",
      "       \n",
      "Test-Accuracy  i :  8\n",
      "Test accuracy:  64.6\n",
      "       \n",
      "Test-Accuracy  i :  9\n",
      "Test accuracy:  64.12\n",
      "       \n",
      "Test-Accuracy  i :  10\n",
      "Test accuracy:  64.28\n",
      "       \n",
      "Test-Accuracy  i :  11\n",
      "Test accuracy:  64.48\n",
      "       \n",
      "-----VALIDIDATION------\n",
      "Valid-Accuracy  i :  0\n",
      "Valid accuracy:  19.08\n",
      "        \n",
      "Valid-Accuracy  i :  1\n",
      "Valid accuracy:  18.42\n",
      "        \n",
      "Valid-Accuracy  i :  2\n",
      "Valid accuracy:  19.38\n",
      "        \n",
      "Valid-Accuracy  i :  3\n",
      "Valid accuracy:  19.24\n",
      "        \n",
      "-----  FINAL  ------\n",
      "Final Test Set Accuracy :  64.16\n",
      "Final Validation Set Accuracy :  19.03\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_svhn) as session: \n",
    "    print('Initialized')\n",
    "    batch = 1000\n",
    "    \n",
    "    test_acc = list()\n",
    "    print('-------TEST--------')\n",
    "    test_no = int(test_labels.shape[0] / batch)\n",
    "    for i in range(test_no - 1):\n",
    "        model_saver.restore(session, model_to_save)\n",
    "        data = test_dataset[i*batch:(i+1)*batch]\n",
    "        labels = test_labels[i*batch:(i+1)*batch]\n",
    "        \n",
    "        _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : data, Y_ : labels, pkeep : 1.0, alpha : 0.002})\n",
    "        accuracy = acc(predictions, labels[:,1:6])\n",
    "        test_acc.append(accuracy)\n",
    "        \n",
    "        print('Test-Accuracy', ' i : ', i)\n",
    "        print('Test accuracy: ', accuracy)\n",
    "        print('       ')\n",
    "        \n",
    "        \n",
    "    valid_acc = list()\n",
    "    print('-----VALIDIDATION------')\n",
    "    valid_no = int(valid_labels.shape[0] /  batch)\n",
    "    for i in range(valid_no - 1):\n",
    "        model_saver.restore(session, \"saved_models/box/CNN_SVHN_Box.ckpt\")\n",
    "        data = valid_dataset[i*batch:(i+1)*batch]\n",
    "        labels = valid_labels[i*batch:(i+1)*batch]\n",
    "        \n",
    "        _, l, predictions = session.run([train_step, cross_entropy, train_prediction], feed_dict={X : data, Y_ : labels, pkeep : 1.0, alpha : 0.002})\n",
    "        accuracy = acc(predictions, labels[:,1:6])\n",
    "        valid_acc.append(accuracy)\n",
    "        \n",
    "        print('Valid-Accuracy', ' i : ', i)\n",
    "        print('Valid accuracy: ', accuracy)\n",
    "        print('        ')\n",
    "        \n",
    "        \n",
    "    test_avg = mean(test_acc)\n",
    "    valid_avg = mean(valid_acc)\n",
    "    \n",
    "    print('-----  FINAL  ------')\n",
    "    print('Final Test Set Accuracy : ',\"%.2f\" % test_avg)\n",
    "    print('Final Validation Set Accuracy : ',\"%.2f\" % valid_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
